{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7GuoX-D2SUi"
      },
      "source": [
        "# Decoder-only based GPT (language model)\n",
        "\n",
        "Here we take a transformer block, the decoder in particular, and use it for the task of language modeling. In general, this is how GPTs are trained. We will do this on a much smaller scale.\n",
        "\n",
        "We take everything we've already built and leverage it in the way Karpathy implements a character level LM here:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ptBhD9c3kTU",
        "outputId": "7a4fab7c-9708-4bce-ee9f-58162ca40621"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/Transformers/models/transformer_blocks.py .\n",
        "!cp drive/MyDrive/Transformers/models/modules.py .\n",
        "!cp -r drive/MyDrive/Transformers/data/ ."
      ],
      "metadata": {
        "id": "DAnCCtxa3i6o"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenmonster"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_XqFHTe4Ozn",
        "outputId": "adc62d67-1a0b-451a-b1dd-37cc5cb14708"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenmonster in /usr/local/lib/python3.10/dist-packages (1.1.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "p0LzXILI2SUk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from torch.utils.data import random_split\n",
        "import sys\n",
        "sys.path.append(\"~/\")\n",
        "from transformer_blocks import Transformer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tokenmonster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "UP_BMiE52SUl",
        "outputId": "cccf4408-0486-42d6-bfb7-c6c90cbe0e84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2652650\n"
          ]
        }
      ],
      "source": [
        "harry_potter_text = \" \"\n",
        "for i in range(4):\n",
        "    book_num = i+1\n",
        "    with open(f'/content/data/hp{book_num}.txt', 'r', encoding='utf-8') as f:\n",
        "        harry_potter_text += f.read()\n",
        "print(len(harry_potter_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTW6wJWB2SUl"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WTsFKtP2SUm"
      },
      "source": [
        "## Tokenization\n",
        "Instead of character level, we're going to model this LM using a tokenizer. in particular, we're going to try to use OpenAI's tiktoken with the gpt2 50k tokenizer. This might end up being too large of a vocab size given compute constraints, but"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "jNAlYTIf2SUm"
      },
      "outputs": [],
      "source": [
        "vocab = tokenmonster.load(\"fiction-2048-consistent-v1\")\n",
        "tokens = vocab.tokenize(\"This is a test.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "ya_OlheL2SUm",
        "outputId": "105fc94e-c1b3-4188-8a0b-d49a150fd57c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([138, 918, 108, 318, 202,  17], dtype=uint16)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "nZ0pfOIT2SUm"
      },
      "outputs": [],
      "source": [
        "token_example = vocab.tokenize(\"hello world test monster tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "y1y5eYUD2SUn",
        "outputId": "325e4c72-fa7b-475d-db06-958482abe304",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 37, 445, 174, 785, 318, 202, 465, 547, 321, 169, 181, 218,  62],\n",
              "      dtype=uint16)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "token_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "-qyadxmt2SUn",
        "outputId": "401894e5-6261-461f-f2e6-a571f5b7e1f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " ' hel',\n",
              " 'lo',\n",
              " ' world',\n",
              " ' te',\n",
              " 'st',\n",
              " ' mon',\n",
              " 'ster',\n",
              " ' to',\n",
              " 'ke',\n",
              " 'ni',\n",
              " 'ze',\n",
              " 'r']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "[vocab.decode([token]) for token in token_example]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "2ks4lD6p2SUn"
      },
      "outputs": [],
      "source": [
        "tokens = np.array(vocab.tokenize(harry_potter_text), dtype=np.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "WH_4N1qN2SUn",
        "outputId": "265a430e-e2da-4177-98f3-84c693b7fe29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1050223]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "dataset = torch.tensor(tokens, dtype=torch.long)\n",
        "print(dataset.shape, dataset.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "rOqbUh7U2SUn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95f20675-a588-4028-cf52-3458465395ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Harry Potter and the Sorcerer's Stone\n",
            "\n",
            "\n",
            "CHAPTER ONE\n",
            "\n",
            "THE BOY WHO LIVED\n",
            "\n",
            "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say\n",
            "that they were perfectly normal, thank you very much. They were the last\n",
            "people you'd expect to be\n",
            "rry muttered.  \"Listen, you'd better go and get someone -\"\n",
            "\"Dumbledore!\" gasped Mr. Crouch. He reached out and seized a handful of Harrys robes, dragging him closer, though his eyes were staring over Harry's head.  \"I need... see\n",
            " Harry.\n",
            "\"Would Harry Potter like a cup of tea?\" he squeaked loudly, over Winky's sobs.\n",
            "\"Er - yeah, okay,\" said Harry.\n",
            "Instantly, about six house-elves came trotting up behind him, bearing a large silver tray laden with a te\n"
          ]
        }
      ],
      "source": [
        "train_size = int(len(dataset) * 0.8)\n",
        "test_size = int(len(dataset) * 0.1)\n",
        "val_size = len(dataset) - train_size - test_size\n",
        "\n",
        "train_data, test_data, val_data = dataset[:train_size], dataset[train_size:train_size+test_size], dataset[train_size+test_size:]\n",
        "\n",
        "train_block = torch.tensor([train_data[i] for i in range(100)])\n",
        "train_list = train_block.tolist()\n",
        "print(vocab.decode(train_list))\n",
        "\n",
        "val_block = torch.tensor([val_data[i] for i in range(100)])\n",
        "val_list = val_block.tolist()\n",
        "print(vocab.decode(val_list))\n",
        "\n",
        "test_block = torch.tensor([test_data[i] for i in range(100)])\n",
        "test_list = test_block.tolist()\n",
        "print(vocab.decode(test_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "VoWJLYvs2SUo",
        "outputId": "bc71d209-09ab-43be-e08f-1396e3a89c00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train set size: 840178, test: 105022, val: 105023\n"
          ]
        }
      ],
      "source": [
        "print(f\"train set size: {train_size}, test: {test_size}, val: {val_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "YKvfgm5e2SUo",
        "outputId": "b6139d23-b1e8-4076-bbc9-cc8b90d69b10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 8]) torch.Size([32, 8])\n",
            "when input is [64, 210] the target: 53\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(10000)\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(data):\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])  # for any sequence x, the target y will be the next 8 tokens\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch(train_data)\n",
        "\n",
        "print(xb.shape, yb.shape)\n",
        "\n",
        "context = xb[0, :2]\n",
        "target = yb[0,1]\n",
        "print(f\"when input is {context.tolist()} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "RbbeNPTF2SUo"
      },
      "outputs": [],
      "source": [
        "class HPDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of possible sequences\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Fetch a single sequence x and its corresponding target y\n",
        "        x = self.data[idx:idx + self.block_size]\n",
        "        y = self.data[idx + 1:idx + self.block_size + 1]\n",
        "        return x, y\n",
        "\n",
        "block_size = 8\n",
        "train_dataset, val_dataset, test_dataset = HPDataset(train_data, block_size), HPDataset(val_data, block_size), HPDataset(test_data, block_size)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader, val_loader, test_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True), DataLoader(val_dataset, batch_size=batch_size, shuffle=True), DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "H9AWjLIa2SUo",
        "outputId": "e5b608d1-9807-4344-d43e-c45a9e958a94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26256\n",
            "3282\n",
            "3282\n"
          ]
        }
      ],
      "source": [
        "print(len(train_loader))\n",
        "print(len(test_loader))\n",
        "print(len(val_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "9D0dkIBU2SUo"
      },
      "outputs": [],
      "source": [
        "class zeptoGPT(nn.Module):\n",
        "    \"\"\"\n",
        "    zepto because it's a really small GPT\n",
        "    \"\"\"\n",
        "    def __init__(self, d_k, d_model, d_v, d_ff, num_heads, num_layers, vocab_size, dropout=0.1) -> None:\n",
        "        super().__init__()\n",
        "        self.decoder_transformer = Transformer(d_k, d_model, d_v, d_ff, num_heads, num_layers, vocab_size=vocab_size, mask=True, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.decoder_transformer(x)\n",
        "        return self.fc(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "oHC-Secr2SUo"
      },
      "outputs": [],
      "source": [
        "def compute_loss(y_target, y_pred, loss_function):\n",
        "    B, T, C = y_pred.shape\n",
        "    y_pred = y_pred.view(B*T, C)\n",
        "    _, max_indices = torch.max(y_pred, dim=1)\n",
        "    y_target_list = y_target.tolist()\n",
        "    max_indices = max_indices.tolist()\n",
        "    y_target = y_target.view(B*T)\n",
        "    return loss_function(y_pred, y_target)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, prompt: str, n = 50):\n",
        "  prompt_tokenized = vocab.encode(prompt)\n",
        "  for i in range(n):\n",
        "\n",
        "    prompt_tensor = torch.tensor(prompt, dtype=torch.)\n",
        "    next_token = model(prompt)"
      ],
      "metadata": {
        "id": "5Pc9PN-Hnrv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_token(model, block):\n",
        "  with torch.no_grad():\n",
        "    y_pred = model(block)\n",
        "    token_probs = nn.functional.softmax(y_pred, dim=-1)\n",
        "    _, max_idx = torch.max(token_probs)\n",
        "  return max_idx"
      ],
      "metadata": {
        "id": "XQ7fkR1zJlI4"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "W2wS6j8Y2SUo"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, loss_function, optim, epochs, device):\n",
        "    losses = [] #group losses for loss visualization\n",
        "    running_loss = 0.0\n",
        "    val_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        print(\"Epoch %d / %d\" % (epoch+1, epochs))\n",
        "        print(\"-\"*10)\n",
        "\n",
        "        for i, batch_data in enumerate(train_loader):\n",
        "            x, y = batch_data\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_pred = model(x)\n",
        "\n",
        "            loss = compute_loss(y, y_pred, loss_function)\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            running_loss += loss.item()\n",
        "            losses.append(loss)\n",
        "\n",
        "            if (i+1) % 1000 == 0:\n",
        "                print(\"Step: {}, average training loss over last 1000 steps: {:.4f}\".format(i+1, running_loss/1000))\n",
        "                running_loss = 0.0\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            correct_pred = 0.0\n",
        "            num_samples = 0\n",
        "            for i, batch_data in enumerate(val_loader):\n",
        "                (y, x) = batch_data\n",
        "                y, x = y.to(device), x.to(device)\n",
        "                y_pred = model(x)\n",
        "                loss = compute_loss(y, y_pred, loss_function)\n",
        "                _, predicted_labels = torch.max(y_pred, 1)\n",
        "                num_samples+=predicted_labels.shape[0]\n",
        "                val_loss += loss.item()\n",
        "\n",
        "            val_losses.append(val_loss)\n",
        "        print(\"Epoch: {}, validation loss: {:.4f}\".format(epoch+1, val_loss/len(val_loader)))\n",
        "\n",
        "    return losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "yj_S3bSm2SUo"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 1e-3\n",
        "NUM_EPOCHS = 1\n",
        "DROPOUT = 0.2\n",
        "D_MODEL = 512\n",
        "NUM_HEADS = 8\n",
        "D_K = int(D_MODEL / NUM_HEADS)\n",
        "D_V = D_K\n",
        "D_FF = D_MODEL * 4\n",
        "NUM_LAYERS = 4\n",
        "VOCAB_SIZE = vocab.vocab_size\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "f314V3J12SUo"
      },
      "outputs": [],
      "source": [
        "model = zeptoGPT(D_K, D_MODEL, D_V, D_FF, num_heads=NUM_HEADS, num_layers=NUM_LAYERS, vocab_size=VOCAB_SIZE)\n",
        "model = model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "iqdfCB9C2SUo"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWmZ65uO6XhP",
        "outputId": "ba48149a-e391-4c2b-d382-75f8aea6abfc"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "RxStEdDG2SUp",
        "outputId": "1db22fa5-0868-48ee-ab72-19d3d0075299",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 1\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 4.5961\n",
            "Step: 2000, average training loss over last 1000 steps: 4.2461\n",
            "Step: 3000, average training loss over last 1000 steps: 4.1655\n",
            "Step: 4000, average training loss over last 1000 steps: 4.1116\n",
            "Step: 5000, average training loss over last 1000 steps: 4.0639\n",
            "Step: 6000, average training loss over last 1000 steps: 4.0293\n",
            "Step: 7000, average training loss over last 1000 steps: 4.0151\n",
            "Step: 8000, average training loss over last 1000 steps: 3.9905\n",
            "Step: 9000, average training loss over last 1000 steps: 3.9744\n",
            "Step: 10000, average training loss over last 1000 steps: 3.9454\n",
            "Step: 11000, average training loss over last 1000 steps: 3.9342\n",
            "Step: 12000, average training loss over last 1000 steps: 3.9155\n",
            "Step: 13000, average training loss over last 1000 steps: 3.8929\n",
            "Step: 14000, average training loss over last 1000 steps: 3.8944\n",
            "Step: 15000, average training loss over last 1000 steps: 3.8736\n",
            "Step: 16000, average training loss over last 1000 steps: 3.8609\n",
            "Step: 17000, average training loss over last 1000 steps: 3.8551\n",
            "Step: 18000, average training loss over last 1000 steps: 3.8382\n",
            "Step: 19000, average training loss over last 1000 steps: 3.8356\n",
            "Step: 20000, average training loss over last 1000 steps: 3.8199\n",
            "Step: 21000, average training loss over last 1000 steps: 3.8170\n",
            "Step: 22000, average training loss over last 1000 steps: 3.8004\n",
            "Step: 23000, average training loss over last 1000 steps: 3.7948\n",
            "Step: 24000, average training loss over last 1000 steps: 3.7861\n",
            "Step: 25000, average training loss over last 1000 steps: 3.7849\n",
            "Step: 26000, average training loss over last 1000 steps: 3.7818\n",
            "Epoch: 1, validation loss: 8.9176\n"
          ]
        }
      ],
      "source": [
        "train_loss, val_loss = train(model, train_loader, val_loader, torch.nn.functional.cross_entropy, optimizer, NUM_EPOCHS, DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_sample = train_data"
      ],
      "metadata": {
        "id": "3m2I7HS8MO8M"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_sample[0])\n",
        "test_block = torch.tensor([text_sample[i] for i in range(8)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXfqcgw2MYSz",
        "outputId": "363ec287-76ec-461b-cf41-69483c085f50"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(36)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_block"
      ],
      "metadata": {
        "id": "nJXjZzCnMoSY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d318be4-7c31-4753-da1f-4850138d7786"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 36, 264,  62, 196,  36, 301,  64, 382])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_list = test_block.tolist()"
      ],
      "metadata": {
        "id": "m0uewcO7MwY_"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.decode(test_list)"
      ],
      "metadata": {
        "id": "P8mbTFkAMpic",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0ff330af-e4b4-4cbe-d576-e35d98cca75f"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Harry Potter'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_block = torch.tensor([text_sample[i] for i in range(100)])\n",
        "test_list = test_block.tolist()\n",
        "vocab.decode(test_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "aAN0uzbYhKa8",
        "outputId": "8047cbcb-9f08-47fd-da2b-2a6f881f6121"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Harry Potter and the Sorcerer's Stone\\n\\n\\nCHAPTER ONE\\n\\nTHE BOY WHO LIVED\\n\\nMr. and Mrs. Dursley, of number four, Privet Drive, were proud to say\\nthat they were perfectly normal, thank you very much. They were the last\\npeople you'd expect to be\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XrBrg4lRhOuv"
      },
      "execution_count": 74,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}