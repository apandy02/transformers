{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aryamanpandya99/Transformers/blob/main/notebooks/decoder_only_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7GuoX-D2SUi"
      },
      "source": [
        "# Decoder-only based GPT (language model)\n",
        "\n",
        "Here we take a transformer block, the decoder in particular, and use it for the task of language modeling. In general, this is how GPTs are trained. We will do this on a much smaller scale.\n",
        "\n",
        "We take everything we've already built and leverage it in the way Karpathy implements a character level LM here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ptBhD9c3kTU",
        "outputId": "63f14505-2ec8-4f41-e6bc-74a7fa708b9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DAnCCtxa3i6o"
      },
      "outputs": [],
      "source": [
        "!cp drive/MyDrive/Transformers/models/transformer_blocks.py .\n",
        "!cp drive/MyDrive/Transformers/models/modules.py .\n",
        "!cp -r drive/MyDrive/Transformers/data/ ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_XqFHTe4Ozn",
        "outputId": "7dcbc20b-d90e-4007-ffc3-abc0506e3a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenmonster\n",
            "  Downloading tokenmonster-1.1.12.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: tokenmonster\n",
            "  Building wheel for tokenmonster (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tokenmonster: filename=tokenmonster-1.1.12-py3-none-any.whl size=15820 sha256=8567d49bd07594f923aa011acce53a1355875c31f14c9be8719056853a4de44f\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/49/56/9db5eb8fd22ea838f03cc48cc4e096d0f1e810dff3e4559abe\n",
            "Successfully built tokenmonster\n",
            "Installing collected packages: tokenmonster\n",
            "Successfully installed tokenmonster-1.1.12\n"
          ]
        }
      ],
      "source": [
        "!pip install tokenmonster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "p0LzXILI2SUk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from torch.utils.data import random_split\n",
        "import sys\n",
        "sys.path.append(\"~/\")\n",
        "from transformer_blocks import Transformer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tokenmonster"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "2fEKfFTtjVa5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UP_BMiE52SUl",
        "outputId": "26f0aaee-4baa-4e32-9867-ef8d1af3288f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2652650\n"
          ]
        }
      ],
      "source": [
        "harry_potter_text = \" \"\n",
        "for i in range(4):\n",
        "    book_num = i+1\n",
        "    with open(f'/content/data/hp{book_num}.txt', 'r', encoding='utf-8') as f:\n",
        "        harry_potter_text += f.read()\n",
        "print(len(harry_potter_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTW6wJWB2SUl"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WTsFKtP2SUm"
      },
      "source": [
        "## Tokenization\n",
        "Instead of character level, we're going to model this LM using a tokenizer. in particular, we're going to try to use OpenAI's tiktoken with the gpt2 50k tokenizer. This might end up being too large of a vocab size given compute constraints, but"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jNAlYTIf2SUm"
      },
      "outputs": [],
      "source": [
        "vocab = tokenmonster.load(\"fiction-1024-consistent-v1\")\n",
        "tokens = vocab.tokenize(\"This is a test.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya_OlheL2SUm",
        "outputId": "d5e1bd1e-2672-47e6-9488-7ba8c28a2153"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([138, 918, 108, 318, 202,  17], dtype=uint16)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nZ0pfOIT2SUm"
      },
      "outputs": [],
      "source": [
        "token_example = vocab.tokenize(\"hello world test monster tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1y5eYUD2SUn",
        "outputId": "a0ef8edf-ffe7-4934-8d2a-069b2bc106e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 37, 445, 174, 785, 318, 202, 465, 547, 321, 169, 181, 218,  62],\n",
              "      dtype=uint16)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "token_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qyadxmt2SUn",
        "outputId": "73f4164c-9da3-491a-c6a4-5f2f7b789f76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " ' hel',\n",
              " 'lo',\n",
              " ' world',\n",
              " ' te',\n",
              " 'st',\n",
              " ' mon',\n",
              " 'ster',\n",
              " ' to',\n",
              " 'ke',\n",
              " 'ni',\n",
              " 'ze',\n",
              " 'r']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "[vocab.decode([token]) for token in token_example]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2ks4lD6p2SUn"
      },
      "outputs": [],
      "source": [
        "tokens = np.array(vocab.tokenize(harry_potter_text), dtype=np.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH_4N1qN2SUn",
        "outputId": "bf7fe23f-ba4a-4d32-ae00-afd1ce867b56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1050223]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "dataset = torch.tensor(tokens, dtype=torch.long)\n",
        "print(dataset.shape, dataset.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbbeNPTF2SUo",
        "outputId": "e32d1fea-ae04-467c-cbdd-bf08aee729df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Harry Potter and the Sorcerer's Stone\n",
            "\n",
            "\n",
            "CHAPTER ONE\n",
            "\n",
            "THE BOY WHO LIVED\n",
            "\n",
            "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say\n",
            "that they were perfectly normal, thank you very much. They were the last\n",
            "people you'd expect to be\n",
            "train set size: 840158, test: 105019, val: 105021, data size: 1050223, dataset_size: 1050198\n"
          ]
        }
      ],
      "source": [
        "class HPDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of possible sequences\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Fetch a single sequence x and its corresponding target y\n",
        "        x = self.data[idx:idx + self.block_size]\n",
        "        y = self.data[idx + 1:idx + self.block_size + 1]\n",
        "        return x, y\n",
        "\n",
        "BLOCK_SIZE = 25\n",
        "hp_data = HPDataset(dataset, BLOCK_SIZE)\n",
        "\n",
        "test_block = torch.tensor([dataset[i] for i in range(100)])\n",
        "test_list = test_block.tolist()\n",
        "print(vocab.decode(test_list))\n",
        "\n",
        "train_size = int(len(hp_data) * 0.8)\n",
        "test_size = int(len(hp_data) * 0.1)\n",
        "val_size = len(hp_data) - train_size - test_size\n",
        "\n",
        "print(f\"train set size: {train_size}, test: {test_size}, val: {val_size}, data size: {len(dataset)}, dataset_size: {hp_data.__len__()}\")\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(hp_data, [train_size, val_size, test_size])\n",
        "\n",
        "batch_size = 64\n",
        "train_loader, val_loader, test_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True), DataLoader(val_dataset, batch_size=batch_size, shuffle=True), DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9AWjLIa2SUo",
        "outputId": "31919c44-0484-4b2f-efee-9715b657b898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13128\n",
            "1641\n",
            "1641\n"
          ]
        }
      ],
      "source": [
        "print(len(train_loader))\n",
        "print(len(test_loader))\n",
        "print(len(val_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5k9W9isInum",
        "outputId": "92355f01-7269-4503-ccd9-4d98e35b0ccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([262,  58, 135, 273, 443, 114,  62,  37, 477, 765, 766, 271, 280,  58,\n",
            "         51, 205, 271, 606, 108, 103, 678, 135, 123,  37, 286]), tensor([ 58, 135, 273, 443, 114,  62,  37, 477, 765, 766, 271, 280,  58,  51,\n",
            "        205, 271, 606, 108, 103, 678, 135, 123,  37, 286, 169]))\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset.__getitem__(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9D0dkIBU2SUo"
      },
      "outputs": [],
      "source": [
        "class rowlingGPT(nn.Module):\n",
        "    \"\"\"\n",
        "    JK Rowling would probably not approve\n",
        "    \"\"\"\n",
        "    def __init__(self, d_k, d_model, d_v, d_ff, num_heads, num_layers, vocab_size, dropout=0.1) -> None:\n",
        "        super().__init__()\n",
        "        self.decoder_transformer = Transformer(d_k, d_model, d_v, d_ff, num_heads, num_layers, vocab_size=vocab_size, mask=True, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.decoder_transformer(x)\n",
        "        return self.fc(self.layer_norm(out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "oHC-Secr2SUo"
      },
      "outputs": [],
      "source": [
        "def compute_loss(y_target, y_pred, loss_function):\n",
        "    B, T, C = y_pred.shape\n",
        "    y_pred = y_pred.view(B*T, C)\n",
        "    _, max_indices = torch.max(y_pred, dim=1)\n",
        "    y_target_list = y_target.tolist()\n",
        "    max_indices = max_indices.tolist()\n",
        "    y_target = y_target.view(B*T)\n",
        "    return loss_function(y_pred, y_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "5Pc9PN-Hnrv-"
      },
      "outputs": [],
      "source": [
        "def generate(model, prompt: str, device,n = 200, block_size=BLOCK_SIZE):\n",
        "  prompt_array = vocab.tokenize(prompt)\n",
        "  prompt_array = np.array(prompt_array[:block_size], dtype=np.int16)\n",
        "  decoded = vocab.decode(prompt_array)\n",
        "  print(f\"prompt: {decoded}\")\n",
        "  cumulative_array = prompt_array\n",
        "  for i in range(n):\n",
        "    prompt_tensor = torch.tensor(prompt_array, dtype=torch.long).to(device)\n",
        "    next_token = predict_next_token(model, prompt_tensor.unsqueeze(0))\n",
        "    next_token_np = next_token.cpu().numpy().flatten()\n",
        "    cumulative_array = np.append(cumulative_array, next_token_np)\n",
        "    prompt_array = np.append(prompt_array[1:], next_token_np)\n",
        "    test_list = cumulative_array.tolist()\n",
        "  print(vocab.decode(test_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XQ7fkR1zJlI4"
      },
      "outputs": [],
      "source": [
        "def predict_next_token(model, block):\n",
        "  with torch.no_grad():\n",
        "    y_pred = model(block)\n",
        "    token_probs = nn.functional.softmax(y_pred, dim=-1)\n",
        "    _, max_idx = torch.max(token_probs, dim=-1)\n",
        "  return max_idx.squeeze()[-1]  # return only the last next token prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "W2wS6j8Y2SUo"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, loss_function, optim, epochs, device):\n",
        "    losses = [] #group losses for loss visualization\n",
        "    running_loss = 0.0\n",
        "    val_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        print(\"Epoch %d / %d\" % (epoch+1, epochs))\n",
        "        print(\"-\"*10)\n",
        "\n",
        "        for i, batch_data in enumerate(train_loader):\n",
        "            x, y = batch_data\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_pred = model(x)\n",
        "\n",
        "            loss = compute_loss(y, y_pred, loss_function)\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            running_loss += loss.item()\n",
        "            losses.append(loss)\n",
        "\n",
        "            if (i+1) % 1000 == 0:\n",
        "                print(\"Step: {}, average training loss over last 1000 steps: {:.4f}\".format(i+1, running_loss/1000))\n",
        "                running_loss = 0.0\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for i, batch_data in enumerate(val_loader):\n",
        "                (y, x) = batch_data\n",
        "                y, x = y.to(device), x.to(device)\n",
        "                y_pred = model(x)\n",
        "                loss = compute_loss(y, y_pred, loss_function)\n",
        "                _, predicted_labels = torch.max(y_pred, 1)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "            val_losses.append(val_loss)\n",
        "        print(\"Epoch: {}, validation loss: {:.4f}\".format(epoch+1, val_loss/len(val_loader)))\n",
        "        print(\"Generated text: \")\n",
        "        generate(model, \"Harry\", device=DEVICE, n=20)\n",
        "\n",
        "    return losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "yj_S3bSm2SUo"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 6e-4\n",
        "NUM_EPOCHS = 18\n",
        "DROPOUT = 0.2\n",
        "D_MODEL = 1024\n",
        "NUM_HEADS = 8\n",
        "D_K = int(D_MODEL / NUM_HEADS)\n",
        "D_V = D_K\n",
        "D_FF = D_MODEL * 4\n",
        "NUM_LAYERS = 2\n",
        "VOCAB_SIZE = vocab.vocab_size\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "f314V3J12SUo"
      },
      "outputs": [],
      "source": [
        "model = rowlingGPT(D_K, D_MODEL, D_V, D_FF, num_heads=NUM_HEADS, num_layers=NUM_LAYERS, vocab_size=VOCAB_SIZE)\n",
        "model = model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "iqdfCB9C2SUo"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWmZ65uO6XhP",
        "outputId": "eac1924c-b0e5-452b-c622-f397a4afb7b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxStEdDG2SUp",
        "outputId": "81050059-7323-4cc7-c80c-67e36ae1934a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 4.2001\n",
            "Step: 2000, average training loss over last 1000 steps: 3.9038\n",
            "Step: 3000, average training loss over last 1000 steps: 3.8369\n",
            "Step: 4000, average training loss over last 1000 steps: 3.7989\n",
            "Step: 5000, average training loss over last 1000 steps: 3.7686\n",
            "Step: 6000, average training loss over last 1000 steps: 3.7438\n",
            "Step: 7000, average training loss over last 1000 steps: 3.7254\n",
            "Step: 8000, average training loss over last 1000 steps: 3.7015\n",
            "Step: 9000, average training loss over last 1000 steps: 3.6890\n",
            "Step: 10000, average training loss over last 1000 steps: 3.6700\n",
            "Step: 11000, average training loss over last 1000 steps: 3.6531\n",
            "Step: 12000, average training loss over last 1000 steps: 3.6410\n",
            "Step: 13000, average training loss over last 1000 steps: 3.6337\n",
            "Epoch: 1, validation loss: 8.6657\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, and\n",
            "Harry, and\n",
            "Harry, and Ha\n",
            "Harry, and Har\n",
            "Harry, and Harry\n",
            "Harry, and Harry's\n",
            "Harry, and Harry's face\n",
            "Harry, and Harry's face,\n",
            "Harry, and Harry's face, and\n",
            "Harry, and Harry's face, and\n",
            "Harry, and Harry's face, and Ha\n",
            "Harry, and Harry's face, and Har\n",
            "Harry, and Harry's face, and Harry\n",
            "Harry, and Harry's face, and Harry's\n",
            "Harry, and Harry's face, and Harry's face\n",
            "Harry, and Harry's face, and Harry's face,\n",
            "Harry, and Harry's face, and Harry's face, and\n",
            "Harry, and Harry's face, and Harry's face, and\n",
            "Harry, and Harry's face, and Harry's face, and Ha\n",
            "Epoch 2 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 4.0764\n",
            "Step: 2000, average training loss over last 1000 steps: 3.6060\n",
            "Step: 3000, average training loss over last 1000 steps: 3.5932\n",
            "Step: 4000, average training loss over last 1000 steps: 3.5846\n",
            "Step: 5000, average training loss over last 1000 steps: 3.5751\n",
            "Step: 6000, average training loss over last 1000 steps: 3.5681\n",
            "Step: 7000, average training loss over last 1000 steps: 3.5602\n",
            "Step: 8000, average training loss over last 1000 steps: 3.5508\n",
            "Step: 9000, average training loss over last 1000 steps: 3.5421\n",
            "Step: 10000, average training loss over last 1000 steps: 3.5326\n",
            "Step: 11000, average training loss over last 1000 steps: 3.5287\n",
            "Step: 12000, average training loss over last 1000 steps: 3.5236\n",
            "Step: 13000, average training loss over last 1000 steps: 3.5141\n",
            "Epoch: 2, validation loss: 8.8691\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, and\n",
            "Harry, and\n",
            "Harry, and Her\n",
            "Harry, and Her\n",
            "Harry, and Hermi\n",
            "Harry, and Hermione\n",
            "Harry, and Hermione,\n",
            "Harry, and Hermione, and\n",
            "Harry, and Hermione, and\n",
            "Harry, and Hermione, and Her\n",
            "Harry, and Hermione, and Her\n",
            "Harry, and Hermione, and Hermi\n",
            "Harry, and Hermione, and Hermione\n",
            "Harry, and Hermione, and Hermione,\n",
            "Harry, and Hermione, and Hermione, and\n",
            "Harry, and Hermione, and Hermione, and\n",
            "Harry, and Hermione, and Hermione, and Her\n",
            "Harry, and Hermione, and Hermione, and Her\n",
            "Harry, and Hermione, and Hermione, and Hermi\n",
            "Epoch 3 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.9493\n",
            "Step: 2000, average training loss over last 1000 steps: 3.4997\n",
            "Step: 3000, average training loss over last 1000 steps: 3.4938\n",
            "Step: 4000, average training loss over last 1000 steps: 3.4851\n",
            "Step: 5000, average training loss over last 1000 steps: 3.4834\n",
            "Step: 6000, average training loss over last 1000 steps: 3.4731\n",
            "Step: 7000, average training loss over last 1000 steps: 3.4704\n",
            "Step: 8000, average training loss over last 1000 steps: 3.4639\n",
            "Step: 9000, average training loss over last 1000 steps: 3.4601\n",
            "Step: 10000, average training loss over last 1000 steps: 3.4471\n",
            "Step: 11000, average training loss over last 1000 steps: 3.4480\n",
            "Step: 12000, average training loss over last 1000 steps: 3.4413\n",
            "Step: 13000, average training loss over last 1000 steps: 3.4370\n",
            "Epoch: 3, validation loss: 9.0126\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, who\n",
            "Harry, who was\n",
            "Harry, who was still\n",
            "Harry, who was still s\n",
            "Harry, who was still s\n",
            "Harry, who was still smil\n",
            "Harry, who was still smiling\n",
            "Harry, who was still smiling the\n",
            "Harry, who was still smiling the\n",
            "Harry, who was still smiling the S\n",
            "Harry, who was still smiling the S\n",
            "Harry, who was still smiling the Spi\n",
            "Harry, who was still smiling the Spider\n",
            "Harry, who was still smiling the Spiders\n",
            "Harry, who was still smiling the Spiders were\n",
            "Harry, who was still smiling the Spiders were\n",
            "\n",
            "Harry, who was still smiling the Spiders were\n",
            "the\n",
            "Harry, who was still smiling the Spiders were\n",
            "they\n",
            "Harry, who was still smiling the Spiders were\n",
            "they'\n",
            "Epoch 4 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.8673\n",
            "Step: 2000, average training loss over last 1000 steps: 3.4232\n",
            "Step: 3000, average training loss over last 1000 steps: 3.4190\n",
            "Step: 4000, average training loss over last 1000 steps: 3.4133\n",
            "Step: 5000, average training loss over last 1000 steps: 3.4082\n",
            "Step: 6000, average training loss over last 1000 steps: 3.4040\n",
            "Step: 7000, average training loss over last 1000 steps: 3.4027\n",
            "Step: 8000, average training loss over last 1000 steps: 3.3980\n",
            "Step: 9000, average training loss over last 1000 steps: 3.3960\n",
            "Step: 10000, average training loss over last 1000 steps: 3.3918\n",
            "Step: 11000, average training loss over last 1000 steps: 3.3856\n",
            "Step: 12000, average training loss over last 1000 steps: 3.3818\n",
            "Step: 13000, average training loss over last 1000 steps: 3.3747\n",
            "Epoch: 4, validation loss: 9.1167\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, and\n",
            "Harry, and\n",
            "Harry, and Her\n",
            "Harry, and Her\n",
            "Harry, and Hermi\n",
            "Harry, and Hermione\n",
            "Harry, and Hermione,\n",
            "Harry, and Hermione, however,\n",
            "Harry, and Hermione, however, was\n",
            "Harry, and Hermione, however, wasn'\n",
            "Harry, and Hermione, however, wasn't\n",
            "Harry, and Hermione, however, wasn't\n",
            "\n",
            "Harry, and Hermione, however, wasn't\n",
            "the\n",
            "Harry, and Hermione, however, wasn't\n",
            "they\n",
            "Harry, and Hermione, however, wasn't\n",
            "they'\n",
            "Harry, and Hermione, however, wasn't\n",
            "they're\n",
            "Harry, and Hermione, however, wasn't\n",
            "they're not\n",
            "Harry, and Hermione, however, wasn't\n",
            "they're not to\n",
            "Harry, and Hermione, however, wasn't\n",
            "they're not to\n",
            "Epoch 5 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.7930\n",
            "Step: 2000, average training loss over last 1000 steps: 3.3635\n",
            "Step: 3000, average training loss over last 1000 steps: 3.3637\n",
            "Step: 4000, average training loss over last 1000 steps: 3.3614\n",
            "Step: 5000, average training loss over last 1000 steps: 3.3566\n",
            "Step: 6000, average training loss over last 1000 steps: 3.3497\n",
            "Step: 7000, average training loss over last 1000 steps: 3.3484\n",
            "Step: 8000, average training loss over last 1000 steps: 3.3401\n",
            "Step: 9000, average training loss over last 1000 steps: 3.3449\n",
            "Step: 10000, average training loss over last 1000 steps: 3.3353\n",
            "Step: 11000, average training loss over last 1000 steps: 3.3373\n",
            "Step: 12000, average training loss over last 1000 steps: 3.3350\n",
            "Step: 13000, average training loss over last 1000 steps: 3.3349\n",
            "Epoch: 5, validation loss: 9.2054\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, who\n",
            "Harry, who was\n",
            "Harry, who was looking\n",
            "Harry, who was looking at\n",
            "Harry, who was looking at\n",
            "Harry, who was looking at Ha\n",
            "Harry, who was looking at Har\n",
            "Harry, who was looking at Harry\n",
            "Harry, who was looking at Harry's\n",
            "Harry, who was looking at Harry's face\n",
            "Harry, who was looking at Harry's face,\n",
            "Harry, who was looking at Harry's face, and\n",
            "Harry, who was looking at Harry's face, and\n",
            "Harry, who was looking at Harry's face, and Ha\n",
            "Harry, who was looking at Harry's face, and Har\n",
            "Harry, who was looking at Harry's face, and Harry\n",
            "Harry, who was looking at Harry's face, and Harry's\n",
            "Harry, who was looking at Harry's face, and Harry's face\n",
            "Harry, who was looking at Harry's face, and Harry's face,\n",
            "Epoch 6 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.7465\n",
            "Step: 2000, average training loss over last 1000 steps: 3.3186\n",
            "Step: 3000, average training loss over last 1000 steps: 3.3207\n",
            "Step: 4000, average training loss over last 1000 steps: 3.3133\n",
            "Step: 5000, average training loss over last 1000 steps: 3.3110\n",
            "Step: 6000, average training loss over last 1000 steps: 3.3105\n",
            "Step: 7000, average training loss over last 1000 steps: 3.3070\n",
            "Step: 8000, average training loss over last 1000 steps: 3.3002\n",
            "Step: 9000, average training loss over last 1000 steps: 3.3027\n",
            "Step: 10000, average training loss over last 1000 steps: 3.2992\n",
            "Step: 11000, average training loss over last 1000 steps: 3.2968\n",
            "Step: 12000, average training loss over last 1000 steps: 3.2937\n",
            "Step: 13000, average training loss over last 1000 steps: 3.2946\n",
            "Epoch: 6, validation loss: 9.2810\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, and\n",
            "Harry, and\n",
            "Harry, and Ro\n",
            "Harry, and Ron\n",
            "Harry, and Ron,\n",
            "Harry, and Ron, who\n",
            "Harry, and Ron, who was\n",
            "Harry, and Ron, who was sit\n",
            "Harry, and Ron, who was sitting\n",
            "Harry, and Ron, who was sitting in the\n",
            "Harry, and Ron, who was sitting in the cor\n",
            "Harry, and Ron, who was sitting in the corner\n",
            "Harry, and Ron, who was sitting in the corner of the\n",
            "Harry, and Ron, who was sitting in the corner of the\n",
            "Harry, and Ron, who was sitting in the corner of the Do\n",
            "Harry, and Ron, who was sitting in the corner of the Dob\n",
            "Harry, and Ron, who was sitting in the corner of the Dobb\n",
            "Harry, and Ron, who was sitting in the corner of the Dobbb\n",
            "Harry, and Ron, who was sitting in the corner of the Dobbby\n",
            "Epoch 7 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.7002\n",
            "Step: 2000, average training loss over last 1000 steps: 3.2798\n",
            "Step: 3000, average training loss over last 1000 steps: 3.2853\n",
            "Step: 4000, average training loss over last 1000 steps: 3.2806\n",
            "Step: 5000, average training loss over last 1000 steps: 3.2755\n",
            "Step: 6000, average training loss over last 1000 steps: 3.2759\n",
            "Step: 7000, average training loss over last 1000 steps: 3.2730\n",
            "Step: 8000, average training loss over last 1000 steps: 3.2705\n",
            "Step: 9000, average training loss over last 1000 steps: 3.2686\n",
            "Step: 10000, average training loss over last 1000 steps: 3.2660\n",
            "Step: 11000, average training loss over last 1000 steps: 3.2654\n",
            "Step: 12000, average training loss over last 1000 steps: 3.2634\n",
            "Step: 13000, average training loss over last 1000 steps: 3.2604\n",
            "Epoch: 7, validation loss: 9.3400\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, and\n",
            "Harry, and\n",
            "Harry, and Ro\n",
            "Harry, and Ron\n",
            "Harry, and Ron,\n",
            "Harry, and Ron, and\n",
            "Harry, and Ron, and\n",
            "Harry, and Ron, and Ha\n",
            "Harry, and Ron, and Har\n",
            "Harry, and Ron, and Harry\n",
            "Harry, and Ron, and Harry's\n",
            "Harry, and Ron, and Harry's face\n",
            "Harry, and Ron, and Harry's face,\n",
            "Harry, and Ron, and Harry's face, and\n",
            "Harry, and Ron, and Harry's face, and\n",
            "Harry, and Ron, and Harry's face, and Ha\n",
            "Harry, and Ron, and Harry's face, and Har\n",
            "Harry, and Ron, and Harry's face, and Harry\n",
            "Harry, and Ron, and Harry's face, and Harry's\n",
            "Epoch 8 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.6655\n",
            "Step: 2000, average training loss over last 1000 steps: 3.2463\n",
            "Step: 3000, average training loss over last 1000 steps: 3.2519\n",
            "Step: 4000, average training loss over last 1000 steps: 3.2485\n",
            "Step: 5000, average training loss over last 1000 steps: 3.2486\n",
            "Step: 6000, average training loss over last 1000 steps: 3.2467\n",
            "Step: 7000, average training loss over last 1000 steps: 3.2474\n",
            "Step: 8000, average training loss over last 1000 steps: 3.2467\n",
            "Step: 9000, average training loss over last 1000 steps: 3.2422\n",
            "Step: 10000, average training loss over last 1000 steps: 3.2414\n",
            "Step: 11000, average training loss over last 1000 steps: 3.2357\n",
            "Step: 12000, average training loss over last 1000 steps: 3.2345\n",
            "Step: 13000, average training loss over last 1000 steps: 3.2362\n",
            "Epoch: 8, validation loss: 9.4134\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, and\n",
            "Harry, and\n",
            "Harry, and Ro\n",
            "Harry, and Ron\n",
            "Harry, and Ron,\n",
            "Harry, and Ron, who\n",
            "Harry, and Ron, who was\n",
            "Harry, and Ron, who was still\n",
            "Harry, and Ron, who was still looking\n",
            "Harry, and Ron, who was still looking at\n",
            "Harry, and Ron, who was still looking at\n",
            "Harry, and Ron, who was still looking at Ha\n",
            "Harry, and Ron, who was still looking at Har\n",
            "Harry, and Ron, who was still looking at Harry\n",
            "Harry, and Ron, who was still looking at Harry's\n",
            "Harry, and Ron, who was still looking at Harry's face\n",
            "Harry, and Ron, who was still looking at Harry's face,\n",
            "Harry, and Ron, who was still looking at Harry's face, and\n",
            "Harry, and Ron, who was still looking at Harry's face, and\n",
            "Epoch 9 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.6402\n",
            "Step: 2000, average training loss over last 1000 steps: 3.2201\n",
            "Step: 3000, average training loss over last 1000 steps: 3.2260\n",
            "Step: 4000, average training loss over last 1000 steps: 3.2266\n",
            "Step: 5000, average training loss over last 1000 steps: 3.2234\n",
            "Step: 6000, average training loss over last 1000 steps: 3.2224\n",
            "Step: 7000, average training loss over last 1000 steps: 3.2212\n",
            "Step: 8000, average training loss over last 1000 steps: 3.2208\n",
            "Step: 9000, average training loss over last 1000 steps: 3.2203\n",
            "Step: 10000, average training loss over last 1000 steps: 3.2155\n",
            "Step: 11000, average training loss over last 1000 steps: 3.2184\n",
            "Step: 12000, average training loss over last 1000 steps: 3.2123\n",
            "Step: 13000, average training loss over last 1000 steps: 3.2130\n",
            "Epoch: 9, validation loss: 9.4565\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry's\n",
            "Harry's face\n",
            "Harry's face.\n",
            "Harry's face. \"\n",
            "Harry's face. \"You\n",
            "Harry's face. \"You'\n",
            "Harry's face. \"You're\n",
            "Harry's face. \"You're here\n",
            "Harry's face. \"You're here.\"\n",
            "Harry's face. \"You're here.\"\n",
            "\n",
            "\n",
            "Harry's face. \"You're here.\"\n",
            "\n",
            "Ha\n",
            "Harry's face. \"You're here.\"\n",
            "\n",
            "Har\n",
            "Harry's face. \"You're here.\"\n",
            "\n",
            "Harry\n",
            "Harry's face. \"You're here.\"\n",
            "\n",
            "Harry was\n",
            "Harry's face. \"You're here.\"\n",
            "\n",
            "Harry wasn'\n",
            "Harry's face. \"You're here.\"\n",
            "\n",
            "Harry wasn't\n",
            "Harry's face. \"You're here.\"\n",
            "\n",
            "Harry wasn't going to\n",
            "Harry's face. \"You're here.\"\n",
            "\n",
            "Harry wasn't going to be\n",
            "Harry's face. \"You're here.\"\n",
            "\n",
            "Harry wasn't going to be any\n",
            "Harry's face. \"You're here.\"\n",
            "\n",
            "Harry wasn't going to be anyone\n",
            "Epoch 10 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.6166\n",
            "Step: 2000, average training loss over last 1000 steps: 3.2085\n",
            "Step: 3000, average training loss over last 1000 steps: 3.2033\n",
            "Step: 4000, average training loss over last 1000 steps: 3.2019\n",
            "Step: 5000, average training loss over last 1000 steps: 3.2060\n",
            "Step: 6000, average training loss over last 1000 steps: 3.2012\n",
            "Step: 7000, average training loss over last 1000 steps: 3.1987\n",
            "Step: 8000, average training loss over last 1000 steps: 3.1980\n",
            "Step: 9000, average training loss over last 1000 steps: 3.2002\n",
            "Step: 10000, average training loss over last 1000 steps: 3.2000\n",
            "Step: 11000, average training loss over last 1000 steps: 3.1949\n",
            "Step: 12000, average training loss over last 1000 steps: 3.1942\n",
            "Step: 13000, average training loss over last 1000 steps: 3.1917\n",
            "Epoch: 10, validation loss: 9.5025\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, and\n",
            "Harry, and\n",
            "Harry, and Ro\n",
            "Harry, and Ron\n",
            "Harry, and Ron,\n",
            "Harry, and Ron, who\n",
            "Harry, and Ron, who was\n",
            "Harry, and Ron, who was still\n",
            "Harry, and Ron, who was still there\n",
            "Harry, and Ron, who was still there,\n",
            "Harry, and Ron, who was still there,\n",
            "\n",
            "Harry, and Ron, who was still there,\n",
            "a\n",
            "Harry, and Ron, who was still there,\n",
            "a\n",
            "Harry, and Ron, who was still there,\n",
            "acr\n",
            "Harry, and Ron, who was still there,\n",
            "acro\n",
            "Harry, and Ron, who was still there,\n",
            "across\n",
            "Harry, and Ron, who was still there,\n",
            "across the\n",
            "Harry, and Ron, who was still there,\n",
            "across the room\n",
            "Harry, and Ron, who was still there,\n",
            "across the room,\n",
            "Epoch 11 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.5922\n",
            "Step: 2000, average training loss over last 1000 steps: 3.1843\n",
            "Step: 3000, average training loss over last 1000 steps: 3.1842\n",
            "Step: 4000, average training loss over last 1000 steps: 3.1864\n",
            "Step: 5000, average training loss over last 1000 steps: 3.1849\n",
            "Step: 6000, average training loss over last 1000 steps: 3.1861\n",
            "Step: 7000, average training loss over last 1000 steps: 3.1866\n",
            "Step: 8000, average training loss over last 1000 steps: 3.1850\n",
            "Step: 9000, average training loss over last 1000 steps: 3.1823\n",
            "Step: 10000, average training loss over last 1000 steps: 3.1796\n",
            "Step: 11000, average training loss over last 1000 steps: 3.1824\n",
            "Step: 12000, average training loss over last 1000 steps: 3.1811\n",
            "Step: 13000, average training loss over last 1000 steps: 3.1753\n",
            "Epoch: 11, validation loss: 9.5444\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry's\n",
            "Harry's face\n",
            "Harry's face was\n",
            "Harry's face was still\n",
            "Harry's face was still\n",
            "\n",
            "Harry's face was still\n",
            "s\n",
            "Harry's face was still\n",
            "s\n",
            "Harry's face was still\n",
            "scar\n",
            "Harry's face was still\n",
            "scar\n",
            "Harry's face was still\n",
            "scarlet\n",
            "Harry's face was still\n",
            "scarlet's\n",
            "Harry's face was still\n",
            "scarlet's a\n",
            "Harry's face was still\n",
            "scarlet's a\n",
            "Harry's face was still\n",
            "scarlet's acr\n",
            "Harry's face was still\n",
            "scarlet's acro\n",
            "Harry's face was still\n",
            "scarlet's across\n",
            "Harry's face was still\n",
            "scarlet's across the\n",
            "Harry's face was still\n",
            "scarlet's across the room\n",
            "Harry's face was still\n",
            "scarlet's across the room,\n",
            "Harry's face was still\n",
            "scarlet's across the room,\n",
            "\n",
            "Epoch 12 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.5797\n",
            "Step: 2000, average training loss over last 1000 steps: 3.1714\n",
            "Step: 3000, average training loss over last 1000 steps: 3.1709\n",
            "Step: 4000, average training loss over last 1000 steps: 3.1737\n",
            "Step: 5000, average training loss over last 1000 steps: 3.1731\n",
            "Step: 6000, average training loss over last 1000 steps: 3.1730\n",
            "Step: 7000, average training loss over last 1000 steps: 3.1684\n",
            "Step: 8000, average training loss over last 1000 steps: 3.1677\n",
            "Step: 9000, average training loss over last 1000 steps: 3.1698\n",
            "Step: 10000, average training loss over last 1000 steps: 3.1622\n",
            "Step: 11000, average training loss over last 1000 steps: 3.1641\n",
            "Step: 12000, average training loss over last 1000 steps: 3.1652\n",
            "Step: 13000, average training loss over last 1000 steps: 3.1664\n",
            "Epoch: 12, validation loss: 9.5650\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry's\n",
            "Harry's eyes\n",
            "Harry's eyes,\n",
            "Harry's eyes, and\n",
            "Harry's eyes, and\n",
            "Harry's eyes, and Ha\n",
            "Harry's eyes, and Har\n",
            "Harry's eyes, and Harry\n",
            "Harry's eyes, and Harry's\n",
            "Harry's eyes, and Harry's eyes\n",
            "Harry's eyes, and Harry's eyes were\n",
            "Harry's eyes, and Harry's eyes were\n",
            "\n",
            "Harry's eyes, and Harry's eyes were\n",
            "s\n",
            "Harry's eyes, and Harry's eyes were\n",
            "s\n",
            "Harry's eyes, and Harry's eyes were\n",
            "spi\n",
            "Harry's eyes, and Harry's eyes were\n",
            "spider\n",
            "Harry's eyes, and Harry's eyes were\n",
            "spiders\n",
            "Harry's eyes, and Harry's eyes were\n",
            "spiders were\n",
            "Harry's eyes, and Harry's eyes were\n",
            "spiders were\n",
            "\n",
            "Harry's eyes, and Harry's eyes were\n",
            "spiders were\n",
            "wo\n",
            "Epoch 13 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.5592\n",
            "Step: 2000, average training loss over last 1000 steps: 3.1606\n",
            "Step: 3000, average training loss over last 1000 steps: 3.1576\n",
            "Step: 4000, average training loss over last 1000 steps: 3.1618\n",
            "Step: 5000, average training loss over last 1000 steps: 3.1581\n",
            "Step: 6000, average training loss over last 1000 steps: 3.1614\n",
            "Step: 7000, average training loss over last 1000 steps: 3.1554\n",
            "Step: 8000, average training loss over last 1000 steps: 3.1502\n",
            "Step: 9000, average training loss over last 1000 steps: 3.1565\n",
            "Step: 10000, average training loss over last 1000 steps: 3.1549\n",
            "Step: 11000, average training loss over last 1000 steps: 3.1533\n",
            "Step: 12000, average training loss over last 1000 steps: 3.1577\n",
            "Step: 13000, average training loss over last 1000 steps: 3.1544\n",
            "Epoch: 13, validation loss: 9.6085\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry's\n",
            "Harry's face\n",
            "Harry's face.\n",
            "Harry's face. \"\n",
            "Harry's face. \"You\n",
            "Harry's face. \"You'\n",
            "Harry's face. \"You're\n",
            "Harry's face. \"You're going to\n",
            "Harry's face. \"You're going to be\n",
            "Harry's face. \"You're going to be \n",
            "Harry's face. \"You're going to be -\n",
            "Harry's face. \"You're going to be --\n",
            "Harry's face. \"You're going to be --\"\n",
            "Harry's face. \"You're going to be --\"\n",
            "\n",
            "Harry's face. \"You're going to be --\"\n",
            "\n",
            "\"\n",
            "Harry's face. \"You're going to be --\"\n",
            "\n",
            "\"No\n",
            "Harry's face. \"You're going to be --\"\n",
            "\n",
            "\"No,\"\n",
            "Harry's face. \"You're going to be --\"\n",
            "\n",
            "\"No,\" said\n",
            "Harry's face. \"You're going to be --\"\n",
            "\n",
            "\"No,\" said\n",
            "Harry's face. \"You're going to be --\"\n",
            "\n",
            "\"No,\" said Ro\n",
            "Epoch 14 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.5500\n",
            "Step: 2000, average training loss over last 1000 steps: 3.1439\n",
            "Step: 3000, average training loss over last 1000 steps: 3.1515\n",
            "Step: 4000, average training loss over last 1000 steps: 3.1488\n",
            "Step: 5000, average training loss over last 1000 steps: 3.1474\n",
            "Step: 6000, average training loss over last 1000 steps: 3.1462\n",
            "Step: 7000, average training loss over last 1000 steps: 3.1449\n",
            "Step: 8000, average training loss over last 1000 steps: 3.1454\n",
            "Step: 9000, average training loss over last 1000 steps: 3.1461\n",
            "Step: 10000, average training loss over last 1000 steps: 3.1428\n",
            "Step: 11000, average training loss over last 1000 steps: 3.1422\n",
            "Step: 12000, average training loss over last 1000 steps: 3.1457\n",
            "Step: 13000, average training loss over last 1000 steps: 3.1452\n",
            "Epoch: 14, validation loss: 9.6268\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry's\n",
            "Harry's eyes\n",
            "Harry's eyes,\n",
            "Harry's eyes, \"\n",
            "Harry's eyes, \"but\n",
            "Harry's eyes, \"but\n",
            "Harry's eyes, \"but Ha\n",
            "Harry's eyes, \"but Har\n",
            "Harry's eyes, \"but Harry\n",
            "Harry's eyes, \"but Harry's\n",
            "Harry's eyes, \"but Harry's arm\n",
            "Harry's eyes, \"but Harry's arms\n",
            "Harry's eyes, \"but Harry's arms were\n",
            "Harry's eyes, \"but Harry's arms were\n",
            "Harry's eyes, \"but Harry's arms werecha\n",
            "Harry's eyes, \"but Harry's arms werechasing\n",
            "Harry's eyes, \"but Harry's arms werechasinging\n",
            "Harry's eyes, \"but Harry's arms werechasinging around\n",
            "Harry's eyes, \"but Harry's arms werechasinging around the\n",
            "Harry's eyes, \"but Harry's arms werechasinging around the room\n",
            "Epoch 15 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.5344\n",
            "Step: 2000, average training loss over last 1000 steps: 3.1344\n",
            "Step: 3000, average training loss over last 1000 steps: 3.1368\n",
            "Step: 4000, average training loss over last 1000 steps: 3.1364\n",
            "Step: 5000, average training loss over last 1000 steps: 3.1342\n",
            "Step: 6000, average training loss over last 1000 steps: 3.1350\n",
            "Step: 7000, average training loss over last 1000 steps: 3.1418\n",
            "Step: 8000, average training loss over last 1000 steps: 3.1377\n",
            "Step: 9000, average training loss over last 1000 steps: 3.1396\n",
            "Step: 10000, average training loss over last 1000 steps: 3.1364\n",
            "Step: 11000, average training loss over last 1000 steps: 3.1358\n",
            "Step: 12000, average training loss over last 1000 steps: 3.1374\n",
            "Step: 13000, average training loss over last 1000 steps: 3.1367\n",
            "Epoch: 15, validation loss: 9.6757\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, but\n",
            "Harry, but\n",
            "Harry, but Ha\n",
            "Harry, but Har\n",
            "Harry, but Harry\n",
            "Harry, but Harry's\n",
            "Harry, but Harry's eyes\n",
            "Harry, but Harry's eyes.\n",
            "Harry, but Harry's eyes. Ha\n",
            "Harry, but Harry's eyes. Har\n",
            "Harry, but Harry's eyes. Harry\n",
            "Harry, but Harry's eyes. Harry\n",
            "\n",
            "Harry, but Harry's eyes. Harry\n",
            "his\n",
            "Harry, but Harry's eyes. Harry\n",
            "hisse\n",
            "Harry, but Harry's eyes. Harry\n",
            "hissed\n",
            "Harry, but Harry's eyes. Harry\n",
            "hissedle\n",
            "Harry, but Harry's eyes. Harry\n",
            "hissedley\n",
            "Harry, but Harry's eyes. Harry\n",
            "hissedley,\n",
            "Harry, but Harry's eyes. Harry\n",
            "hissedley, and\n",
            "Epoch 16 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.5289\n",
            "Step: 2000, average training loss over last 1000 steps: 3.1292\n",
            "Step: 3000, average training loss over last 1000 steps: 3.1281\n",
            "Step: 4000, average training loss over last 1000 steps: 3.1301\n",
            "Step: 5000, average training loss over last 1000 steps: 3.1281\n",
            "Step: 6000, average training loss over last 1000 steps: 3.1272\n",
            "Step: 7000, average training loss over last 1000 steps: 3.1247\n",
            "Step: 8000, average training loss over last 1000 steps: 3.1306\n",
            "Step: 9000, average training loss over last 1000 steps: 3.1284\n",
            "Step: 10000, average training loss over last 1000 steps: 3.1295\n",
            "Step: 11000, average training loss over last 1000 steps: 3.1289\n",
            "Step: 12000, average training loss over last 1000 steps: 3.1266\n",
            "Step: 13000, average training loss over last 1000 steps: 3.1274\n",
            "Epoch: 16, validation loss: 9.7048\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, and\n",
            "Harry, and\n",
            "Harry, and Her\n",
            "Harry, and Her\n",
            "Harry, and Hermi\n",
            "Harry, and Hermione\n",
            "Harry, and Hermione,\n",
            "Harry, and Hermione, looking\n",
            "Harry, and Hermione, looking at\n",
            "Harry, and Hermione, looking at\n",
            "Harry, and Hermione, looking at Ha\n",
            "Harry, and Hermione, looking at Har\n",
            "Harry, and Hermione, looking at Harry\n",
            "Harry, and Hermione, looking at Harry's\n",
            "Harry, and Hermione, looking at Harry's face\n",
            "Harry, and Hermione, looking at Harry's face.\n",
            "Harry, and Hermione, looking at Harry's face.\n",
            "\"\n",
            "Harry, and Hermione, looking at Harry's face.\n",
            "\"What\n",
            "Harry, and Hermione, looking at Harry's face.\n",
            "\"What?\n",
            "Epoch 17 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.5168\n",
            "Step: 2000, average training loss over last 1000 steps: 3.1187\n",
            "Step: 3000, average training loss over last 1000 steps: 3.1233\n",
            "Step: 4000, average training loss over last 1000 steps: 3.1237\n",
            "Step: 5000, average training loss over last 1000 steps: 3.1196\n",
            "Step: 6000, average training loss over last 1000 steps: 3.1221\n",
            "Step: 7000, average training loss over last 1000 steps: 3.1225\n",
            "Step: 8000, average training loss over last 1000 steps: 3.1229\n",
            "Step: 9000, average training loss over last 1000 steps: 3.1183\n",
            "Step: 10000, average training loss over last 1000 steps: 3.1165\n",
            "Step: 11000, average training loss over last 1000 steps: 3.1239\n",
            "Step: 12000, average training loss over last 1000 steps: 3.1197\n",
            "Step: 13000, average training loss over last 1000 steps: 3.1220\n",
            "Epoch: 17, validation loss: 9.7050\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, and\n",
            "Harry, and\n",
            "Harry, and Ro\n",
            "Harry, and Ron\n",
            "Harry, and Ron,\n",
            "Harry, and Ron, who\n",
            "Harry, and Ron, who was\n",
            "Harry, and Ron, who was still\n",
            "Harry, and Ron, who was still s\n",
            "Harry, and Ron, who was still s\n",
            "Harry, and Ron, who was still smil\n",
            "Harry, and Ron, who was still smiling\n",
            "Harry, and Ron, who was still smiling.\n",
            "Harry, and Ron, who was still smiling. \"\n",
            "Harry, and Ron, who was still smiling. \"You\n",
            "Harry, and Ron, who was still smiling. \"You'\n",
            "Harry, and Ron, who was still smiling. \"You're\n",
            "Harry, and Ron, who was still smiling. \"You're not\n",
            "Harry, and Ron, who was still smiling. \"You're not going to\n",
            "Epoch 18 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.5163\n",
            "Step: 2000, average training loss over last 1000 steps: 3.1176\n",
            "Step: 3000, average training loss over last 1000 steps: 3.1088\n",
            "Step: 4000, average training loss over last 1000 steps: 3.1157\n",
            "Step: 5000, average training loss over last 1000 steps: 3.1143\n",
            "Step: 6000, average training loss over last 1000 steps: 3.1163\n",
            "Step: 7000, average training loss over last 1000 steps: 3.1132\n",
            "Step: 8000, average training loss over last 1000 steps: 3.1120\n",
            "Step: 9000, average training loss over last 1000 steps: 3.1168\n",
            "Step: 10000, average training loss over last 1000 steps: 3.1109\n",
            "Step: 11000, average training loss over last 1000 steps: 3.1144\n",
            "Step: 12000, average training loss over last 1000 steps: 3.1176\n",
            "Step: 13000, average training loss over last 1000 steps: 3.1155\n",
            "Epoch: 18, validation loss: 9.7517\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, and\n",
            "Harry, and\n",
            "Harry, and Ro\n",
            "Harry, and Ron\n",
            "Harry, and Ron,\n",
            "Harry, and Ron, who\n",
            "Harry, and Ron, who was\n",
            "Harry, and Ron, who was looking\n",
            "Harry, and Ron, who was looking at\n",
            "Harry, and Ron, who was looking at\n",
            "Harry, and Ron, who was looking at Ha\n",
            "Harry, and Ron, who was looking at Har\n",
            "Harry, and Ron, who was looking at Harry\n",
            "Harry, and Ron, who was looking at Harry's\n",
            "Harry, and Ron, who was looking at Harry's eyes\n",
            "Harry, and Ron, who was looking at Harry's eyes,\n",
            "Harry, and Ron, who was looking at Harry's eyes, \"\n",
            "Harry, and Ron, who was looking at Harry's eyes, \"you\n",
            "Harry, and Ron, who was looking at Harry's eyes, \"you'\n"
          ]
        }
      ],
      "source": [
        "train_loss, val_loss = train(model, train_loader, val_loader, torch.nn.functional.cross_entropy, optimizer, NUM_EPOCHS, DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "3m2I7HS8MO8M"
      },
      "outputs": [],
      "source": [
        "text_sample = dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "QXfqcgw2MYSz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5720fbfe-2b96-4e8c-8d4a-b381d545144e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(36)\n"
          ]
        }
      ],
      "source": [
        "print(text_sample[0])\n",
        "test_block = torch.tensor([text_sample[i] for i in range(8)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "nJXjZzCnMoSY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8ec8bcc-d77b-49ab-8629-26f2606d0861"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 36, 264,  62, 196,  36, 301,  64, 382])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "test_block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "m0uewcO7MwY_"
      },
      "outputs": [],
      "source": [
        "test_list = test_block.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "P8mbTFkAMpic",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "307dc970-d12e-4da3-da61-dabbba13eb0b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Harry Potter'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "vocab.decode(test_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "aAN0uzbYhKa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "0dd8298f-553e-4034-e5ca-243c58d92fa4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Harry Potter and the Sorcerer's Stone\\n\\n\\nCHAPTER ONE\\n\\nTHE BOY WHO LIVED\\n\\nMr. and Mrs. Dursley, of number four, Privet Drive, were proud to say\\nthat they were perfectly normal, thank you very much. They were the last\\npeople you'd expect to be\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "test_block = torch.tensor([text_sample[i] for i in range(100)])\n",
        "test_list = test_block.tolist()\n",
        "vocab.decode(test_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "XrBrg4lRhOuv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d17ee92-d8f7-48db-e12f-ff9a6cc6072a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: Walking out in the dark\n",
            "Walking out in the dark corridor, and Harry's Potter, and Harry's face.\n",
            "\n",
            "\"You're going to be a bit more time?\" said Harry. \"He's\n",
            "they'd become on, and Harry, who had never seen the Ministry of Magic Hogwarts, and Harry's arms around the room, and saw Harry, who was sitting down the table, and Harry's face.\n",
            "\"You're not,\" said Harry, \"You're not going to be able to keep him up they're going to be able to Hogwarts Harry knew Ron, who had just been brought up to Ron, whosed Harry's arms were\n",
            "chair, and Harry's eyes were\n",
            "with Harry's eyes were\n",
            "screaming the Ministry of Magic's Malfoy, and Harry's arms around the Harry's\n",
            "and Ron, who had\n",
            "they heard Harry's voice, \"I'm not going to be --\"\n",
            "\n",
            "\"You're not going to be able to Harry, who had never seen anything that Harry had never seen anything about the Hogwarts, but Harry hadn't seen anything to Harry's face, and Ron, who was sitting down in the corner,\n",
            "they're going to be able to keep up they're going to be a bit of Muggle Harry's face.\n",
            "\"What is it?\" said Hermione, who was sitting down the table, and Harry's head, and Harry's arm, and Ron, who had just been across the room, and\n",
            "they'd began a small, smount Petunia, and Peered down the Triwizard Tournament, and Harry's arm.\n",
            "\n",
            "\"What's that?\" said Harry. \"You're going to be -\"\n",
            "\n",
            "\"What?\" said Harry. \"He's got a bit of Harry's face. \"You're not going to be able to Hogwarts, but Harry hadn't seen anything to\n",
            "they're going to be a bit of Ron, who had just been brought up to Hogwarts, but Harry didn't feel much more like that Harry's Potter's Harry's shoulder, and Harry, who had just been\n",
            "they heard Muggle Harry's head, and Harry's Ron, who was looking at Harry. \"You're going to be a bit of Hogwarts Harry's Potter, who wasn't going to be a bit of Harry's eyes off his face.\n",
            "\n",
            "\"You're not,\" said Ron, \"we'll be able to Harry's bedroom, and Harry's arm.\n",
            "\n",
            "\"What?\" said Harry. \"You're going to be a bit of Hogwarts, and Harry's face, and Harry's face, \"I'm not going to be -\"\n",
            "\n",
            "\"You're not going to be a good-bye, but Harry's face, and Ron, who had just been\n",
            "the Chamber of Secrets, who was sitting down the table, and Harry's arm.\n",
            "\n",
            "\"You're going to be -\"\n",
            "\n",
            "\"You're not going to be able to Harry, who hadn't been\n",
            "been going on Hogwarts, and Harry's arms weren't going to be awizards were\n",
            "they're going to be a bit of Hogwarts, and Harry's face, \"you're going to get us.\"\n",
            "\n",
            "Harry stared at Harry, who had just been\n",
            "they'd become on, and Harry's hand, and Harry's shoulder,\n"
          ]
        }
      ],
      "source": [
        "generate(model, \"Walking out in the dark\", device= DEVICE, n = 1000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}