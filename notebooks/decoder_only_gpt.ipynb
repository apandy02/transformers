{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aryamanpandya99/Transformers/blob/main/notebooks/decoder_only_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7GuoX-D2SUi"
      },
      "source": [
        "# Decoder-only based GPT (language model)\n",
        "\n",
        "Here we take a transformer block, the decoder in particular, and use it for the task of language modeling. In general, this is how GPTs are trained. We will do this on a much smaller scale.\n",
        "\n",
        "We take everything we've already built and leverage it in the way Karpathy implements a character level LM here:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ptBhD9c3kTU",
        "outputId": "ca43b0e2-4fea-4121-a2c4-244839a2b359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/Transformers/models/transformer_blocks.py .\n",
        "!cp drive/MyDrive/Transformers/models/modules.py .\n",
        "!cp -r drive/MyDrive/Transformers/data/ ."
      ],
      "metadata": {
        "id": "DAnCCtxa3i6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenmonster"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_XqFHTe4Ozn",
        "outputId": "0fcea36d-d5ed-4a4d-cec3-d64b10cbe359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenmonster\n",
            "  Downloading tokenmonster-1.1.12.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: tokenmonster\n",
            "  Building wheel for tokenmonster (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tokenmonster: filename=tokenmonster-1.1.12-py3-none-any.whl size=15820 sha256=286ea71772d537ebafac372cfda04f7e72a01542aac12db34aacd58dc8b951ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/49/56/9db5eb8fd22ea838f03cc48cc4e096d0f1e810dff3e4559abe\n",
            "Successfully built tokenmonster\n",
            "Installing collected packages: tokenmonster\n",
            "Successfully installed tokenmonster-1.1.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0LzXILI2SUk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from torch.utils.data import random_split\n",
        "import sys\n",
        "sys.path.append(\"~/\")\n",
        "from transformer_blocks import Transformer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tokenmonster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP_BMiE52SUl",
        "outputId": "b7ed8462-e799-447f-b84e-6062b6669d06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2652650\n"
          ]
        }
      ],
      "source": [
        "harry_potter_text = \" \"\n",
        "for i in range(4):\n",
        "    book_num = i+1\n",
        "    with open(f'/content/data/hp{book_num}.txt', 'r', encoding='utf-8') as f:\n",
        "        harry_potter_text += f.read()\n",
        "print(len(harry_potter_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTW6wJWB2SUl"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WTsFKtP2SUm"
      },
      "source": [
        "## Tokenization\n",
        "Instead of character level, we're going to model this LM using a tokenizer. in particular, we're going to try to use OpenAI's tiktoken with the gpt2 50k tokenizer. This might end up being too large of a vocab size given compute constraints, but"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNAlYTIf2SUm"
      },
      "outputs": [],
      "source": [
        "vocab = tokenmonster.load(\"fiction-2048-consistent-v1\")\n",
        "tokens = vocab.tokenize(\"This is a test.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ya_OlheL2SUm",
        "outputId": "c5dca95f-b17f-4041-e8b6-1087b8550377",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 149, 1674,  110,  374,  233,   17], dtype=uint16)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ0pfOIT2SUm"
      },
      "outputs": [],
      "source": [
        "token_example = vocab.tokenize(\"hello world test monster tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1y5eYUD2SUn",
        "outputId": "2cae800d-d151-4c29-c1f3-df4712de5572",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  37,  586,  196, 1261,  374,  233,  627,  773,  377,   37,  601,\n",
              "         53,  252,   62], dtype=uint16)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "token_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qyadxmt2SUn",
        "outputId": "0cf40e76-bff8-4dca-f97b-09b396f0b761",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " ' hel',\n",
              " 'lo',\n",
              " ' world',\n",
              " ' te',\n",
              " 'st',\n",
              " ' mon',\n",
              " 'ster',\n",
              " ' to',\n",
              " '',\n",
              " ' ken',\n",
              " 'i',\n",
              " 'ze',\n",
              " 'r']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "[vocab.decode([token]) for token in token_example]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ks4lD6p2SUn"
      },
      "outputs": [],
      "source": [
        "tokens = np.array(vocab.tokenize(harry_potter_text), dtype=np.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WH_4N1qN2SUn",
        "outputId": "61662fa9-e5b4-4d12-edf4-fc1ccd4c7e35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([900724]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "dataset = torch.tensor(tokens, dtype=torch.long)\n",
        "print(dataset.shape, dataset.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOqbUh7U2SUn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02a3e398-460c-4b16-ca07-90434956e458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Harry Potter and the Sorcerer's Stone\n",
            "\n",
            "\n",
            "CHAPTER ONE\n",
            "\n",
            "THE BOY WHO LIVED\n",
            "\n",
            "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say\n",
            "that they were perfectly normal, thank you very much. They were the last\n",
            "people you'd expect to be involved in anything strange or mysterious,\n",
            "because they just didn't hold with such no\n",
            " Viktor Krum, the famous International Quidditch player.  It was as though the eighteen-year-old Krum thought he. Harry, was an equal - a real rival -\n",
            "\"You haff never . . . you haff not...\"\n",
            "\"No,\" said Harry very firmly.\n",
            "\n",
            "moved from his own foot and tricked Mr. Malfoy into giving Dobby, thereby setting Dobby free.  The other was covered in pink and orange stripes.\n",
            "\"Dobby, what're you doing here?\" Harry said in amazement. \"Dobby has come to work at Hogwarts, sir!\" Dob\n"
          ]
        }
      ],
      "source": [
        "train_size = int(len(dataset) * 0.8)\n",
        "test_size = int(len(dataset) * 0.1)\n",
        "val_size = len(dataset) - train_size - test_size\n",
        "\n",
        "train_data, test_data, val_data = dataset[:train_size], dataset[train_size:train_size+test_size], dataset[train_size+test_size:]\n",
        "\n",
        "train_block = torch.tensor([train_data[i] for i in range(100)])\n",
        "train_list = train_block.tolist()\n",
        "print(vocab.decode(train_list))\n",
        "\n",
        "val_block = torch.tensor([val_data[i] for i in range(100)])\n",
        "val_list = val_block.tolist()\n",
        "print(vocab.decode(val_list))\n",
        "\n",
        "test_block = torch.tensor([test_data[i] for i in range(100)])\n",
        "test_list = test_block.tolist()\n",
        "print(vocab.decode(test_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoWJLYvs2SUo",
        "outputId": "59c4ecdd-841f-42dd-fc63-6e2d6e5131d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train set size: 720579, test: 90072, val: 90073\n"
          ]
        }
      ],
      "source": [
        "print(f\"train set size: {train_size}, test: {test_size}, val: {val_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbbeNPTF2SUo"
      },
      "outputs": [],
      "source": [
        "class HPDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of possible sequences\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Fetch a single sequence x and its corresponding target y\n",
        "        x = self.data[idx:idx + self.block_size]\n",
        "        y = self.data[idx + 1:idx + self.block_size + 1]\n",
        "        return x, y\n",
        "\n",
        "BLOCK_SIZE = 32\n",
        "train_dataset, val_dataset, test_dataset = HPDataset(train_data, BLOCK_SIZE), HPDataset(val_data, BLOCK_SIZE), HPDataset(test_data, BLOCK_SIZE)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader, val_loader, test_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True), DataLoader(val_dataset, batch_size=batch_size, shuffle=True), DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9AWjLIa2SUo",
        "outputId": "1f2ca571-6c4e-48a5-951a-a7df721374dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11259\n",
            "1407\n",
            "1407\n"
          ]
        }
      ],
      "source": [
        "print(len(train_loader))\n",
        "print(len(test_loader))\n",
        "print(len(val_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9D0dkIBU2SUo"
      },
      "outputs": [],
      "source": [
        "class zeptoGPT(nn.Module):\n",
        "    \"\"\"\n",
        "    zepto because it's a really small GPT\n",
        "    \"\"\"\n",
        "    def __init__(self, d_k, d_model, d_v, d_ff, num_heads, num_layers, vocab_size, dropout=0.1) -> None:\n",
        "        super().__init__()\n",
        "        self.decoder_transformer = Transformer(d_k, d_model, d_v, d_ff, num_heads, num_layers, vocab_size=vocab_size, mask=True, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.decoder_transformer(x)\n",
        "        return self.fc(self.layer_norm(out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHC-Secr2SUo"
      },
      "outputs": [],
      "source": [
        "def compute_loss(y_target, y_pred, loss_function):\n",
        "    B, T, C = y_pred.shape\n",
        "    y_pred = y_pred.view(B*T, C)\n",
        "    _, max_indices = torch.max(y_pred, dim=1)\n",
        "    y_target_list = y_target.tolist()\n",
        "    max_indices = max_indices.tolist()\n",
        "    y_target = y_target.view(B*T)\n",
        "    return loss_function(y_pred, y_target)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, prompt: str, device,n = 50, block_size=BLOCK_SIZE):\n",
        "  prompt_array = vocab.tokenize(prompt)\n",
        "  print(prompt_array.shape)\n",
        "  prompt_array = np.array(prompt_array[:block_size], dtype=np.int16)\n",
        "  print(prompt_array.shape)\n",
        "  print(prompt_array.tolist())\n",
        "  decoded = vocab.decode(prompt_array)\n",
        "  print(f\"prompt: {decoded}\")\n",
        "  cumulative_array = prompt_array\n",
        "  for i in range(n):\n",
        "    prompt_tensor = torch.tensor(prompt_array, dtype=torch.long).to(device)\n",
        "    next_token = predict_next_token(model, prompt_tensor.unsqueeze(0))\n",
        "    next_token_np = next_token.cpu().numpy().flatten()\n",
        "    cumulative_array = np.append(cumulative_array, next_token_np)\n",
        "    prompt_array = np.append(prompt_array[1:], next_token_np)\n",
        "    test_list = cumulative_array.tolist()\n",
        "    print(vocab.decode(test_list))"
      ],
      "metadata": {
        "id": "5Pc9PN-Hnrv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_token(model, block):\n",
        "  with torch.no_grad():\n",
        "    y_pred = model(block)\n",
        "    token_probs = nn.functional.softmax(y_pred, dim=-1)\n",
        "    _, max_idx = torch.max(token_probs, dim=-1)\n",
        "  return max_idx.squeeze()[-1]  # return only the last next token prediction"
      ],
      "metadata": {
        "id": "XQ7fkR1zJlI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2wS6j8Y2SUo"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, loss_function, optim, epochs, device):\n",
        "    losses = [] #group losses for loss visualization\n",
        "    running_loss = 0.0\n",
        "    val_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        print(\"Epoch %d / %d\" % (epoch+1, epochs))\n",
        "        print(\"-\"*10)\n",
        "\n",
        "        for i, batch_data in enumerate(train_loader):\n",
        "            x, y = batch_data\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_pred = model(x)\n",
        "\n",
        "            loss = compute_loss(y, y_pred, loss_function)\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            running_loss += loss.item()\n",
        "            losses.append(loss)\n",
        "\n",
        "            if (i+1) % 1000 == 0:\n",
        "                print(\"Step: {}, average training loss over last 1000 steps: {:.4f}\".format(i+1, running_loss/1000))\n",
        "                running_loss = 0.0\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            correct_pred = 0.0\n",
        "            num_samples = 0\n",
        "            for i, batch_data in enumerate(val_loader):\n",
        "                (y, x) = batch_data\n",
        "                y, x = y.to(device), x.to(device)\n",
        "                y_pred = model(x)\n",
        "                loss = compute_loss(y, y_pred, loss_function)\n",
        "                _, predicted_labels = torch.max(y_pred, 1)\n",
        "                num_samples+=predicted_labels.shape[0]\n",
        "                val_loss += loss.item()\n",
        "\n",
        "            val_losses.append(val_loss)\n",
        "        print(\"Epoch: {}, validation loss: {:.4f}\".format(epoch+1, val_loss/len(val_loader)))\n",
        "        print(\"Generated text: \")\n",
        "        generate(model, \"Harry\", device=DEVICE, n=20)\n",
        "\n",
        "    return losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yj_S3bSm2SUo"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 6e-4\n",
        "NUM_EPOCHS = 5\n",
        "DROPOUT = 0.2\n",
        "D_MODEL = 1024\n",
        "NUM_HEADS = 8\n",
        "D_K = int(D_MODEL / NUM_HEADS)\n",
        "D_V = D_K\n",
        "D_FF = D_MODEL * 4\n",
        "NUM_LAYERS = 2\n",
        "VOCAB_SIZE = vocab.vocab_size\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f314V3J12SUo"
      },
      "outputs": [],
      "source": [
        "model = zeptoGPT(D_K, D_MODEL, D_V, D_FF, num_heads=NUM_HEADS, num_layers=NUM_LAYERS, vocab_size=VOCAB_SIZE)\n",
        "model = model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqdfCB9C2SUo"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWmZ65uO6XhP",
        "outputId": "ff251929-9645-489e-9352-bd9d43d5eb5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxStEdDG2SUp",
        "outputId": "3bee4532-6d64-4035-ebb6-34fb6e597e3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 5\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 4.5192\n",
            "Step: 2000, average training loss over last 1000 steps: 4.0947\n",
            "Step: 3000, average training loss over last 1000 steps: 3.9889\n",
            "Step: 4000, average training loss over last 1000 steps: 3.9341\n",
            "Step: 5000, average training loss over last 1000 steps: 3.8920\n",
            "Step: 6000, average training loss over last 1000 steps: 3.8589\n",
            "Step: 7000, average training loss over last 1000 steps: 3.8303\n",
            "Step: 8000, average training loss over last 1000 steps: 3.8098\n",
            "Step: 9000, average training loss over last 1000 steps: 3.7888\n",
            "Step: 10000, average training loss over last 1000 steps: 3.7644\n",
            "Step: 11000, average training loss over last 1000 steps: 3.7484\n",
            "Epoch: 1, validation loss: 9.2498\n",
            "Generated text: \n",
            "(3,)\n",
            "(3,)\n",
            "[149, 582, 226]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, and\n",
            "Harry, and\n",
            "Harry, and Har\n",
            "Harry, and Harry\n",
            "Harry, and Harry's\n",
            "Harry, and Harry's\n",
            "\n",
            "Harry, and Harry's\n",
            "the\n",
            "Harry, and Harry's\n",
            "they\n",
            "Harry, and Harry's\n",
            "they'\n",
            "Harry, and Harry's\n",
            "they're\n",
            "Harry, and Harry's\n",
            "they're,\n",
            "Harry, and Harry's\n",
            "they're, you\n",
            "Harry, and Harry's\n",
            "they're, you'\n",
            "Harry, and Harry's\n",
            "they're, you've\n",
            "Harry, and Harry's\n",
            "they're, you've got\n",
            "Harry, and Harry's\n",
            "they're, you've got a\n",
            "Harry, and Harry's\n",
            "they're, you've got a\n",
            "\n",
            "Harry, and Harry's\n",
            "they're, you've got a\n",
            "the\n",
            "Harry, and Harry's\n",
            "they're, you've got a\n",
            "they\n",
            "Epoch 2 / 5\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 4.6919\n",
            "Step: 2000, average training loss over last 1000 steps: 3.7077\n",
            "Step: 3000, average training loss over last 1000 steps: 3.6988\n",
            "Step: 4000, average training loss over last 1000 steps: 3.6872\n",
            "Step: 5000, average training loss over last 1000 steps: 3.6722\n",
            "Step: 6000, average training loss over last 1000 steps: 3.6638\n",
            "Step: 7000, average training loss over last 1000 steps: 3.6542\n",
            "Step: 8000, average training loss over last 1000 steps: 3.6370\n",
            "Step: 9000, average training loss over last 1000 steps: 3.6304\n",
            "Step: 10000, average training loss over last 1000 steps: 3.6223\n",
            "Step: 11000, average training loss over last 1000 steps: 3.6136\n",
            "Epoch: 2, validation loss: 9.4476\n",
            "Generated text: \n",
            "(3,)\n",
            "(3,)\n",
            "[149, 582, 226]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, and\n",
            "Harry, and\n",
            "Harry, and Har\n",
            "Harry, and Harry\n",
            "Harry, and Harry's\n",
            "Harry, and Harry's face\n",
            "Harry, and Harry's face,\n",
            "Harry, and Harry's face, but\n",
            "Harry, and Harry's face, but\n",
            "Harry, and Harry's face, but Har\n",
            "Harry, and Harry's face, but Harry\n",
            "Harry, and Harry's face, but Harry's\n",
            "Harry, and Harry's face, but Harry's face\n",
            "Harry, and Harry's face, but Harry's face,\n",
            "Harry, and Harry's face, but Harry's face, and\n",
            "Harry, and Harry's face, but Harry's face, and\n",
            "Harry, and Harry's face, but Harry's face, and Har\n",
            "Harry, and Harry's face, but Harry's face, and Harry\n",
            "Harry, and Harry's face, but Harry's face, and Harry's\n",
            "Epoch 3 / 5\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 4.5271\n",
            "Step: 2000, average training loss over last 1000 steps: 3.5881\n",
            "Step: 3000, average training loss over last 1000 steps: 3.5791\n",
            "Step: 4000, average training loss over last 1000 steps: 3.5724\n",
            "Step: 5000, average training loss over last 1000 steps: 3.5641\n",
            "Step: 6000, average training loss over last 1000 steps: 3.5586\n",
            "Step: 7000, average training loss over last 1000 steps: 3.5519\n",
            "Step: 8000, average training loss over last 1000 steps: 3.5416\n",
            "Step: 9000, average training loss over last 1000 steps: 3.5345\n",
            "Step: 10000, average training loss over last 1000 steps: 3.5315\n",
            "Step: 11000, average training loss over last 1000 steps: 3.5264\n",
            "Epoch: 3, validation loss: 9.5568\n",
            "Generated text: \n",
            "(3,)\n",
            "(3,)\n",
            "[149, 582, 226]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, his\n",
            "Harry, his eyes\n",
            "Harry, his eyes were\n",
            "Harry, his eyes were still\n",
            "Harry, his eyes were still ali\n",
            "Harry, his eyes were still alive\n",
            "Harry, his eyes were still alive,\n",
            "Harry, his eyes were still alive, but\n",
            "Harry, his eyes were still alive, but\n",
            "Harry, his eyes were still alive, but Har\n",
            "Harry, his eyes were still alive, but Harry\n",
            "Harry, his eyes were still alive, but Harry,\n",
            "Harry, his eyes were still alive, but Harry, and\n",
            "Harry, his eyes were still alive, but Harry, and\n",
            "Harry, his eyes were still alive, but Harry, and Her\n",
            "Harry, his eyes were still alive, but Harry, and Her\n",
            "Harry, his eyes were still alive, but Harry, and Hermi\n",
            "Harry, his eyes were still alive, but Harry, and Hermi\n",
            "Harry, his eyes were still alive, but Harry, and Hermione\n",
            "Epoch 4 / 5\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 4.4229\n",
            "Step: 2000, average training loss over last 1000 steps: 3.5031\n",
            "Step: 3000, average training loss over last 1000 steps: 3.5016\n",
            "Step: 4000, average training loss over last 1000 steps: 3.4905\n",
            "Step: 5000, average training loss over last 1000 steps: 3.4899\n",
            "Step: 6000, average training loss over last 1000 steps: 3.4830\n",
            "Step: 7000, average training loss over last 1000 steps: 3.4805\n",
            "Step: 8000, average training loss over last 1000 steps: 3.4658\n",
            "Step: 9000, average training loss over last 1000 steps: 3.4619\n",
            "Step: 10000, average training loss over last 1000 steps: 3.4597\n",
            "Step: 11000, average training loss over last 1000 steps: 3.4552\n",
            "Epoch: 4, validation loss: 9.6359\n",
            "Generated text: \n",
            "(3,)\n",
            "(3,)\n",
            "[149, 582, 226]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, and\n",
            "Harry, and\n",
            "Harry, and Her\n",
            "Harry, and Her\n",
            "Harry, and Hermi\n",
            "Harry, and Hermi\n",
            "Harry, and Hermione\n",
            "Harry, and Hermione,\n",
            "Harry, and Hermione, and\n",
            "Harry, and Hermione, and\n",
            "Harry, and Hermione, and Har\n",
            "Harry, and Hermione, and Harry\n",
            "Harry, and Hermione, and Harry's\n",
            "Harry, and Hermione, and Harry's arm\n",
            "Harry, and Hermione, and Harry's arm\n",
            "Harry, and Hermione, and Harry's armchair\n",
            "Harry, and Hermione, and Harry's armchair in front of\n",
            "Harry, and Hermione, and Harry's armchair in front of the\n",
            "Harry, and Hermione, and Harry's armchair in front of the fire\n",
            "Epoch 5 / 5\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 4.3378\n",
            "Step: 2000, average training loss over last 1000 steps: 3.4394\n",
            "Step: 3000, average training loss over last 1000 steps: 3.4366\n",
            "Step: 4000, average training loss over last 1000 steps: 3.4259\n",
            "Step: 5000, average training loss over last 1000 steps: 3.4261\n",
            "Step: 6000, average training loss over last 1000 steps: 3.4235\n",
            "Step: 7000, average training loss over last 1000 steps: 3.4181\n",
            "Step: 8000, average training loss over last 1000 steps: 3.4115\n",
            "Step: 9000, average training loss over last 1000 steps: 3.4048\n",
            "Step: 10000, average training loss over last 1000 steps: 3.4035\n",
            "Step: 11000, average training loss over last 1000 steps: 3.3983\n",
            "Epoch: 5, validation loss: 9.7137\n",
            "Generated text: \n",
            "(3,)\n",
            "(3,)\n",
            "[149, 582, 226]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, and\n",
            "Harry, and\n",
            "Harry, and Har\n",
            "Harry, and Harry\n",
            "Harry, and Harry's\n",
            "Harry, and Harry's eyes\n",
            "Harry, and Harry's eyes.\n",
            "Harry, and Harry's eyes. Har\n",
            "Harry, and Harry's eyes. Harry\n",
            "Harry, and Harry's eyes. Harry had\n",
            "Harry, and Harry's eyes. Harry had seen\n",
            "Harry, and Harry's eyes. Harry had seen the\n",
            "Harry, and Harry's eyes. Harry had seen the\n",
            "Harry, and Harry's eyes. Harry had seen the S\n",
            "Harry, and Harry's eyes. Harry had seen the S\n",
            "Harry, and Harry's eyes. Harry had seen the Sob\n",
            "Harry, and Harry's eyes. Harry had seen the Sob\n",
            "Harry, and Harry's eyes. Harry had seen the Soblet\n",
            "Harry, and Harry's eyes. Harry had seen the Soblet's\n"
          ]
        }
      ],
      "source": [
        "train_loss, val_loss = train(model, train_loader, val_loader, torch.nn.functional.cross_entropy, optimizer, NUM_EPOCHS, DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_sample = train_data"
      ],
      "metadata": {
        "id": "3m2I7HS8MO8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_sample[0])\n",
        "test_block = torch.tensor([text_sample[i] for i in range(8)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXfqcgw2MYSz",
        "outputId": "20486099-a4ca-4fba-bcd5-dc6a0138508f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(36)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_block"
      ],
      "metadata": {
        "id": "nJXjZzCnMoSY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ac9e6d4-421a-472d-b3e3-c03f2e615ffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  36,  582,  226,   36,  354,  240,  172, 1528])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_list = test_block.tolist()"
      ],
      "metadata": {
        "id": "m0uewcO7MwY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.decode(test_list)"
      ],
      "metadata": {
        "id": "P8mbTFkAMpic",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "25845fce-5650-41f5-debc-a61266dc82c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Harry Potter and the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_block = torch.tensor([text_sample[i] for i in range(100)])\n",
        "test_list = test_block.tolist()\n",
        "vocab.decode(test_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "aAN0uzbYhKa8",
        "outputId": "753467a5-52f9-4834-c0ea-ab0905864910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Harry Potter and the Sorcerer's Stone\\n\\n\\nCHAPTER ONE\\n\\nTHE BOY WHO LIVED\\n\\nMr. and Mrs. Dursley, of number four, Privet Drive, were proud to say\\nthat they were perfectly normal, thank you very much. They were the last\\npeople you'd expect to be involved in anything strange or mysterious,\\nbecause they just didn't hold with such no\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(model, \"in the dark\", device= DEVICE, n = 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrBrg4lRhOuv",
        "outputId": "5d7f1671-8171-4772-a66b-8429c4cf95a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3,)\n",
            "(3,)\n",
            "[37, 1381, 813]\n",
            "prompt: in the dark\n",
            "in the darkness\n",
            "in the darkness.\n",
            "in the darkness. Har\n",
            "in the darkness. Harry\n",
            "in the darkness. Harry had\n",
            "in the darkness. Harry had seen\n",
            "in the darkness. Harry had seen the\n",
            "in the darkness. Harry had seen the\n",
            "in the darkness. Harry had seen the S\n",
            "in the darkness. Harry had seen the Sna\n",
            "in the darkness. Harry had seen the Snape\n",
            "in the darkness. Harry had seen the Snape's\n",
            "in the darkness. Harry had seen the Snape's eyes\n",
            "in the darkness. Harry had seen the Snape's eyes.\n",
            "in the darkness. Harry had seen the Snape's eyes. Har\n",
            "in the darkness. Harry had seen the Snape's eyes. Harry\n",
            "in the darkness. Harry had seen the Snape's eyes. Harry had\n",
            "in the darkness. Harry had seen the Snape's eyes. Harry had seen\n",
            "in the darkness. Harry had seen the Snape's eyes. Harry had seen the\n",
            "in the darkness. Harry had seen the Snape's eyes. Harry had seen the\n",
            "in the darkness. Harry had seen the Snape's eyes. Harry had seen the S\n",
            "in the darkness. Harry had seen the Snape's eyes. Harry had seen the S\n",
            "in the darkness. Harry had seen the Snape's eyes. Harry had seen the Seven\n",
            "in the darkness. Harry had seen the Snape's eyes. Harry had seen the Sevent\n",
            "in the darkness. Harry had seen the Snape's eyes. Harry had seen the Sevent\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}