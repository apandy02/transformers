{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7GuoX-D2SUi"
      },
      "source": [
        "# Decoder-only based GPT (language model)\n",
        "\n",
        "Here we take a transformer block, the decoder in particular, and use it for the task of language modeling. In general, this is how GPTs are trained. We will do this on a much smaller scale.\n",
        "\n",
        "We take everything we've already built and leverage it in the way Karpathy implements a character level LM here:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ptBhD9c3kTU",
        "outputId": "73446fa8-0ed8-442b-8fc8-a0db269197ea"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/Transformers/models/transformer_blocks.py .\n",
        "!cp drive/MyDrive/Transformers/models/modules.py .\n",
        "!cp -r drive/MyDrive/Transformers/data/ ."
      ],
      "metadata": {
        "id": "DAnCCtxa3i6o"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenmonster"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_XqFHTe4Ozn",
        "outputId": "0321862d-fd25-431f-a2cf-5cb1d88e18cb"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenmonster in /usr/local/lib/python3.10/dist-packages (1.1.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "p0LzXILI2SUk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from torch.utils.data import random_split\n",
        "import sys\n",
        "sys.path.append(\"~/\")\n",
        "from transformer_blocks import Transformer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tokenmonster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "UP_BMiE52SUl",
        "outputId": "e96a889a-d39a-4972-83bf-49f4658b3092",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2652650\n"
          ]
        }
      ],
      "source": [
        "harry_potter_text = \" \"\n",
        "for i in range(4):\n",
        "    book_num = i+1\n",
        "    with open(f'/content/data/hp{book_num}.txt', 'r', encoding='utf-8') as f:\n",
        "        harry_potter_text += f.read()\n",
        "print(len(harry_potter_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTW6wJWB2SUl"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WTsFKtP2SUm"
      },
      "source": [
        "## Tokenization\n",
        "Instead of character level, we're going to model this LM using a tokenizer. in particular, we're going to try to use OpenAI's tiktoken with the gpt2 50k tokenizer. This might end up being too large of a vocab size given compute constraints, but"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "jNAlYTIf2SUm"
      },
      "outputs": [],
      "source": [
        "vocab = tokenmonster.load(\"fiction-2048-consistent-v1\")\n",
        "tokens = vocab.tokenize(\"This is a test.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "ya_OlheL2SUm",
        "outputId": "4f919864-ee1f-4e01-96e7-68dbc453fe20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 149, 1674,  110,  374,  233,   17], dtype=uint16)"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "nZ0pfOIT2SUm"
      },
      "outputs": [],
      "source": [
        "token_example = vocab.tokenize(\"hello world test monster tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "y1y5eYUD2SUn",
        "outputId": "cb189495-4396-4f38-a4fa-f4893a513cff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  37,  586,  196, 1261,  374,  233,  627,  773,  377,   37,  601,\n",
              "         53,  252,   62], dtype=uint16)"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ],
      "source": [
        "token_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "-qyadxmt2SUn",
        "outputId": "7b609a1c-1b59-43ae-b0c9-da615cf2c956",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " ' hel',\n",
              " 'lo',\n",
              " ' world',\n",
              " ' te',\n",
              " 'st',\n",
              " ' mon',\n",
              " 'ster',\n",
              " ' to',\n",
              " '',\n",
              " ' ken',\n",
              " 'i',\n",
              " 'ze',\n",
              " 'r']"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ],
      "source": [
        "[vocab.decode([token]) for token in token_example]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "2ks4lD6p2SUn"
      },
      "outputs": [],
      "source": [
        "tokens = np.array(vocab.tokenize(harry_potter_text), dtype=np.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "WH_4N1qN2SUn",
        "outputId": "3e20b119-93ae-4f42-b77b-a8610298f214",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([900724]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "dataset = torch.tensor(tokens, dtype=torch.long)\n",
        "print(dataset.shape, dataset.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "rOqbUh7U2SUn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be549060-7d85-4d99-f99f-a2eefc67abad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Harry Potter and the Sorcerer's Stone\n",
            "\n",
            "\n",
            "CHAPTER ONE\n",
            "\n",
            "THE BOY WHO LIVED\n",
            "\n",
            "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say\n",
            "that they were perfectly normal, thank you very much. They were the last\n",
            "people you'd expect to be involved in anything strange or mysterious,\n",
            "because they just didn't hold with such no\n",
            " Viktor Krum, the famous International Quidditch player.  It was as though the eighteen-year-old Krum thought he. Harry, was an equal - a real rival -\n",
            "\"You haff never . . . you haff not...\"\n",
            "\"No,\" said Harry very firmly.\n",
            "\n",
            "moved from his own foot and tricked Mr. Malfoy into giving Dobby, thereby setting Dobby free.  The other was covered in pink and orange stripes.\n",
            "\"Dobby, what're you doing here?\" Harry said in amazement. \"Dobby has come to work at Hogwarts, sir!\" Dob\n"
          ]
        }
      ],
      "source": [
        "train_size = int(len(dataset) * 0.8)\n",
        "test_size = int(len(dataset) * 0.1)\n",
        "val_size = len(dataset) - train_size - test_size\n",
        "\n",
        "train_data, test_data, val_data = dataset[:train_size], dataset[train_size:train_size+test_size], dataset[train_size+test_size:]\n",
        "\n",
        "train_block = torch.tensor([train_data[i] for i in range(100)])\n",
        "train_list = train_block.tolist()\n",
        "print(vocab.decode(train_list))\n",
        "\n",
        "val_block = torch.tensor([val_data[i] for i in range(100)])\n",
        "val_list = val_block.tolist()\n",
        "print(vocab.decode(val_list))\n",
        "\n",
        "test_block = torch.tensor([test_data[i] for i in range(100)])\n",
        "test_list = test_block.tolist()\n",
        "print(vocab.decode(test_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "VoWJLYvs2SUo",
        "outputId": "8c9b8e39-4474-46de-b4ae-6eb5d821b800",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train set size: 720579, test: 90072, val: 90073\n"
          ]
        }
      ],
      "source": [
        "print(f\"train set size: {train_size}, test: {test_size}, val: {val_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "YKvfgm5e2SUo",
        "outputId": "249b8b03-d181-4a4b-d527-3b4bdfaf790b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 8]) torch.Size([32, 8])\n",
            "when input is [3, 398] the target: 1034\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(10000)\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(data):\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])  # for any sequence x, the target y will be the next 8 tokens\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch(train_data)\n",
        "\n",
        "print(xb.shape, yb.shape)\n",
        "\n",
        "context = xb[0, :2]\n",
        "target = yb[0,1]\n",
        "print(f\"when input is {context.tolist()} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "RbbeNPTF2SUo"
      },
      "outputs": [],
      "source": [
        "class HPDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of possible sequences\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Fetch a single sequence x and its corresponding target y\n",
        "        x = self.data[idx:idx + self.block_size]\n",
        "        y = self.data[idx + 1:idx + self.block_size + 1]\n",
        "        return x, y\n",
        "\n",
        "block_size = 8\n",
        "train_dataset, val_dataset, test_dataset = HPDataset(train_data, block_size), HPDataset(val_data, block_size), HPDataset(test_data, block_size)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader, val_loader, test_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True), DataLoader(val_dataset, batch_size=batch_size, shuffle=True), DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "H9AWjLIa2SUo",
        "outputId": "a445ef9d-dc16-4219-ffc3-394bc7c300d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22518\n",
            "2815\n",
            "2815\n"
          ]
        }
      ],
      "source": [
        "print(len(train_loader))\n",
        "print(len(test_loader))\n",
        "print(len(val_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "9D0dkIBU2SUo"
      },
      "outputs": [],
      "source": [
        "class zeptoGPT(nn.Module):\n",
        "    \"\"\"\n",
        "    zepto because it's a really small GPT\n",
        "    \"\"\"\n",
        "    def __init__(self, d_k, d_model, d_v, d_ff, num_heads, num_layers, vocab_size, dropout=0.1) -> None:\n",
        "        super().__init__()\n",
        "        self.decoder_transformer = Transformer(d_k, d_model, d_v, d_ff, num_heads, num_layers, vocab_size=vocab_size, mask=True, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.decoder_transformer(x)\n",
        "        return self.fc(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "oHC-Secr2SUo"
      },
      "outputs": [],
      "source": [
        "def compute_loss(y_target, y_pred, loss_function):\n",
        "    B, T, C = y_pred.shape\n",
        "    y_pred = y_pred.view(B*T, C)\n",
        "    _, max_indices = torch.max(y_pred, dim=1)\n",
        "    y_target_list = y_target.tolist()\n",
        "    max_indices = max_indices.tolist()\n",
        "    y_target = y_target.view(B*T)\n",
        "    return loss_function(y_pred, y_target)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, prompt: str, device,n = 50):\n",
        "  prompt_array = vocab.tokenize(prompt)\n",
        "  prompt_array = np.array(prompt_array[:8], dtype=np.int16)\n",
        "  print(prompt_array.tolist())\n",
        "  decoded = vocab.decode(prompt_array)\n",
        "  print(f\"prompt: {decoded}\")\n",
        "  for i in range(n):\n",
        "    prompt_tensor = torch.tensor(prompt_array, dtype=torch.long).to(device)\n",
        "    next_token = predict_next_token(model, prompt_tensor.unsqueeze(0))\n",
        "    test_list = next_token.tolist()\n",
        "    print(vocab.decode(test_list[0]))"
      ],
      "metadata": {
        "id": "5Pc9PN-Hnrv-"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_token(model, block):\n",
        "  with torch.no_grad():\n",
        "    y_pred = model(block)\n",
        "    token_probs = nn.functional.softmax(y_pred, dim=-1)\n",
        "    _, max_idx = torch.max(token_probs, dim=-1)\n",
        "\n",
        "  return max_idx[-1]"
      ],
      "metadata": {
        "id": "XQ7fkR1zJlI4"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "W2wS6j8Y2SUo"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, loss_function, optim, epochs, device):\n",
        "    losses = [] #group losses for loss visualization\n",
        "    running_loss = 0.0\n",
        "    val_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        print(\"Epoch %d / %d\" % (epoch+1, epochs))\n",
        "        print(\"-\"*10)\n",
        "\n",
        "        for i, batch_data in enumerate(train_loader):\n",
        "            x, y = batch_data\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_pred = model(x)\n",
        "\n",
        "            loss = compute_loss(y, y_pred, loss_function)\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            running_loss += loss.item()\n",
        "            losses.append(loss)\n",
        "\n",
        "            if (i+1) % 1000 == 0:\n",
        "                print(\"Step: {}, average training loss over last 1000 steps: {:.4f}\".format(i+1, running_loss/1000))\n",
        "                running_loss = 0.0\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            correct_pred = 0.0\n",
        "            num_samples = 0\n",
        "            for i, batch_data in enumerate(val_loader):\n",
        "                (y, x) = batch_data\n",
        "                y, x = y.to(device), x.to(device)\n",
        "                y_pred = model(x)\n",
        "                loss = compute_loss(y, y_pred, loss_function)\n",
        "                _, predicted_labels = torch.max(y_pred, 1)\n",
        "                num_samples+=predicted_labels.shape[0]\n",
        "                val_loss += loss.item()\n",
        "\n",
        "            val_losses.append(val_loss)\n",
        "        print(\"Epoch: {}, validation loss: {:.4f}\".format(epoch+1, val_loss/len(val_loader)))\n",
        "\n",
        "    return losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "yj_S3bSm2SUo"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 1e-3\n",
        "NUM_EPOCHS = 1\n",
        "DROPOUT = 0.2\n",
        "D_MODEL = 1024\n",
        "NUM_HEADS = 8\n",
        "D_K = int(D_MODEL / NUM_HEADS)\n",
        "D_V = D_K\n",
        "D_FF = D_MODEL * 4\n",
        "NUM_LAYERS = 4\n",
        "VOCAB_SIZE = vocab.vocab_size\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "f314V3J12SUo"
      },
      "outputs": [],
      "source": [
        "model = zeptoGPT(D_K, D_MODEL, D_V, D_FF, num_heads=NUM_HEADS, num_layers=NUM_LAYERS, vocab_size=VOCAB_SIZE)\n",
        "model = model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "iqdfCB9C2SUo"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWmZ65uO6XhP",
        "outputId": "732a0b45-6b92-4998-ec02-30de66ffc4d0"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "RxStEdDG2SUp",
        "outputId": "405ea360-3609-4258-fb3c-4e91236cfa34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 1\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 5.1052\n",
            "Step: 2000, average training loss over last 1000 steps: 4.6587\n",
            "Step: 3000, average training loss over last 1000 steps: 4.5170\n",
            "Step: 4000, average training loss over last 1000 steps: 4.4380\n",
            "Step: 5000, average training loss over last 1000 steps: 4.3691\n",
            "Step: 6000, average training loss over last 1000 steps: 4.3248\n",
            "Step: 7000, average training loss over last 1000 steps: 4.2872\n",
            "Step: 8000, average training loss over last 1000 steps: 4.2664\n",
            "Step: 9000, average training loss over last 1000 steps: 4.2211\n",
            "Step: 10000, average training loss over last 1000 steps: 4.2211\n",
            "Step: 11000, average training loss over last 1000 steps: 4.1867\n",
            "Step: 12000, average training loss over last 1000 steps: 4.1623\n",
            "Step: 13000, average training loss over last 1000 steps: 4.1472\n",
            "Step: 14000, average training loss over last 1000 steps: 4.1352\n",
            "Step: 15000, average training loss over last 1000 steps: 4.1238\n",
            "Step: 16000, average training loss over last 1000 steps: 4.1018\n",
            "Step: 17000, average training loss over last 1000 steps: 4.0805\n",
            "Step: 18000, average training loss over last 1000 steps: 4.0732\n",
            "Step: 19000, average training loss over last 1000 steps: 4.0777\n",
            "Step: 20000, average training loss over last 1000 steps: 4.0494\n",
            "Step: 21000, average training loss over last 1000 steps: 4.0453\n",
            "Step: 22000, average training loss over last 1000 steps: 4.0295\n",
            "Epoch: 1, validation loss: 9.5009\n"
          ]
        }
      ],
      "source": [
        "train_loss, val_loss = train(model, train_loader, val_loader, torch.nn.functional.cross_entropy, optimizer, NUM_EPOCHS, DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_sample = train_data"
      ],
      "metadata": {
        "id": "3m2I7HS8MO8M"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_sample[0])\n",
        "test_block = torch.tensor([text_sample[i] for i in range(8)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXfqcgw2MYSz",
        "outputId": "16c6ae29-863c-4acb-8f7e-49de45d1082c"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(36)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_block"
      ],
      "metadata": {
        "id": "nJXjZzCnMoSY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ade72bda-4ada-4a06-f5dc-3c03fe8c5091"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  36,  582,  226,   36,  354,  240,  172, 1528])"
            ]
          },
          "metadata": {},
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_list = test_block.tolist()"
      ],
      "metadata": {
        "id": "m0uewcO7MwY_"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.decode(test_list)"
      ],
      "metadata": {
        "id": "P8mbTFkAMpic",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bccf8bc5-df09-47b3-8a85-ff36e7f66d86"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Harry Potter and the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_block = torch.tensor([text_sample[i] for i in range(100)])\n",
        "test_list = test_block.tolist()\n",
        "vocab.decode(test_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "aAN0uzbYhKa8",
        "outputId": "064b4401-41ec-4337-920e-1ba5f327889a"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Harry Potter and the Sorcerer's Stone\\n\\n\\nCHAPTER ONE\\n\\nTHE BOY WHO LIVED\\n\\nMr. and Mrs. Dursley, of number four, Privet Drive, were proud to say\\nthat they were perfectly normal, thank you very much. They were the last\\npeople you'd expect to be involved in anything strange or mysterious,\\nbecause they just didn't hold with such no\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(model, \"This is my text random extra stuff\", device= DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrBrg4lRhOuv",
        "outputId": "343aef10-dc62-4d5c-fe46-9979bae364ec"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[149, 1674, 338, 989, 653, 167, 57, 299]\n",
            "prompt: This is my text random ex\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " go\n",
            " exactly\n",
            " exactly\n",
            " go\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " go\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " go\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " go\n",
            " go\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " go\n",
            " go\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " exactly\n",
            " exactly\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}