{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aryamanpandya99/Transformers/blob/main/notebooks/decoder_only_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7GuoX-D2SUi"
      },
      "source": [
        "# Decoder-only based GPT (language model)\n",
        "\n",
        "Here we take a transformer block, the decoder in particular, and use it for the task of language modeling. In general, this is how GPTs are trained. We will do this on a much smaller scale.\n",
        "\n",
        "We take everything we've already built and leverage it in the way Karpathy implements a character level LM here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ptBhD9c3kTU",
        "outputId": "aa0792bf-c97e-4caf-beeb-1316b9c3f435"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DAnCCtxa3i6o"
      },
      "outputs": [],
      "source": [
        "!cp drive/MyDrive/Transformers/models/transformer_blocks.py .\n",
        "!cp drive/MyDrive/Transformers/models/modules.py .\n",
        "!cp -r drive/MyDrive/Transformers/data/ ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_XqFHTe4Ozn",
        "outputId": "208a4e63-5501-4a3a-c619-ef5c9b155a18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenmonster\n",
            "  Downloading tokenmonster-1.1.12.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: tokenmonster\n",
            "  Building wheel for tokenmonster (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tokenmonster: filename=tokenmonster-1.1.12-py3-none-any.whl size=15820 sha256=268dfb701f51cc0e26fe9f5dc56120536a3c0203963ffd07f7786bcb54eec2df\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/49/56/9db5eb8fd22ea838f03cc48cc4e096d0f1e810dff3e4559abe\n",
            "Successfully built tokenmonster\n",
            "Installing collected packages: tokenmonster\n",
            "Successfully installed tokenmonster-1.1.12\n"
          ]
        }
      ],
      "source": [
        "!pip install tokenmonster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "p0LzXILI2SUk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from torch.utils.data import random_split\n",
        "import sys\n",
        "sys.path.append(\"~/\")\n",
        "from transformer_blocks import Transformer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tokenmonster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UP_BMiE52SUl",
        "outputId": "865da0b8-2a41-4714-be7a-c2256c10d08c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2652650\n"
          ]
        }
      ],
      "source": [
        "harry_potter_text = \" \"\n",
        "for i in range(4):\n",
        "    book_num = i+1\n",
        "    with open(f'/content/data/hp{book_num}.txt', 'r', encoding='utf-8') as f:\n",
        "        harry_potter_text += f.read()\n",
        "print(len(harry_potter_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTW6wJWB2SUl"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WTsFKtP2SUm"
      },
      "source": [
        "## Tokenization\n",
        "Instead of character level, we're going to model this LM using a tokenizer. in particular, we're going to try to use OpenAI's tiktoken with the gpt2 50k tokenizer. This might end up being too large of a vocab size given compute constraints, but"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jNAlYTIf2SUm"
      },
      "outputs": [],
      "source": [
        "vocab = tokenmonster.load(\"fiction-1024-consistent-v1\")\n",
        "tokens = vocab.tokenize(\"This is a test.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya_OlheL2SUm",
        "outputId": "1e05359c-bc45-4272-9b40-75496c26656d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([138, 918, 108, 318, 202,  17], dtype=uint16)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nZ0pfOIT2SUm"
      },
      "outputs": [],
      "source": [
        "token_example = vocab.tokenize(\"hello world test monster tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1y5eYUD2SUn",
        "outputId": "5b23b285-c8a8-4e13-c9e2-29660c955e3e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 37, 445, 174, 785, 318, 202, 465, 547, 321, 169, 181, 218,  62],\n",
              "      dtype=uint16)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "token_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qyadxmt2SUn",
        "outputId": "44afd003-becf-4d44-f9b5-1bb11d639665"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " ' hel',\n",
              " 'lo',\n",
              " ' world',\n",
              " ' te',\n",
              " 'st',\n",
              " ' mon',\n",
              " 'ster',\n",
              " ' to',\n",
              " 'ke',\n",
              " 'ni',\n",
              " 'ze',\n",
              " 'r']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "[vocab.decode([token]) for token in token_example]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2ks4lD6p2SUn"
      },
      "outputs": [],
      "source": [
        "tokens = np.array(vocab.tokenize(harry_potter_text), dtype=np.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH_4N1qN2SUn",
        "outputId": "5d9c662b-e502-42a3-9bbf-2fcc3afe1438"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1050223]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "dataset = torch.tensor(tokens, dtype=torch.long)\n",
        "print(dataset.shape, dataset.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbbeNPTF2SUo",
        "outputId": "b7a92858-893f-4fce-8c79-e569cdff663c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Harry Potter and the Sorcerer's Stone\n",
            "\n",
            "\n",
            "CHAPTER ONE\n",
            "\n",
            "THE BOY WHO LIVED\n",
            "\n",
            "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say\n",
            "that they were perfectly normal, thank you very much. They were the last\n",
            "people you'd expect to be\n",
            "train set size: 840158, test: 105019, val: 105021, data size: 1050223, dataset_size: 1050198\n"
          ]
        }
      ],
      "source": [
        "class HPDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of possible sequences\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Fetch a single sequence x and its corresponding target y\n",
        "        x = self.data[idx:idx + self.block_size]\n",
        "        y = self.data[idx + 1:idx + self.block_size + 1]\n",
        "        return x, y\n",
        "\n",
        "BLOCK_SIZE = 25\n",
        "hp_data = HPDataset(dataset, BLOCK_SIZE)\n",
        "\n",
        "test_block = torch.tensor([dataset[i] for i in range(100)])\n",
        "test_list = test_block.tolist()\n",
        "print(vocab.decode(test_list))\n",
        "\n",
        "train_size = int(len(hp_data) * 0.8)\n",
        "test_size = int(len(hp_data) * 0.1)\n",
        "val_size = len(hp_data) - train_size - test_size\n",
        "\n",
        "print(f\"train set size: {train_size}, test: {test_size}, val: {val_size}, data size: {len(dataset)}, dataset_size: {hp_data.__len__()}\")\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(hp_data, [train_size, val_size, test_size])\n",
        "\n",
        "batch_size = 64\n",
        "train_loader, val_loader, test_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True), DataLoader(val_dataset, batch_size=batch_size, shuffle=True), DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9AWjLIa2SUo",
        "outputId": "bbd9af62-a16c-4a97-b8d1-17d979288fbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13128\n",
            "1641\n",
            "1641\n"
          ]
        }
      ],
      "source": [
        "print(len(train_loader))\n",
        "print(len(test_loader))\n",
        "print(len(val_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5k9W9isInum",
        "outputId": "319b9425-abd4-41a3-8d6a-5e524e97b4e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([ 34,   5, 647,  36, 264,  62, 196, 494,  62,  37, 300,  69,  17, 392,\n",
            "        881, 490, 358, 823, 271, 216,  56, 384, 271,   3,  16]), tensor([  5, 647,  36, 264,  62, 196, 494,  62,  37, 300,  69,  17, 392, 881,\n",
            "        490, 358, 823, 271, 216,  56, 384, 271,   3,  16,   5]))\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset.__getitem__(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9D0dkIBU2SUo"
      },
      "outputs": [],
      "source": [
        "class rowlingGPT(nn.Module):\n",
        "    \"\"\"\n",
        "    JK Rowling would probably not approve\n",
        "    \"\"\"\n",
        "    def __init__(self, d_k, d_model, d_v, d_ff, num_heads, num_layers, vocab_size, dropout=0.1) -> None:\n",
        "        super().__init__()\n",
        "        self.decoder_transformer = Transformer(d_k, d_model, d_v, d_ff, num_heads, num_layers, vocab_size=vocab_size, mask=True, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.decoder_transformer(x)\n",
        "        return self.fc(self.layer_norm(out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "oHC-Secr2SUo"
      },
      "outputs": [],
      "source": [
        "def compute_loss(y_target, y_pred, loss_function):\n",
        "    B, T, C = y_pred.shape\n",
        "    y_pred = y_pred.view(B*T, C)\n",
        "    _, max_indices = torch.max(y_pred, dim=1)\n",
        "    y_target_list = y_target.tolist()\n",
        "    max_indices = max_indices.tolist()\n",
        "    y_target = y_target.view(B*T)\n",
        "    return loss_function(y_pred, y_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5Pc9PN-Hnrv-"
      },
      "outputs": [],
      "source": [
        "def generate(model, prompt: str, device,n = 50, block_size=BLOCK_SIZE):\n",
        "  prompt_array = vocab.tokenize(prompt)\n",
        "  print(prompt_array.shape)\n",
        "  prompt_array = np.array(prompt_array[:block_size], dtype=np.int16)\n",
        "  print(prompt_array.shape)\n",
        "  print(prompt_array.tolist())\n",
        "  decoded = vocab.decode(prompt_array)\n",
        "  print(f\"prompt: {decoded}\")\n",
        "  cumulative_array = prompt_array\n",
        "  for i in range(n):\n",
        "    prompt_tensor = torch.tensor(prompt_array, dtype=torch.long).to(device)\n",
        "    next_token = predict_next_token(model, prompt_tensor.unsqueeze(0))\n",
        "    next_token_np = next_token.cpu().numpy().flatten()\n",
        "    cumulative_array = np.append(cumulative_array, next_token_np)\n",
        "    prompt_array = np.append(prompt_array[1:], next_token_np)\n",
        "    test_list = cumulative_array.tolist()\n",
        "    print(vocab.decode(test_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "XQ7fkR1zJlI4"
      },
      "outputs": [],
      "source": [
        "def predict_next_token(model, block):\n",
        "  with torch.no_grad():\n",
        "    y_pred = model(block)\n",
        "    token_probs = nn.functional.softmax(y_pred, dim=-1)\n",
        "    _, max_idx = torch.max(token_probs, dim=-1)\n",
        "  return max_idx.squeeze()[-1]  # return only the last next token prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "W2wS6j8Y2SUo"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, loss_function, optim, epochs, device):\n",
        "    losses = [] #group losses for loss visualization\n",
        "    running_loss = 0.0\n",
        "    val_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        print(\"Epoch %d / %d\" % (epoch+1, epochs))\n",
        "        print(\"-\"*10)\n",
        "\n",
        "        for i, batch_data in enumerate(train_loader):\n",
        "            x, y = batch_data\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_pred = model(x)\n",
        "\n",
        "            loss = compute_loss(y, y_pred, loss_function)\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            running_loss += loss.item()\n",
        "            losses.append(loss)\n",
        "\n",
        "            if (i+1) % 1000 == 0:\n",
        "                print(\"Step: {}, average training loss over last 1000 steps: {:.4f}\".format(i+1, running_loss/1000))\n",
        "                running_loss = 0.0\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for i, batch_data in enumerate(val_loader):\n",
        "                (y, x) = batch_data\n",
        "                y, x = y.to(device), x.to(device)\n",
        "                y_pred = model(x)\n",
        "                loss = compute_loss(y, y_pred, loss_function)\n",
        "                _, predicted_labels = torch.max(y_pred, 1)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "            val_losses.append(val_loss)\n",
        "        print(\"Epoch: {}, validation loss: {:.4f}\".format(epoch+1, val_loss/len(val_loader)))\n",
        "        print(\"Generated text: \")\n",
        "        generate(model, \"Harry\", device=DEVICE, n=20)\n",
        "\n",
        "    return losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yj_S3bSm2SUo"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 6e-4\n",
        "NUM_EPOCHS = 20\n",
        "DROPOUT = 0.2\n",
        "D_MODEL = 1024\n",
        "NUM_HEADS = 8\n",
        "D_K = int(D_MODEL / NUM_HEADS)\n",
        "D_V = D_K\n",
        "D_FF = D_MODEL * 4\n",
        "NUM_LAYERS = 2\n",
        "VOCAB_SIZE = vocab.vocab_size\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "f314V3J12SUo"
      },
      "outputs": [],
      "source": [
        "model = rowlingGPT(D_K, D_MODEL, D_V, D_FF, num_heads=NUM_HEADS, num_layers=NUM_LAYERS, vocab_size=VOCAB_SIZE)\n",
        "model = model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "iqdfCB9C2SUo"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWmZ65uO6XhP",
        "outputId": "4ab3817d-205a-43fd-aad3-83a5fc0061b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxStEdDG2SUp",
        "outputId": "615a4c2a-f018-414c-9b77-d18f210b709d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 20\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 4.2043\n",
            "Step: 2000, average training loss over last 1000 steps: 3.9020\n",
            "Step: 3000, average training loss over last 1000 steps: 3.8385\n",
            "Step: 4000, average training loss over last 1000 steps: 3.8025\n",
            "Step: 5000, average training loss over last 1000 steps: 3.7699\n",
            "Step: 6000, average training loss over last 1000 steps: 3.7490\n",
            "Step: 7000, average training loss over last 1000 steps: 3.7234\n",
            "Step: 8000, average training loss over last 1000 steps: 3.7069\n",
            "Step: 9000, average training loss over last 1000 steps: 3.6878\n",
            "Step: 10000, average training loss over last 1000 steps: 3.6697\n",
            "Step: 11000, average training loss over last 1000 steps: 3.6624\n",
            "Step: 12000, average training loss over last 1000 steps: 3.6471\n",
            "Step: 13000, average training loss over last 1000 steps: 3.6346\n",
            "Epoch: 1, validation loss: 8.6949\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, and\n",
            "Harry, and\n",
            "Harry, and Her\n",
            "Harry, and Her\n",
            "Harry, and Hermi\n",
            "Harry, and Hermione\n",
            "Harry, and Hermione,\n",
            "Harry, and Hermione, and\n",
            "Harry, and Hermione, and\n",
            "Harry, and Hermione, and Her\n",
            "Harry, and Hermione, and Her\n",
            "Harry, and Hermione, and Hermi\n",
            "Harry, and Hermione, and Hermione\n",
            "Harry, and Hermione, and Hermione,\n",
            "Harry, and Hermione, and Hermione, and\n",
            "Harry, and Hermione, and Hermione, and\n",
            "Harry, and Hermione, and Hermione, and Her\n",
            "Harry, and Hermione, and Hermione, and Her\n",
            "Harry, and Hermione, and Hermione, and Hermi\n",
            "Epoch 2 / 20\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 4.0747\n",
            "Step: 2000, average training loss over last 1000 steps: 3.6036\n",
            "Step: 3000, average training loss over last 1000 steps: 3.5976\n",
            "Step: 4000, average training loss over last 1000 steps: 3.5892\n",
            "Step: 5000, average training loss over last 1000 steps: 3.5807\n",
            "Step: 6000, average training loss over last 1000 steps: 3.5664\n",
            "Step: 7000, average training loss over last 1000 steps: 3.5580\n",
            "Step: 8000, average training loss over last 1000 steps: 3.5532\n",
            "Step: 9000, average training loss over last 1000 steps: 3.5418\n",
            "Step: 10000, average training loss over last 1000 steps: 3.5366\n",
            "Step: 11000, average training loss over last 1000 steps: 3.5257\n",
            "Step: 12000, average training loss over last 1000 steps: 3.5263\n",
            "Step: 13000, average training loss over last 1000 steps: 3.5125\n",
            "Epoch: 2, validation loss: 8.9100\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry's\n",
            "Harry's face\n",
            "Harry's face,\n",
            "Harry's face, and\n",
            "Harry's face, and\n",
            "Harry's face, and Ha\n",
            "Harry's face, and Har\n",
            "Harry's face, and Harry\n",
            "Harry's face, and Harry's\n",
            "Harry's face, and Harry's face\n",
            "Harry's face, and Harry's face,\n",
            "Harry's face, and Harry's face, and\n",
            "Harry's face, and Harry's face, and\n",
            "Harry's face, and Harry's face, and Ha\n",
            "Harry's face, and Harry's face, and Har\n",
            "Harry's face, and Harry's face, and Harry\n",
            "Harry's face, and Harry's face, and Harry's\n",
            "Harry's face, and Harry's face, and Harry's face\n",
            "Harry's face, and Harry's face, and Harry's face,\n",
            "Harry's face, and Harry's face, and Harry's face, and\n",
            "Epoch 3 / 20\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.9516\n",
            "Step: 2000, average training loss over last 1000 steps: 3.4951\n",
            "Step: 3000, average training loss over last 1000 steps: 3.4945\n",
            "Step: 4000, average training loss over last 1000 steps: 3.4896\n",
            "Step: 5000, average training loss over last 1000 steps: 3.4807\n",
            "Step: 6000, average training loss over last 1000 steps: 3.4719\n",
            "Step: 7000, average training loss over last 1000 steps: 3.4684\n",
            "Step: 8000, average training loss over last 1000 steps: 3.4625\n",
            "Step: 9000, average training loss over last 1000 steps: 3.4638\n",
            "Step: 10000, average training loss over last 1000 steps: 3.4529\n",
            "Step: 11000, average training loss over last 1000 steps: 3.4421\n",
            "Step: 12000, average training loss over last 1000 steps: 3.4432\n",
            "Step: 13000, average training loss over last 1000 steps: 3.4392\n",
            "Epoch: 3, validation loss: 9.0360\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry's\n",
            "Harry's arm\n",
            "Harry's arms\n",
            "Harry's arms.\n",
            "Harry's arms. \n",
            "Harry's arms.  \"\n",
            "Harry's arms.  \"You\n",
            "Harry's arms.  \"You'\n",
            "Harry's arms.  \"You're\n",
            "Harry's arms.  \"You're not\n",
            "Harry's arms.  \"You're not going to\n",
            "Harry's arms.  \"You're not going to be\n",
            "Harry's arms.  \"You're not going to be\n",
            "Harry's arms.  \"You're not going to become\n",
            "Harry's arms.  \"You're not going to become on\n",
            "Harry's arms.  \"You're not going to become onto\n",
            "Harry's arms.  \"You're not going to become onto the\n",
            "Harry's arms.  \"You're not going to become onto the\n",
            "Harry's arms.  \"You're not going to become onto the Ha\n",
            "Harry's arms.  \"You're not going to become onto the Har\n",
            "Epoch 4 / 20\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.8650\n",
            "Step: 2000, average training loss over last 1000 steps: 3.4226\n",
            "Step: 3000, average training loss over last 1000 steps: 3.4217\n",
            "Step: 4000, average training loss over last 1000 steps: 3.4137\n",
            "Step: 5000, average training loss over last 1000 steps: 3.4152\n",
            "Step: 6000, average training loss over last 1000 steps: 3.4106\n",
            "Step: 7000, average training loss over last 1000 steps: 3.4060\n",
            "Step: 8000, average training loss over last 1000 steps: 3.3996\n",
            "Step: 9000, average training loss over last 1000 steps: 3.3944\n",
            "Step: 10000, average training loss over last 1000 steps: 3.3901\n",
            "Step: 11000, average training loss over last 1000 steps: 3.3833\n",
            "Step: 12000, average training loss over last 1000 steps: 3.3814\n",
            "Step: 13000, average training loss over last 1000 steps: 3.3808\n",
            "Epoch: 4, validation loss: 9.1338\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry's\n",
            "Harry's face\n",
            "Harry's face,\n",
            "Harry's face, and\n",
            "Harry's face, and\n",
            "Harry's face, and Ha\n",
            "Harry's face, and Har\n",
            "Harry's face, and Harry\n",
            "Harry's face, and Harry's\n",
            "Harry's face, and Harry's bed\n",
            "Harry's face, and Harry's bed\n",
            "Harry's face, and Harry's bedroom\n",
            "Harry's face, and Harry's bedroom,\n",
            "Harry's face, and Harry's bedroom, and\n",
            "Harry's face, and Harry's bedroom, and\n",
            "Harry's face, and Harry's bedroom, and Ha\n",
            "Harry's face, and Harry's bedroom, and Har\n",
            "Harry's face, and Harry's bedroom, and Harry\n",
            "Harry's face, and Harry's bedroom, and Harry's\n",
            "Harry's face, and Harry's bedroom, and Harry's face\n",
            "Epoch 5 / 20\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.8005\n",
            "Step: 2000, average training loss over last 1000 steps: 3.3702\n",
            "Step: 3000, average training loss over last 1000 steps: 3.3636\n",
            "Step: 4000, average training loss over last 1000 steps: 3.3628\n",
            "Step: 5000, average training loss over last 1000 steps: 3.3597\n",
            "Step: 6000, average training loss over last 1000 steps: 3.3554\n",
            "Step: 7000, average training loss over last 1000 steps: 3.3553\n",
            "Step: 8000, average training loss over last 1000 steps: 3.3500\n",
            "Step: 9000, average training loss over last 1000 steps: 3.3441\n",
            "Step: 10000, average training loss over last 1000 steps: 3.3432\n",
            "Step: 11000, average training loss over last 1000 steps: 3.3376\n",
            "Step: 12000, average training loss over last 1000 steps: 3.3367\n",
            "Step: 13000, average training loss over last 1000 steps: 3.3337\n",
            "Epoch: 5, validation loss: 9.2390\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, and\n",
            "Harry, and\n",
            "Harry, and Ro\n",
            "Harry, and Ron\n",
            "Harry, and Ron,\n",
            "Harry, and Ron, and\n",
            "Harry, and Ron, and\n",
            "Harry, and Ron, and Ha\n",
            "Harry, and Ron, and Har\n",
            "Harry, and Ron, and Harry\n",
            "Harry, and Ron, and Harry's\n",
            "Harry, and Ron, and Harry's face\n",
            "Harry, and Ron, and Harry's face,\n",
            "Harry, and Ron, and Harry's face, and\n",
            "Harry, and Ron, and Harry's face, and\n",
            "Harry, and Ron, and Harry's face, and Ha\n",
            "Harry, and Ron, and Harry's face, and Har\n",
            "Harry, and Ron, and Harry's face, and Harry\n",
            "Harry, and Ron, and Harry's face, and Harry's\n",
            "Epoch 6 / 20\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.7513\n",
            "Step: 2000, average training loss over last 1000 steps: 3.3157\n",
            "Step: 3000, average training loss over last 1000 steps: 3.3203\n",
            "Step: 4000, average training loss over last 1000 steps: 3.3184\n",
            "Step: 5000, average training loss over last 1000 steps: 3.3158\n",
            "Step: 6000, average training loss over last 1000 steps: 3.3157\n",
            "Step: 7000, average training loss over last 1000 steps: 3.3114\n",
            "Step: 8000, average training loss over last 1000 steps: 3.3117\n",
            "Step: 9000, average training loss over last 1000 steps: 3.3072\n",
            "Step: 10000, average training loss over last 1000 steps: 3.3025\n",
            "Step: 11000, average training loss over last 1000 steps: 3.3052\n",
            "Step: 12000, average training loss over last 1000 steps: 3.3000\n",
            "Step: 13000, average training loss over last 1000 steps: 3.2947\n",
            "Epoch: 6, validation loss: 9.3020\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry's\n",
            "Harry's face\n",
            "Harry's face was\n",
            "Harry's face wasn'\n",
            "Harry's face wasn't\n",
            "Harry's face wasn't going to\n",
            "Harry's face wasn't going to be\n",
            "Harry's face wasn't going to be any\n",
            "Harry's face wasn't going to be anyone\n",
            "Harry's face wasn't going to be anyone el\n",
            "Harry's face wasn't going to be anyone else\n",
            "Harry's face wasn't going to be anyone elses\n",
            "Harry's face wasn't going to be anyone elses were\n",
            "Harry's face wasn't going to be anyone elses were now\n",
            "Harry's face wasn't going to be anyone elses were now,\n",
            "Harry's face wasn't going to be anyone elses were now, and\n",
            "Harry's face wasn't going to be anyone elses were now, anding\n",
            "Harry's face wasn't going to be anyone elses were now, anding the\n",
            "Harry's face wasn't going to be anyone elses were now, anding they\n",
            "Harry's face wasn't going to be anyone elses were now, anding they'\n",
            "Epoch 7 / 20\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.7127\n",
            "Step: 2000, average training loss over last 1000 steps: 3.2843\n",
            "Step: 3000, average training loss over last 1000 steps: 3.2899\n",
            "Step: 4000, average training loss over last 1000 steps: 3.2849\n",
            "Step: 5000, average training loss over last 1000 steps: 3.2795\n",
            "Step: 6000, average training loss over last 1000 steps: 3.2825\n",
            "Step: 7000, average training loss over last 1000 steps: 3.2797\n",
            "Step: 8000, average training loss over last 1000 steps: 3.2724\n",
            "Step: 9000, average training loss over last 1000 steps: 3.2744\n",
            "Step: 10000, average training loss over last 1000 steps: 3.2704\n",
            "Step: 12000, average training loss over last 1000 steps: 3.2685\n",
            "Step: 13000, average training loss over last 1000 steps: 3.2592\n",
            "Epoch: 7, validation loss: 9.3779\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry's\n",
            "Harry's arm\n",
            "Harry's armed\n",
            "Harry's armed his\n",
            "Harry's armed his w\n",
            "Harry's armed his w\n",
            "Harry's armed his wand\n",
            "Harry's armed his wand and\n",
            "Harry's armed his wand and\n",
            "Harry's armed his wand and Ro\n",
            "Harry's armed his wand and Ron\n",
            "Harry's armed his wand and Ron,\n",
            "Harry's armed his wand and Ron, and\n",
            "Harry's armed his wand and Ron, and\n",
            "Harry's armed his wand and Ron, and Her\n",
            "Harry's armed his wand and Ron, and Her\n",
            "Harry's armed his wand and Ron, and Hermi\n",
            "Harry's armed his wand and Ron, and Hermione\n",
            "Harry's armed his wand and Ron, and Hermione,\n",
            "Harry's armed his wand and Ron, and Hermione, and\n",
            "Epoch 8 / 20\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.6756\n",
            "Step: 2000, average training loss over last 1000 steps: 3.2563\n",
            "Step: 3000, average training loss over last 1000 steps: 3.2573\n",
            "Step: 4000, average training loss over last 1000 steps: 3.2539\n",
            "Step: 5000, average training loss over last 1000 steps: 3.2527\n",
            "Step: 6000, average training loss over last 1000 steps: 3.2554\n",
            "Step: 7000, average training loss over last 1000 steps: 3.2523\n",
            "Step: 8000, average training loss over last 1000 steps: 3.2496\n",
            "Step: 9000, average training loss over last 1000 steps: 3.2466\n",
            "Step: 10000, average training loss over last 1000 steps: 3.2412\n",
            "Step: 11000, average training loss over last 1000 steps: 3.2392\n",
            "Step: 12000, average training loss over last 1000 steps: 3.2390\n",
            "Step: 13000, average training loss over last 1000 steps: 3.2396\n",
            "Epoch: 8, validation loss: 9.4211\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, who\n",
            "Harry, who had\n",
            "Harry, who had never\n",
            "Harry, who had never been\n",
            "Harry, who had never been a\n",
            "Harry, who had never been a\n",
            "Harry, who had never been alive\n",
            "Harry, who had never been aliver\n",
            "Harry, who had never been aliver\n",
            "\n",
            "Harry, who had never been aliver\n",
            "to\n",
            "Harry, who had never been aliver\n",
            "to\n",
            "Harry, who had never been aliver\n",
            "to Ha\n",
            "Harry, who had never been aliver\n",
            "to Har\n",
            "Harry, who had never been aliver\n",
            "to Harry\n",
            "Harry, who had never been aliver\n",
            "to Harry's\n",
            "Harry, who had never been aliver\n",
            "to Harry's\n",
            "\n",
            "Harry, who had never been aliver\n",
            "to Harry's\n",
            "was\n",
            "Harry, who had never been aliver\n",
            "to Harry's\n",
            "wasn'\n",
            "Harry, who had never been aliver\n",
            "to Harry's\n",
            "wasn't\n",
            "Epoch 9 / 20\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.6490\n",
            "Step: 2000, average training loss over last 1000 steps: 3.2304\n",
            "Step: 3000, average training loss over last 1000 steps: 3.2301\n",
            "Step: 4000, average training loss over last 1000 steps: 3.2290\n",
            "Step: 5000, average training loss over last 1000 steps: 3.2288\n",
            "Step: 6000, average training loss over last 1000 steps: 3.2304\n",
            "Step: 7000, average training loss over last 1000 steps: 3.2266\n",
            "Step: 8000, average training loss over last 1000 steps: 3.2204\n",
            "Step: 9000, average training loss over last 1000 steps: 3.2274\n",
            "Step: 10000, average training loss over last 1000 steps: 3.2206\n",
            "Step: 11000, average training loss over last 1000 steps: 3.2215\n",
            "Step: 12000, average training loss over last 1000 steps: 3.2161\n",
            "Step: 13000, average training loss over last 1000 steps: 3.2182\n",
            "Epoch: 9, validation loss: 9.4558\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry's\n",
            "Harry's\n",
            "\n",
            "Harry's\n",
            "was\n",
            "Harry's\n",
            "wasn'\n",
            "Harry's\n",
            "wasn't\n",
            "Harry's\n",
            "wasn't going to\n",
            "Harry's\n",
            "wasn't going to be\n",
            "Harry's\n",
            "wasn't going to be?\n",
            "Harry's\n",
            "wasn't going to be?\"\n",
            "Harry's\n",
            "wasn't going to be?\"\n",
            "Harry's\n",
            "wasn't going to be?\" Ro\n",
            "Harry's\n",
            "wasn't going to be?\" Ron\n",
            "Harry's\n",
            "wasn't going to be?\" Ron,\n",
            "Harry's\n",
            "wasn't going to be?\" Ron, who\n",
            "Harry's\n",
            "wasn't going to be?\" Ron, who was\n",
            "Harry's\n",
            "wasn't going to be?\" Ron, who was\n",
            "\n",
            "Harry's\n",
            "wasn't going to be?\" Ron, who was\n",
            "the\n",
            "Harry's\n",
            "wasn't going to be?\" Ron, who was\n",
            "the\n",
            "Harry's\n",
            "wasn't going to be?\" Ron, who was\n",
            "the Dark\n",
            "Harry's\n",
            "wasn't going to be?\" Ron, who was\n",
            "the Dark\n",
            "Epoch 10 / 20\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.6248\n",
            "Step: 2000, average training loss over last 1000 steps: 3.2116\n",
            "Step: 3000, average training loss over last 1000 steps: 3.2142\n",
            "Step: 4000, average training loss over last 1000 steps: 3.2108\n",
            "Step: 5000, average training loss over last 1000 steps: 3.2066\n",
            "Step: 6000, average training loss over last 1000 steps: 3.2026\n",
            "Step: 7000, average training loss over last 1000 steps: 3.2108\n",
            "Step: 8000, average training loss over last 1000 steps: 3.2046\n",
            "Step: 9000, average training loss over last 1000 steps: 3.2061\n",
            "Step: 10000, average training loss over last 1000 steps: 3.2051\n",
            "Step: 11000, average training loss over last 1000 steps: 3.1978\n",
            "Step: 12000, average training loss over last 1000 steps: 3.2029\n",
            "Step: 13000, average training loss over last 1000 steps: 3.2004\n",
            "Epoch: 10, validation loss: 9.4974\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, and\n",
            "Harry, and\n",
            "Harry, and Ro\n",
            "Harry, and Ron\n",
            "Harry, and Ron,\n",
            "Harry, and Ron, and\n",
            "Harry, and Ron, and\n",
            "Harry, and Ron, and Her\n",
            "Harry, and Ron, and Her\n",
            "Harry, and Ron, and Hermi\n",
            "Harry, and Ron, and Hermione\n",
            "Harry, and Ron, and Hermione,\n",
            "Harry, and Ron, and Hermione, and\n",
            "Harry, and Ron, and Hermione, and\n",
            "Harry, and Ron, and Hermione, and Ro\n",
            "Harry, and Ron, and Hermione, and Ron\n",
            "Harry, and Ron, and Hermione, and Ron,\n",
            "Harry, and Ron, and Hermione, and Ron, and\n",
            "Harry, and Ron, and Hermione, and Ron, and\n",
            "Epoch 11 / 20\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.5974\n",
            "Step: 2000, average training loss over last 1000 steps: 3.1904\n",
            "Step: 3000, average training loss over last 1000 steps: 3.1970\n",
            "Step: 4000, average training loss over last 1000 steps: 3.1968\n",
            "Step: 5000, average training loss over last 1000 steps: 3.1916\n",
            "Step: 6000, average training loss over last 1000 steps: 3.1919\n",
            "Step: 7000, average training loss over last 1000 steps: 3.1941\n",
            "Step: 8000, average training loss over last 1000 steps: 3.1883\n",
            "Step: 9000, average training loss over last 1000 steps: 3.1882\n",
            "Step: 10000, average training loss over last 1000 steps: 3.1892\n",
            "Step: 11000, average training loss over last 1000 steps: 3.1846\n",
            "Step: 12000, average training loss over last 1000 steps: 3.1849\n",
            "Step: 13000, average training loss over last 1000 steps: 3.1799\n",
            "Epoch: 11, validation loss: 9.5177\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, and\n",
            "Harry, and\n",
            "Harry, and Ro\n",
            "Harry, and Ron\n",
            "Harry, and Ron,\n",
            "Harry, and Ron, and\n",
            "Harry, and Ron, and\n",
            "Harry, and Ron, and Her\n",
            "Harry, and Ron, and Her\n",
            "Harry, and Ron, and Hermi\n",
            "Harry, and Ron, and Hermione\n",
            "Harry, and Ron, and Hermione,\n",
            "Harry, and Ron, and Hermione, and\n",
            "Harry, and Ron, and Hermione, and\n",
            "Harry, and Ron, and Hermione, and Ro\n",
            "Harry, and Ron, and Hermione, and Ron\n",
            "Harry, and Ron, and Hermione, and Ron,\n",
            "Harry, and Ron, and Hermione, and Ron, and\n",
            "Harry, and Ron, and Hermione, and Ron, and\n",
            "Epoch 12 / 20\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.5822\n",
            "Step: 2000, average training loss over last 1000 steps: 3.1822\n",
            "Step: 3000, average training loss over last 1000 steps: 3.1801\n",
            "Step: 4000, average training loss over last 1000 steps: 3.1794\n",
            "Step: 5000, average training loss over last 1000 steps: 3.1784\n",
            "Step: 6000, average training loss over last 1000 steps: 3.1775\n",
            "Step: 7000, average training loss over last 1000 steps: 3.1743\n",
            "Step: 8000, average training loss over last 1000 steps: 3.1752\n",
            "Step: 9000, average training loss over last 1000 steps: 3.1746\n",
            "Step: 10000, average training loss over last 1000 steps: 3.1734\n",
            "Step: 11000, average training loss over last 1000 steps: 3.1740\n",
            "Step: 12000, average training loss over last 1000 steps: 3.1719\n",
            "Step: 13000, average training loss over last 1000 steps: 3.1715\n",
            "Epoch: 12, validation loss: 9.5468\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, and\n",
            "Harry, and\n",
            "Harry, and Ro\n",
            "Harry, and Ron\n",
            "Harry, and Ron,\n",
            "Harry, and Ron, who\n",
            "Harry, and Ron, who was\n",
            "Harry, and Ron, who was still\n",
            "Harry, and Ron, who was still looking\n",
            "Harry, and Ron, who was still looking at\n",
            "Harry, and Ron, who was still looking at\n",
            "Harry, and Ron, who was still looking at Ha\n",
            "Harry, and Ron, who was still looking at Har\n",
            "Harry, and Ron, who was still looking at Harry\n",
            "Harry, and Ron, who was still looking at Harry's\n",
            "Harry, and Ron, who was still looking at Harry's arm\n",
            "Harry, and Ron, who was still looking at Harry's armed\n",
            "Harry, and Ron, who was still looking at Harry's armed.\n",
            "\n",
            "Harry, and Ron, who was still looking at Harry's armed.\n",
            "\n",
            "\"\n",
            "Epoch 13 / 20\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.5667\n",
            "Step: 2000, average training loss over last 1000 steps: 3.1637\n",
            "Step: 3000, average training loss over last 1000 steps: 3.1642\n",
            "Step: 4000, average training loss over last 1000 steps: 3.1679\n",
            "Step: 5000, average training loss over last 1000 steps: 3.1651\n",
            "Step: 6000, average training loss over last 1000 steps: 3.1621\n",
            "Step: 7000, average training loss over last 1000 steps: 3.1675\n",
            "Step: 8000, average training loss over last 1000 steps: 3.1621\n",
            "Step: 9000, average training loss over last 1000 steps: 3.1618\n",
            "Step: 10000, average training loss over last 1000 steps: 3.1640\n",
            "Step: 11000, average training loss over last 1000 steps: 3.1653\n",
            "Step: 12000, average training loss over last 1000 steps: 3.1611\n",
            "Step: 13000, average training loss over last 1000 steps: 3.1619\n",
            "Epoch: 13, validation loss: 9.5732\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry's\n",
            "Harry's eyes\n",
            "Harry's eyes were\n",
            "Harry's eyes were s\n",
            "Harry's eyes were s\n",
            "Harry's eyes were scre\n",
            "Harry's eyes were scre\n",
            "Harry's eyes were scream\n",
            "Harry's eyes were screaming\n",
            "Harry's eyes were screaming the\n",
            "Harry's eyes were screaming the\n",
            "Harry's eyes were screaming the Po\n",
            "Harry's eyes were screaming the Pot\n",
            "Harry's eyes were screaming the Potter\n",
            "Harry's eyes were screaming the Potter,\n",
            "Harry's eyes were screaming the Potter, and\n",
            "Harry's eyes were screaming the Potter, and\n",
            "Harry's eyes were screaming the Potter, and Ha\n",
            "Harry's eyes were screaming the Potter, and Har\n",
            "Harry's eyes were screaming the Potter, and Harry\n",
            "Epoch 14 / 20\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.5593\n",
            "Step: 2000, average training loss over last 1000 steps: 3.1571\n",
            "Step: 3000, average training loss over last 1000 steps: 3.1525\n",
            "Step: 4000, average training loss over last 1000 steps: 3.1539\n",
            "Step: 5000, average training loss over last 1000 steps: 3.1536\n",
            "Step: 6000, average training loss over last 1000 steps: 3.1535\n",
            "Step: 7000, average training loss over last 1000 steps: 3.1558\n",
            "Step: 8000, average training loss over last 1000 steps: 3.1514\n",
            "Step: 9000, average training loss over last 1000 steps: 3.1508\n",
            "Step: 10000, average training loss over last 1000 steps: 3.1510\n",
            "Step: 11000, average training loss over last 1000 steps: 3.1508\n",
            "Step: 12000, average training loss over last 1000 steps: 3.1507\n",
            "Step: 13000, average training loss over last 1000 steps: 3.1545\n",
            "Epoch: 14, validation loss: 9.6053\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, and\n",
            "Harry, and\n",
            "Harry, and Her\n",
            "Harry, and Her\n",
            "Harry, and Hermi\n",
            "Harry, and Hermione\n",
            "Harry, and Hermione,\n",
            "Harry, and Hermione, and\n",
            "Harry, and Hermione, and\n",
            "Harry, and Hermione, and Her\n",
            "Harry, and Hermione, and Her\n",
            "Harry, and Hermione, and Hermi\n",
            "Harry, and Hermione, and Hermione\n",
            "Harry, and Hermione, and Hermione,\n",
            "Harry, and Hermione, and Hermione, and\n",
            "Harry, and Hermione, and Hermione, and\n",
            "Harry, and Hermione, and Hermione, and Her\n",
            "Harry, and Hermione, and Hermione, and Her\n",
            "Harry, and Hermione, and Hermione, and Hermi\n",
            "Epoch 15 / 20\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.5426\n",
            "Step: 2000, average training loss over last 1000 steps: 3.1498\n",
            "Step: 3000, average training loss over last 1000 steps: 3.1482\n",
            "Step: 4000, average training loss over last 1000 steps: 3.1437\n",
            "Step: 5000, average training loss over last 1000 steps: 3.1447\n",
            "Step: 6000, average training loss over last 1000 steps: 3.1402\n",
            "Step: 7000, average training loss over last 1000 steps: 3.1470\n",
            "Step: 8000, average training loss over last 1000 steps: 3.1441\n",
            "Step: 9000, average training loss over last 1000 steps: 3.1466\n",
            "Step: 10000, average training loss over last 1000 steps: 3.1407\n",
            "Step: 11000, average training loss over last 1000 steps: 3.1430\n",
            "Step: 12000, average training loss over last 1000 steps: 3.1407\n",
            "Step: 13000, average training loss over last 1000 steps: 3.1451\n",
            "Epoch: 15, validation loss: 9.6354\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry's\n",
            "Harry's face\n",
            "Harry's face,\n",
            "Harry's face, he\n",
            "Harry's face, he could\n",
            "Harry's face, he couldn'\n",
            "Harry's face, he couldn't\n",
            "Harry's face, he couldn't be\n",
            "Harry's face, he couldn't beli\n",
            "Harry's face, he couldn't belie\n",
            "Harry's face, he couldn't believe\n",
            "Harry's face, he couldn't believe it\n",
            "Harry's face, he couldn't believe it had\n",
            "Harry's face, he couldn't believe it had never\n",
            "Harry's face, he couldn't believe it had never been\n",
            "Harry's face, he couldn't believe it had never been a\n",
            "Harry's face, he couldn't believe it had never been a\n",
            "Harry's face, he couldn't believe it had never been alive\n",
            "Harry's face, he couldn't believe it had never been aliver\n",
            "Harry's face, he couldn't believe it had never been aliver\n",
            "Epoch 16 / 20\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.5357\n",
            "Step: 2000, average training loss over last 1000 steps: 3.1325\n",
            "Step: 3000, average training loss over last 1000 steps: 3.1388\n",
            "Step: 4000, average training loss over last 1000 steps: 3.1361\n",
            "Step: 5000, average training loss over last 1000 steps: 3.1352\n",
            "Step: 6000, average training loss over last 1000 steps: 3.1367\n",
            "Step: 7000, average training loss over last 1000 steps: 3.1398\n",
            "Step: 8000, average training loss over last 1000 steps: 3.1358\n",
            "Step: 9000, average training loss over last 1000 steps: 3.1380\n",
            "Step: 10000, average training loss over last 1000 steps: 3.1364\n",
            "Step: 11000, average training loss over last 1000 steps: 3.1331\n",
            "Step: 12000, average training loss over last 1000 steps: 3.1362\n",
            "Step: 13000, average training loss over last 1000 steps: 3.1388\n",
            "Epoch: 16, validation loss: 9.6328\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, \"\n",
            "Harry, \"but\n",
            "Harry, \"but\n",
            "Harry, \"but Ha\n",
            "Harry, \"but Har\n",
            "Harry, \"but Harry\n",
            "Harry, \"but Harry's\n",
            "Harry, \"but Harry's head\n",
            "Harry, \"but Harry's head.\n",
            "\n",
            "Harry, \"but Harry's head.\n",
            "\n",
            "\"\n",
            "Harry, \"but Harry's head.\n",
            "\n",
            "\"What\n",
            "Harry, \"but Harry's head.\n",
            "\n",
            "\"What's\n",
            "Harry, \"but Harry's head.\n",
            "\n",
            "\"What's\n",
            "Harry, \"but Harry's head.\n",
            "\n",
            "\"What's Ha\n",
            "Harry, \"but Harry's head.\n",
            "\n",
            "\"What's Har\n",
            "Harry, \"but Harry's head.\n",
            "\n",
            "\"What's Harry\n",
            "Harry, \"but Harry's head.\n",
            "\n",
            "\"What's Harry's\n",
            "Harry, \"but Harry's head.\n",
            "\n",
            "\"What's Harry's\n",
            "\n",
            "Harry, \"but Harry's head.\n",
            "\n",
            "\"What's Harry's\n",
            "s\n",
            "Epoch 17 / 20\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.5267\n",
            "Step: 2000, average training loss over last 1000 steps: 3.1257\n",
            "Step: 3000, average training loss over last 1000 steps: 3.1304\n",
            "Step: 4000, average training loss over last 1000 steps: 3.1262\n",
            "Step: 5000, average training loss over last 1000 steps: 3.1273\n",
            "Step: 6000, average training loss over last 1000 steps: 3.1309\n",
            "Step: 7000, average training loss over last 1000 steps: 3.1315\n",
            "Step: 8000, average training loss over last 1000 steps: 3.1308\n",
            "Step: 9000, average training loss over last 1000 steps: 3.1294\n",
            "Step: 10000, average training loss over last 1000 steps: 3.1332\n",
            "Step: 11000, average training loss over last 1000 steps: 3.1291\n",
            "Step: 12000, average training loss over last 1000 steps: 3.1278\n",
            "Step: 13000, average training loss over last 1000 steps: 3.1273\n",
            "Epoch: 17, validation loss: 9.6807\n",
            "Generated text: \n",
            "(4,)\n",
            "(4,)\n",
            "[138, 264, 62, 196]\n",
            "prompt: Harry\n",
            "Harry,\n",
            "Harry, and\n",
            "Harry, and\n",
            "Harry, and Her\n",
            "Harry, and Her\n",
            "Harry, and Hermi\n",
            "Harry, and Hermione\n",
            "Harry, and Hermione,\n",
            "Harry, and Hermione, and\n",
            "Harry, and Hermione, and\n",
            "Harry, and Hermione, and Her\n",
            "Harry, and Hermione, and Her\n",
            "Harry, and Hermione, and Hermi\n",
            "Harry, and Hermione, and Hermione\n",
            "Harry, and Hermione, and Hermione,\n",
            "Harry, and Hermione, and Hermione, and\n",
            "Harry, and Hermione, and Hermione, and\n",
            "Harry, and Hermione, and Hermione, and Ha\n",
            "Harry, and Hermione, and Hermione, and Har\n",
            "Harry, and Hermione, and Hermione, and Harry\n",
            "Epoch 18 / 20\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.5216\n",
            "Step: 2000, average training loss over last 1000 steps: 3.1215\n",
            "Step: 3000, average training loss over last 1000 steps: 3.1238\n",
            "Step: 4000, average training loss over last 1000 steps: 3.1243\n",
            "Step: 5000, average training loss over last 1000 steps: 3.1204\n",
            "Step: 6000, average training loss over last 1000 steps: 3.1252\n"
          ]
        }
      ],
      "source": [
        "train_loss, val_loss = train(model, train_loader, val_loader, torch.nn.functional.cross_entropy, optimizer, NUM_EPOCHS, DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3m2I7HS8MO8M"
      },
      "outputs": [],
      "source": [
        "text_sample = dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXfqcgw2MYSz"
      },
      "outputs": [],
      "source": [
        "print(text_sample[0])\n",
        "test_block = torch.tensor([text_sample[i] for i in range(8)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJXjZzCnMoSY"
      },
      "outputs": [],
      "source": [
        "test_block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0uewcO7MwY_"
      },
      "outputs": [],
      "source": [
        "test_list = test_block.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8mbTFkAMpic"
      },
      "outputs": [],
      "source": [
        "vocab.decode(test_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAN0uzbYhKa8"
      },
      "outputs": [],
      "source": [
        "test_block = torch.tensor([text_sample[i] for i in range(100)])\n",
        "test_list = test_block.tolist()\n",
        "vocab.decode(test_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrBrg4lRhOuv"
      },
      "outputs": [],
      "source": [
        "generate(model, \"in the dark\", device= DEVICE, n = 100)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}