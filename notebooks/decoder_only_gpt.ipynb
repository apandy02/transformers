{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7GuoX-D2SUi"
      },
      "source": [
        "# Decoder-only based GPT (language model)\n",
        "\n",
        "Here we take a transformer block, the decoder in particular, and use it for the task of language modeling. In general, this is how GPTs are trained. We will do this on a much smaller scale.\n",
        "\n",
        "We take everything we've already built and leverage it in the way Karpathy implements a character level LM here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "p0LzXILI2SUk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "from transformers.transformer_blocks import Transformer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tokenmonster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2fEKfFTtjVa5"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device('mps')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UP_BMiE52SUl",
        "outputId": "26f0aaee-4baa-4e32-9867-ef8d1af3288f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1548865\n"
          ]
        }
      ],
      "source": [
        "harry_potter_text = \" \"\n",
        "for i in range(1, 4): # first 4 books\n",
        "    with open(f'data/hp{i}.txt', 'r', encoding='utf-8') as f:\n",
        "        harry_potter_text += f.read()\n",
        "print(len(harry_potter_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WTsFKtP2SUm"
      },
      "source": [
        "## Tokenization\n",
        "Instead of character level, we're going to model this LM using a tokenizer. in particular, we're going to try to use OpenAI's tiktoken with the gpt2 50k tokenizer. This might end up being too large of a vocab size given compute constraints, but"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jNAlYTIf2SUm"
      },
      "outputs": [],
      "source": [
        "vocab = tokenmonster.load(\"fiction-1024-consistent-v1\")\n",
        "tokens = vocab.tokenize(\"This is a test.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya_OlheL2SUm",
        "outputId": "d5e1bd1e-2672-47e6-9488-7ba8c28a2153"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([138, 918, 108, 318, 202,  17], dtype=uint16)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nZ0pfOIT2SUm"
      },
      "outputs": [],
      "source": [
        "token_example = vocab.tokenize(\"hello world test monster tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1y5eYUD2SUn",
        "outputId": "a0ef8edf-ffe7-4934-8d2a-069b2bc106e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 37, 445, 174, 785, 318, 202, 465, 547, 321, 169, 181, 218,  62],\n",
              "      dtype=uint16)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qyadxmt2SUn",
        "outputId": "73f4164c-9da3-491a-c6a4-5f2f7b789f76"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['',\n",
              " ' hel',\n",
              " 'lo',\n",
              " ' world',\n",
              " ' te',\n",
              " 'st',\n",
              " ' mon',\n",
              " 'ster',\n",
              " ' to',\n",
              " 'ke',\n",
              " 'ni',\n",
              " 'ze',\n",
              " 'r']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[vocab.decode([token]) for token in token_example]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2ks4lD6p2SUn"
      },
      "outputs": [],
      "source": [
        "tokens = np.array(vocab.tokenize(harry_potter_text), dtype=np.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH_4N1qN2SUn",
        "outputId": "bf7fe23f-ba4a-4d32-ae00-afd1ce867b56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([642743]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "dataset = torch.tensor(tokens, dtype=torch.long)\n",
        "print(dataset.shape, dataset.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbbeNPTF2SUo",
        "outputId": "e32d1fea-ae04-467c-cbdd-bf08aee729df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Harry Potter and the Sorcerer's Stone\n",
            "CHAPTER ONE\n",
            "THE BOY WHO LIVED\n",
            "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved\n",
            "train set size: 514174, test: 64271, val: 64273, data size: 642743, dataset_size: 642718\n"
          ]
        }
      ],
      "source": [
        "class HPDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of possible sequences\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Fetch a single sequence x and its corresponding target y\n",
        "        x = self.data[idx:idx + self.block_size]\n",
        "        y = self.data[idx + 1:idx + self.block_size + 1]\n",
        "        return x, y\n",
        "\n",
        "BLOCK_SIZE = 25\n",
        "hp_data = HPDataset(dataset, BLOCK_SIZE)\n",
        "\n",
        "test_block = torch.tensor([dataset[i] for i in range(100)])\n",
        "test_list = test_block.tolist()\n",
        "print(vocab.decode(test_list))\n",
        "\n",
        "train_size = int(len(hp_data) * 0.8)\n",
        "test_size = int(len(hp_data) * 0.1)\n",
        "val_size = len(hp_data) - train_size - test_size\n",
        "\n",
        "print(f\"train set size: {train_size}, test: {test_size}, val: {val_size}, data size: {len(dataset)}, dataset_size: {hp_data.__len__()}\")\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(hp_data, [train_size, val_size, test_size])\n",
        "\n",
        "batch_size = 64\n",
        "train_loader, val_loader, test_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True), DataLoader(val_dataset, batch_size=batch_size, shuffle=True), DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9AWjLIa2SUo",
        "outputId": "31919c44-0484-4b2f-efee-9715b657b898"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8034\n",
            "1005\n",
            "1005\n"
          ]
        }
      ],
      "source": [
        "print(len(train_loader))\n",
        "print(len(test_loader))\n",
        "print(len(val_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5k9W9isInum",
        "outputId": "92355f01-7269-4503-ccd9-4d98e35b0ccc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([152, 472, 759, 123, 217, 343, 194,  15, 412,  36, 264,  62, 196, 129,\n",
            "         38, 124, 132, 123,  37, 266, 195, 516, 315, 161, 537]), tensor([472, 759, 123, 217, 343, 194,  15, 412,  36, 264,  62, 196, 129,  38,\n",
            "        124, 132, 123,  37, 266, 195, 516, 315, 161, 537, 864]))\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset.__getitem__(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9D0dkIBU2SUo"
      },
      "outputs": [],
      "source": [
        "class rowlingGPT(nn.Module):\n",
        "    \"\"\"\n",
        "    JK Rowling would probably not approve\n",
        "    \"\"\"\n",
        "    def __init__(self, d_k, d_model, d_v, d_ff, num_heads, num_layers, vocab_size, dropout=0.1) -> None:\n",
        "        super().__init__()\n",
        "        self.decoder_transformer = Transformer(d_k, d_model, d_v, d_ff, num_heads, num_layers, vocab_size=vocab_size, mask=True, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.decoder_transformer(x)\n",
        "        return self.fc(self.layer_norm(out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "oHC-Secr2SUo"
      },
      "outputs": [],
      "source": [
        "def compute_loss(y_target, y_pred, loss_function):\n",
        "    B, T, C = y_pred.shape\n",
        "    y_pred = y_pred.view(B*T, C)\n",
        "    _, max_indices = torch.max(y_pred, dim=1)\n",
        "    y_target_list = y_target.tolist()\n",
        "    max_indices = max_indices.tolist()\n",
        "    y_target = y_target.view(B*T)\n",
        "    return loss_function(y_pred, y_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5Pc9PN-Hnrv-"
      },
      "outputs": [],
      "source": [
        "def generate(model, prompt: str, device,n = 200, block_size=BLOCK_SIZE):\n",
        "  prompt_array = vocab.tokenize(prompt)\n",
        "  prompt_array = np.array(prompt_array[:block_size], dtype=np.int16)\n",
        "  decoded = vocab.decode(prompt_array)\n",
        "  print(f\"prompt: {decoded}\")\n",
        "  cumulative_array = prompt_array\n",
        "  for i in range(n):\n",
        "    prompt_tensor = torch.tensor(prompt_array, dtype=torch.long).to(device)\n",
        "    next_token = predict_next_token(model, prompt_tensor.unsqueeze(0))\n",
        "    next_token_np = next_token.cpu().numpy().flatten()\n",
        "    cumulative_array = np.append(cumulative_array, next_token_np)\n",
        "    prompt_array = np.append(prompt_array[1:], next_token_np)\n",
        "    test_list = cumulative_array.tolist()\n",
        "  print(vocab.decode(test_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "XQ7fkR1zJlI4"
      },
      "outputs": [],
      "source": [
        "def predict_next_token(model, block):\n",
        "  with torch.no_grad():\n",
        "    y_pred = model(block)\n",
        "    token_probs = nn.functional.softmax(y_pred, dim=-1)\n",
        "    _, max_idx = torch.max(token_probs, dim=-1)\n",
        "  return max_idx.squeeze()[-1]  # return only the last next token prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "W2wS6j8Y2SUo"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, loss_function, optim, epochs, device):\n",
        "    losses = [] #group losses for loss visualization\n",
        "    running_loss = 0.0\n",
        "    val_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        print(\"Epoch %d / %d\" % (epoch+1, epochs))\n",
        "        print(\"-\"*10)\n",
        "\n",
        "        for i, batch_data in enumerate(train_loader):\n",
        "            x, y = batch_data\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_pred = model(x)\n",
        "\n",
        "            loss = compute_loss(y, y_pred, loss_function)\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            running_loss += loss.item()\n",
        "            losses.append(loss)\n",
        "\n",
        "            if (i+1) % 1000 == 0:\n",
        "                print(\"Step: {}, average training loss over last 1000 steps: {:.4f}\".format(i+1, running_loss/1000))\n",
        "                running_loss = 0.0\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for i, batch_data in enumerate(val_loader):\n",
        "                x, y = batch_data  # FIX: use x, y order for validation too\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                y_pred = model(x)\n",
        "                loss = compute_loss(y, y_pred, loss_function)\n",
        "                _, predicted_labels = torch.max(y_pred, 1)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "            val_losses.append(val_loss)\n",
        "        print(\"Epoch: {}, validation loss: {:.4f}\".format(epoch+1, val_loss/len(val_loader)))\n",
        "        print(\"Generated text: \")\n",
        "        generate(model, \"Harry\", device=DEVICE, n=20)\n",
        "\n",
        "    return losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "yj_S3bSm2SUo"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 6e-4\n",
        "NUM_EPOCHS = 18\n",
        "DROPOUT = 0.2\n",
        "D_MODEL = 1024\n",
        "NUM_HEADS = 8\n",
        "D_K = int(D_MODEL / NUM_HEADS)\n",
        "D_V = D_K\n",
        "D_FF = D_MODEL * 4\n",
        "NUM_LAYERS = 2\n",
        "VOCAB_SIZE = vocab.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "f314V3J12SUo"
      },
      "outputs": [],
      "source": [
        "model = rowlingGPT(D_K, D_MODEL, D_V, D_FF, num_heads=NUM_HEADS, num_layers=NUM_LAYERS, vocab_size=VOCAB_SIZE)\n",
        "model = model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "iqdfCB9C2SUo"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWmZ65uO6XhP",
        "outputId": "eac1924c-b0e5-452b-c622-f397a4afb7b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='mps')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxStEdDG2SUp",
        "outputId": "81050059-7323-4cc7-c80c-67e36ae1934a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.1862\n",
            "Step: 2000, average training loss over last 1000 steps: 2.6507\n",
            "Step: 3000, average training loss over last 1000 steps: 2.4584\n",
            "Step: 4000, average training loss over last 1000 steps: 2.3036\n",
            "Step: 5000, average training loss over last 1000 steps: 2.1652\n",
            "Step: 6000, average training loss over last 1000 steps: 2.0403\n",
            "Step: 7000, average training loss over last 1000 steps: 1.9243\n",
            "Step: 8000, average training loss over last 1000 steps: 1.8099\n",
            "Epoch: 1, validation loss: 1.5744\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry, who was still watching them. He was going to ask Harry, who was still\n",
            "Epoch 2 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 1.7296\n",
            "Step: 2000, average training loss over last 1000 steps: 1.5897\n",
            "Step: 3000, average training loss over last 1000 steps: 1.5146\n",
            "Step: 4000, average training loss over last 1000 steps: 1.4422\n",
            "Step: 5000, average training loss over last 1000 steps: 1.3813\n",
            "Step: 6000, average training loss over last 1000 steps: 1.3247\n",
            "Step: 7000, average training loss over last 1000 steps: 1.2767\n",
            "Step: 8000, average training loss over last 1000 steps: 1.2266\n",
            "Epoch: 2, validation loss: 0.9931\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry was sure he knew what that was about. Neville, her face set. He seemed to have\n",
            "Epoch 3 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 1.1993\n",
            "Step: 2000, average training loss over last 1000 steps: 1.1330\n",
            "Step: 3000, average training loss over last 1000 steps: 1.1038\n",
            "Step: 4000, average training loss over last 1000 steps: 1.0735\n",
            "Step: 5000, average training loss over last 1000 steps: 1.0539\n",
            "Step: 6000, average training loss over last 1000 steps: 1.0242\n",
            "Step: 7000, average training loss over last 1000 steps: 1.0016\n",
            "Step: 8000, average training loss over last 1000 steps: 0.9823\n",
            "Epoch: 3, validation loss: 0.7712\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry, who was still smiling. \"Come on, now, Angelina,\n",
            "Epoch 4 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.9684\n",
            "Step: 2000, average training loss over last 1000 steps: 0.9265\n",
            "Step: 3000, average training loss over last 1000 steps: 0.9121\n",
            "Step: 4000, average training loss over last 1000 steps: 0.8985\n",
            "Step: 5000, average training loss over last 1000 steps: 0.8823\n",
            "Step: 6000, average training loss over last 1000 steps: 0.8721\n",
            "Step: 7000, average training loss over last 1000 steps: 0.8584\n",
            "Step: 8000, average training loss over last 1000 steps: 0.8451\n",
            "Epoch: 4, validation loss: 0.6579\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry didn't like the look on Malfoy, who was now trying to peer into\n",
            "Epoch 5 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.8387\n",
            "Step: 2000, average training loss over last 1000 steps: 0.8065\n",
            "Step: 3000, average training loss over last 1000 steps: 0.7996\n",
            "Step: 4000, average training loss over last 1000 steps: 0.7921\n",
            "Step: 5000, average training loss over last 1000 steps: 0.7797\n",
            "Step: 6000, average training loss over last 1000 steps: 0.7721\n",
            "Step: 7000, average training loss over last 1000 steps: 0.7634\n",
            "Step: 8000, average training loss over last 1000 steps: 0.7527\n",
            "Epoch: 5, validation loss: 0.5901\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry's heart leapt - \"you'll be lucky he was so thi\n",
            "Epoch 6 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.7487\n",
            "Step: 2000, average training loss over last 1000 steps: 0.7269\n",
            "Step: 3000, average training loss over last 1000 steps: 0.7201\n",
            "Step: 4000, average training loss over last 1000 steps: 0.7136\n",
            "Step: 5000, average training loss over last 1000 steps: 0.7071\n",
            "Step: 6000, average training loss over last 1000 steps: 0.7003\n",
            "Step: 7000, average training loss over last 1000 steps: 0.6943\n",
            "Step: 8000, average training loss over last 1000 steps: 0.6878\n",
            "Epoch: 6, validation loss: 0.5450\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry and Ron were still at Hogwarts, by his large and t\n",
            "Epoch 7 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.6851\n",
            "Step: 2000, average training loss over last 1000 steps: 0.6624\n",
            "Step: 3000, average training loss over last 1000 steps: 0.6606\n",
            "Step: 4000, average training loss over last 1000 steps: 0.6558\n",
            "Step: 5000, average training loss over last 1000 steps: 0.6511\n",
            "Step: 6000, average training loss over last 1000 steps: 0.6458\n",
            "Step: 7000, average training loss over last 1000 steps: 0.6420\n",
            "Step: 8000, average training loss over last 1000 steps: 0.6367\n",
            "Epoch: 7, validation loss: 0.5098\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry, who was still lying motionless on the ground, trying hard not to smile. And\n",
            "Epoch 8 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.6346\n",
            "Step: 2000, average training loss over last 1000 steps: 0.6148\n",
            "Step: 3000, average training loss over last 1000 steps: 0.6145\n",
            "Step: 4000, average training loss over last 1000 steps: 0.6109\n",
            "Step: 5000, average training loss over last 1000 steps: 0.6077\n",
            "Step: 6000, average training loss over last 1000 steps: 0.6038\n",
            "Step: 7000, average training loss over last 1000 steps: 0.6022\n",
            "Step: 8000, average training loss over last 1000 steps: 0.5946\n",
            "Epoch: 8, validation loss: 0.4855\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry, who was still purring on the bed.\n",
            "����\"Oh,\n",
            "Epoch 9 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.5962\n",
            "Step: 2000, average training loss over last 1000 steps: 0.5792\n",
            "Step: 3000, average training loss over last 1000 steps: 0.5807\n",
            "Step: 4000, average training loss over last 1000 steps: 0.5760\n",
            "Step: 5000, average training loss over last 1000 steps: 0.5745\n",
            "Step: 6000, average training loss over last 1000 steps: 0.5708\n",
            "Step: 7000, average training loss over last 1000 steps: 0.5695\n",
            "Step: 8000, average training loss over last 1000 steps: 0.5670\n",
            "Epoch: 9, validation loss: 0.4686\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry, who was still eyeing them suspital wing. He was\n",
            "Epoch 10 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.5652\n",
            "Step: 2000, average training loss over last 1000 steps: 0.5518\n",
            "Step: 3000, average training loss over last 1000 steps: 0.5517\n",
            "Step: 4000, average training loss over last 1000 steps: 0.5500\n",
            "Step: 5000, average training loss over last 1000 steps: 0.5484\n",
            "Step: 6000, average training loss over last 1000 steps: 0.5465\n",
            "Step: 7000, average training loss over last 1000 steps: 0.5449\n",
            "Step: 8000, average training loss over last 1000 steps: 0.5439\n",
            "Epoch: 10, validation loss: 0.4563\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry was sure he knew what was happening. He had shouted. \"Let\n",
            "Epoch 11 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.5424\n",
            "Step: 2000, average training loss over last 1000 steps: 0.5298\n",
            "Step: 3000, average training loss over last 1000 steps: 0.5301\n",
            "Step: 4000, average training loss over last 1000 steps: 0.5312\n",
            "Step: 5000, average training loss over last 1000 steps: 0.5297\n",
            "Step: 6000, average training loss over last 1000 steps: 0.5278\n",
            "Step: 7000, average training loss over last 1000 steps: 0.5264\n",
            "Step: 8000, average training loss over last 1000 steps: 0.5257\n",
            "Epoch: 11, validation loss: 0.4503\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry, who could feel him quivering. \"Impaled upon your own s\n",
            "Epoch 12 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.5261\n",
            "Step: 2000, average training loss over last 1000 steps: 0.5122\n",
            "Step: 3000, average training loss over last 1000 steps: 0.5134\n",
            "Step: 4000, average training loss over last 1000 steps: 0.5137\n",
            "Step: 5000, average training loss over last 1000 steps: 0.5144\n",
            "Step: 6000, average training loss over last 1000 steps: 0.5149\n",
            "Step: 7000, average training loss over last 1000 steps: 0.5122\n",
            "Step: 8000, average training loss over last 1000 steps: 0.5124\n",
            "Epoch: 12, validation loss: 0.4442\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry looked up, hardly daring to believe before now that this umbre\n",
            "Epoch 13 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.5129\n",
            "Step: 2000, average training loss over last 1000 steps: 0.5010\n",
            "Step: 3000, average training loss over last 1000 steps: 0.5023\n",
            "Step: 4000, average training loss over last 1000 steps: 0.5040\n",
            "Step: 5000, average training loss over last 1000 steps: 0.5028\n",
            "Step: 6000, average training loss over last 1000 steps: 0.5024\n",
            "Step: 7000, average training loss over last 1000 steps: 0.5020\n",
            "Step: 8000, average training loss over last 1000 steps: 0.5023\n",
            "Epoch: 13, validation loss: 0.4404\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry and Ron had given him for Christmas, and the\n",
            "Epoch 14 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.5020\n",
            "Step: 2000, average training loss over last 1000 steps: 0.4912\n",
            "Step: 3000, average training loss over last 1000 steps: 0.4934\n",
            "Step: 4000, average training loss over last 1000 steps: 0.4929\n",
            "Step: 5000, average training loss over last 1000 steps: 0.4936\n",
            "Step: 6000, average training loss over last 1000 steps: 0.4939\n",
            "Step: 7000, average training loss over last 1000 steps: 0.4938\n",
            "Step: 8000, average training loss over last 1000 steps: 0.4938\n",
            "Epoch: 14, validation loss: 0.4364\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry looked up at the owner of the hand on his shoulder and felt a bucket o' b\n",
            "Epoch 15 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.4940\n",
            "Step: 2000, average training loss over last 1000 steps: 0.4836\n",
            "Step: 3000, average training loss over last 1000 steps: 0.4850\n",
            "Step: 4000, average training loss over last 1000 steps: 0.4858\n",
            "Step: 5000, average training loss over last 1000 steps: 0.4866\n",
            "Step: 6000, average training loss over last 1000 steps: 0.4871\n",
            "Step: 7000, average training loss over last 1000 steps: 0.4876\n",
            "Step: 8000, average training loss over last 1000 steps: 0.4868\n",
            "Epoch: 15, validation loss: 0.4346\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry, who had expected to live longer than three years or so,\" said the wi\n",
            "Epoch 16 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.4867\n",
            "Step: 2000, average training loss over last 1000 steps: 0.4763\n",
            "Step: 3000, average training loss over last 1000 steps: 0.4780\n",
            "Step: 4000, average training loss over last 1000 steps: 0.4797\n",
            "Step: 5000, average training loss over last 1000 steps: 0.4803\n",
            "Step: 6000, average training loss over last 1000 steps: 0.4810\n",
            "Step: 7000, average training loss over last 1000 steps: 0.4811\n",
            "Step: 8000, average training loss over last 1000 steps: 0.4812\n",
            "Epoch: 16, validation loss: 0.4316\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry and Hermione, who were sitting in front of him.\n",
            "�\n",
            "Epoch 17 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.4818\n",
            "Step: 2000, average training loss over last 1000 steps: 0.4717\n",
            "Step: 3000, average training loss over last 1000 steps: 0.4720\n",
            "Step: 4000, average training loss over last 1000 steps: 0.4741\n",
            "Step: 5000, average training loss over last 1000 steps: 0.4764\n",
            "Step: 6000, average training loss over last 1000 steps: 0.4759\n",
            "Step: 7000, average training loss over last 1000 steps: 0.4769\n",
            "Step: 8000, average training loss over last 1000 steps: 0.4774\n",
            "Epoch: 17, validation loss: 0.4312\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry and Ron, whose eyes he could just make out the name \"T M\n",
            "Epoch 18 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.4774\n",
            "Step: 2000, average training loss over last 1000 steps: 0.4667\n",
            "Step: 3000, average training loss over last 1000 steps: 0.4687\n",
            "Step: 4000, average training loss over last 1000 steps: 0.4702\n",
            "Step: 5000, average training loss over last 1000 steps: 0.4709\n",
            "Step: 6000, average training loss over last 1000 steps: 0.4728\n",
            "Step: 7000, average training loss over last 1000 steps: 0.4719\n",
            "Step: 8000, average training loss over last 1000 steps: 0.4724\n",
            "Epoch: 18, validation loss: 0.4292\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry looked up at the giant chuckled. \"Little tyke,\" chor\n"
          ]
        }
      ],
      "source": [
        "train_loss, val_loss = train(model, train_loader, val_loader, torch.nn.functional.cross_entropy, optimizer, NUM_EPOCHS, DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "3m2I7HS8MO8M"
      },
      "outputs": [],
      "source": [
        "text_sample = dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXfqcgw2MYSz",
        "outputId": "5720fbfe-2b96-4e8c-8d4a-b381d545144e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(36)\n"
          ]
        }
      ],
      "source": [
        "print(text_sample[0])\n",
        "test_block = torch.tensor([text_sample[i] for i in range(8)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJXjZzCnMoSY",
        "outputId": "d8ec8bcc-d77b-49ab-8629-26f2606d0861"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 36, 264,  62, 196,  36, 301,  64, 382])"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "m0uewcO7MwY_"
      },
      "outputs": [],
      "source": [
        "test_list = test_block.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "P8mbTFkAMpic",
        "outputId": "307dc970-d12e-4da3-da61-dabbba13eb0b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' Harry Potter'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab.decode(test_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "aAN0uzbYhKa8",
        "outputId": "0dd8298f-553e-4034-e5ca-243c58d92fa4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\" Harry Potter and the Sorcerer's Stone\\nCHAPTER ONE\\nTHE BOY WHO LIVED\\nMr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved\""
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_block = torch.tensor([text_sample[i] for i in range(100)])\n",
        "test_list = test_block.tolist()\n",
        "vocab.decode(test_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrBrg4lRhOuv",
        "outputId": "5d17ee92-d8f7-48db-e12f-ff9a6cc6072a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prompt: Harry Potter and the monkeys\n",
            "Harry Potter and the monkeyship, sit. Doesn't want to be seen. She's a horrible mess. Saw her running through the landscape up on the fourth floor, sir, dodging between the trees. Crying something dreadful,\" he said happily. \"Poor thing,\" he added unconvincingly.\n",
            "����\"Did she say who did it?\" said Dumbledore, still sounding amused. \"Search the skies, if you will.... Hagrid, I could do with a cup of tea. Or a large brandy.\"\n",
            "����\"O' -- o' course, Professor,\" said Hagrid, who sounded weak with happiness. \"Come in, come in....\"\n",
            "����Harry and Hermione dashed across to him.\n",
            "����\"Ron -- are you okay?\"\n",
            "����But they all refused to say.\n",
            "����\"She says the crystal ball's told her that if I tell you, I'll have a horrible accident!\" squeaked Neville as he clambered back down the ladder toward Harry and Ron, who had now reached the landing.\n",
            "����\"That's convenient,\" snorted Ron. \"You know, I'm starting to think Hermione was right about her\" -- he jabbed his thumb toward the trapdoor overhead -- \"she's a right old fraud.\"\n",
            "����\"Yeah, maybe,\" said Harry as they reached the entrance hall and crossed into the Great Hall. It had been decorated with hundreds and hundreds of candle-filled pumpkins, a cloud of fluttering live bats, and many flaming orange streamers, which were swimming lazily across the stormy ceiling like brilliant watersnakes.\n",
            "����The food was delicious; even Hermione and Ron, who were full to bursting with Honeydukes sweets, managed second helpings of everything. Harry kept glancing at the staff table. Professor Lupin\n",
            "����looked cheerful and as well as he ever did; he was talking animatedly to tiny little Professor Flitwick, the Charms teacher, had already decorated his classroom with shimmering lights that turned out to be real, fluttering fairies. The students were all happily discussing their plans for the holidays. Both Ron and Hermione had decided to remain at Hogwarts, and though Ron said it was because he couldn't stand two weeks with Percy, and Hermione insisted she needed to use the library, Harry wasn't fooled; they were doing it to keep him company, and he was very grateful.\n",
            "����To everyone's delight except Harry's, there was to be another Hogsmeade visit. Neither Ron nor Hermione felt like going, however, so they and Harry wandered onto the grounds, still talking about the extraordinary events of the previous night and wondering where Sirius and Buckbea\n"
          ]
        }
      ],
      "source": [
        "generate(model, \"Harry Potter and the monkeys\", device= DEVICE, n = 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
