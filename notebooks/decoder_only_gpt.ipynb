{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7GuoX-D2SUi"
      },
      "source": [
        "# Decoder-only based GPT (language model)\n",
        "\n",
        "Here we take a transformer block, the decoder in particular, and use it for the task of language modeling. In general, this is how GPTs are trained. We will do this on a much smaller scale.\n",
        "\n",
        "We take everything we've already built and leverage it in the way Karpathy implements a character level LM here:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ptBhD9c3kTU",
        "outputId": "745c861c-7bad-4c5c-f5e1-e6aee573e540"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/Transformers/models/transformer_blocks.py .\n",
        "!cp drive/MyDrive/Transformers/models/modules.py .\n",
        "!cp -r drive/MyDrive/Transformers/data/ ."
      ],
      "metadata": {
        "id": "DAnCCtxa3i6o"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenmonster"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_XqFHTe4Ozn",
        "outputId": "00337261-396a-4f7f-d8f2-4cefdf9cfc2e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenmonster in /usr/local/lib/python3.10/dist-packages (1.1.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "p0LzXILI2SUk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from torch.utils.data import random_split\n",
        "import sys\n",
        "sys.path.append(\"~/\")\n",
        "from transformer_blocks import Transformer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tokenmonster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "UP_BMiE52SUl",
        "outputId": "688e230a-1d48-48ae-cfcd-40cc2afc12b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2652650\n"
          ]
        }
      ],
      "source": [
        "harry_potter_text = \" \"\n",
        "for i in range(4):\n",
        "    book_num = i+1\n",
        "    with open(f'/content/data/hp{book_num}.txt', 'r', encoding='utf-8') as f:\n",
        "        harry_potter_text += f.read()\n",
        "print(len(harry_potter_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTW6wJWB2SUl"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WTsFKtP2SUm"
      },
      "source": [
        "## Tokenization\n",
        "Instead of character level, we're going to model this LM using a tokenizer. in particular, we're going to try to use OpenAI's tiktoken with the gpt2 50k tokenizer. This might end up being too large of a vocab size given compute constraints, but"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "jNAlYTIf2SUm"
      },
      "outputs": [],
      "source": [
        "vocab = tokenmonster.load(\"fiction-1024-consistent-v1\")\n",
        "tokens = vocab.tokenize(\"This is a test.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ya_OlheL2SUm",
        "outputId": "6ae0ee3b-56e7-45a0-d553-13572667b23d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([138, 918, 108, 318, 202,  17], dtype=uint16)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "nZ0pfOIT2SUm"
      },
      "outputs": [],
      "source": [
        "token_example = vocab.tokenize(\"hello world test monster tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "y1y5eYUD2SUn",
        "outputId": "cac8be89-659c-4f6f-cc3b-07024455444c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 37, 445, 174, 785, 318, 202, 465, 547, 321, 169, 181, 218,  62],\n",
              "      dtype=uint16)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "token_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "-qyadxmt2SUn",
        "outputId": "105aaa9b-4e19-4fd5-f81c-5981fa57087b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " ' hel',\n",
              " 'lo',\n",
              " ' world',\n",
              " ' te',\n",
              " 'st',\n",
              " ' mon',\n",
              " 'ster',\n",
              " ' to',\n",
              " 'ke',\n",
              " 'ni',\n",
              " 'ze',\n",
              " 'r']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "[vocab.decode([token]) for token in token_example]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "2ks4lD6p2SUn"
      },
      "outputs": [],
      "source": [
        "tokens = np.array(vocab.tokenize(harry_potter_text), dtype=np.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "WH_4N1qN2SUn",
        "outputId": "29088660-43a2-4164-810c-8c5ce3737964",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1050223]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "dataset = torch.tensor(tokens, dtype=torch.long)\n",
        "print(dataset.shape, dataset.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "rOqbUh7U2SUn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad44179c-0c42-4fdf-af0e-a8bb0c894bfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Harry Potter and the Sorcerer's Stone\n",
            "\n",
            "\n",
            "CHAPTER ONE\n",
            "\n",
            "THE BOY WHO LIVED\n",
            "\n",
            "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say\n",
            "that they were perfectly normal, thank you very much. They were the last\n",
            "people you'd expect to be\n",
            "rry muttered.  \"Listen, you'd better go and get someone -\"\n",
            "\"Dumbledore!\" gasped Mr. Crouch. He reached out and seized a handful of Harrys robes, dragging him closer, though his eyes were staring over Harry's head.  \"I need... see\n",
            " Harry.\n",
            "\"Would Harry Potter like a cup of tea?\" he squeaked loudly, over Winky's sobs.\n",
            "\"Er - yeah, okay,\" said Harry.\n",
            "Instantly, about six house-elves came trotting up behind him, bearing a large silver tray laden with a te\n"
          ]
        }
      ],
      "source": [
        "train_size = int(len(dataset) * 0.8)\n",
        "test_size = int(len(dataset) * 0.1)\n",
        "val_size = len(dataset) - train_size - test_size\n",
        "\n",
        "train_data, test_data, val_data = dataset[:train_size], dataset[train_size:train_size+test_size], dataset[train_size+test_size:]\n",
        "\n",
        "train_block = torch.tensor([train_data[i] for i in range(100)])\n",
        "train_list = train_block.tolist()\n",
        "print(vocab.decode(train_list))\n",
        "\n",
        "val_block = torch.tensor([val_data[i] for i in range(100)])\n",
        "val_list = val_block.tolist()\n",
        "print(vocab.decode(val_list))\n",
        "\n",
        "test_block = torch.tensor([test_data[i] for i in range(100)])\n",
        "test_list = test_block.tolist()\n",
        "print(vocab.decode(test_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "VoWJLYvs2SUo",
        "outputId": "dd5ca769-2191-4217-ff05-4949c0ef7425",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train set size: 840178, test: 105022, val: 105023\n"
          ]
        }
      ],
      "source": [
        "print(f\"train set size: {train_size}, test: {test_size}, val: {val_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "RbbeNPTF2SUo"
      },
      "outputs": [],
      "source": [
        "class HPDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of possible sequences\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Fetch a single sequence x and its corresponding target y\n",
        "        x = self.data[idx:idx + self.block_size]\n",
        "        y = self.data[idx + 1:idx + self.block_size + 1]\n",
        "        return x, y\n",
        "\n",
        "BLOCK_SIZE = 8\n",
        "train_dataset, val_dataset, test_dataset = HPDataset(train_data, BLOCK_SIZE), HPDataset(val_data, BLOCK_SIZE), HPDataset(test_data, BLOCK_SIZE)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader, val_loader, test_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True), DataLoader(val_dataset, batch_size=batch_size, shuffle=True), DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "H9AWjLIa2SUo",
        "outputId": "b9424537-ea5f-4abe-cf4a-b4512357fb0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13128\n",
            "1641\n",
            "1641\n"
          ]
        }
      ],
      "source": [
        "print(len(train_loader))\n",
        "print(len(test_loader))\n",
        "print(len(val_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "9D0dkIBU2SUo"
      },
      "outputs": [],
      "source": [
        "class zeptoGPT(nn.Module):\n",
        "    \"\"\"\n",
        "    zepto because it's a really small GPT\n",
        "    \"\"\"\n",
        "    def __init__(self, d_k, d_model, d_v, d_ff, num_heads, num_layers, vocab_size, dropout=0.1) -> None:\n",
        "        super().__init__()\n",
        "        self.decoder_transformer = Transformer(d_k, d_model, d_v, d_ff, num_heads, num_layers, vocab_size=vocab_size, mask=True, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.decoder_transformer(x)\n",
        "        return self.fc(self.layer_norm(out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "oHC-Secr2SUo"
      },
      "outputs": [],
      "source": [
        "def compute_loss(y_target, y_pred, loss_function):\n",
        "    B, T, C = y_pred.shape\n",
        "    y_pred = y_pred.view(B*T, C)\n",
        "    _, max_indices = torch.max(y_pred, dim=1)\n",
        "    y_target_list = y_target.tolist()\n",
        "    max_indices = max_indices.tolist()\n",
        "    y_target = y_target.view(B*T)\n",
        "    return loss_function(y_pred, y_target)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, prompt: str, device,n = 50, block_size=BLOCK_SIZE):\n",
        "  prompt_array = vocab.tokenize(prompt)\n",
        "  print(prompt_array.shape)\n",
        "  prompt_array = np.array(prompt_array[:block_size], dtype=np.int16)\n",
        "  print(prompt_array.shape)\n",
        "  print(prompt_array.tolist())\n",
        "  decoded = vocab.decode(prompt_array)\n",
        "  print(f\"prompt: {decoded}\")\n",
        "  cumulative_array = prompt_array\n",
        "  for i in range(n):\n",
        "    prompt_tensor = torch.tensor(prompt_array, dtype=torch.long).to(device)\n",
        "    next_token = predict_next_token(model, prompt_tensor.unsqueeze(0))\n",
        "    next_token_np = next_token.cpu().numpy().flatten()\n",
        "    cumulative_array = np.append(cumulative_array, next_token_np)\n",
        "    prompt_array = np.append(prompt_array[1:], next_token_np)\n",
        "    test_list = cumulative_array.tolist()\n",
        "    print(vocab.decode(test_list))"
      ],
      "metadata": {
        "id": "5Pc9PN-Hnrv-"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_token(model, block):\n",
        "  with torch.no_grad():\n",
        "    y_pred = model(block)\n",
        "    token_probs = nn.functional.softmax(y_pred, dim=-1)\n",
        "    _, max_idx = torch.max(token_probs, dim=-1)\n",
        "  return max_idx.squeeze()[-1]  # return only the last next token prediction"
      ],
      "metadata": {
        "id": "XQ7fkR1zJlI4"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "W2wS6j8Y2SUo"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, loss_function, optim, epochs, device):\n",
        "    losses = [] #group losses for loss visualization\n",
        "    running_loss = 0.0\n",
        "    val_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        print(\"Epoch %d / %d\" % (epoch+1, epochs))\n",
        "        print(\"-\"*10)\n",
        "\n",
        "        for i, batch_data in enumerate(train_loader):\n",
        "            x, y = batch_data\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_pred = model(x)\n",
        "\n",
        "            loss = compute_loss(y, y_pred, loss_function)\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            running_loss += loss.item()\n",
        "            losses.append(loss)\n",
        "\n",
        "            if (i+1) % 1000 == 0:\n",
        "                print(\"Step: {}, average training loss over last 1000 steps: {:.4f}\".format(i+1, running_loss/1000))\n",
        "                running_loss = 0.0\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            correct_pred = 0.0\n",
        "            num_samples = 0\n",
        "            for i, batch_data in enumerate(val_loader):\n",
        "                (y, x) = batch_data\n",
        "                y, x = y.to(device), x.to(device)\n",
        "                y_pred = model(x)\n",
        "                loss = compute_loss(y, y_pred, loss_function)\n",
        "                _, predicted_labels = torch.max(y_pred, 1)\n",
        "                num_samples+=predicted_labels.shape[0]\n",
        "                val_loss += loss.item()\n",
        "\n",
        "            val_losses.append(val_loss)\n",
        "        print(\"Epoch: {}, validation loss: {:.4f}\".format(epoch+1, val_loss/len(val_loader)))\n",
        "        print(\"Generated text: \")\n",
        "        generate(model, \"Harry\", device=DEVICE, n=20)\n",
        "\n",
        "    return losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "yj_S3bSm2SUo"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 6e-4\n",
        "NUM_EPOCHS = 3\n",
        "DROPOUT = 0.2\n",
        "D_MODEL = 1024\n",
        "NUM_HEADS = 8\n",
        "D_K = int(D_MODEL / NUM_HEADS)\n",
        "D_V = D_K\n",
        "D_FF = D_MODEL * 4\n",
        "NUM_LAYERS = 2\n",
        "VOCAB_SIZE = vocab.vocab_size\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "f314V3J12SUo"
      },
      "outputs": [],
      "source": [
        "model = zeptoGPT(D_K, D_MODEL, D_V, D_FF, num_heads=NUM_HEADS, num_layers=NUM_LAYERS, vocab_size=VOCAB_SIZE)\n",
        "model = model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "iqdfCB9C2SUo"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWmZ65uO6XhP",
        "outputId": "f625cf7d-5941-4e6e-c4db-fe517ffa91a1"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "RxStEdDG2SUp",
        "outputId": "f686ebe9-d371-4a2b-e80f-dd497af4bc52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 3\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 4.3593\n",
            "Step: 2000, average training loss over last 1000 steps: 3.9800\n",
            "Step: 3000, average training loss over last 1000 steps: 3.8774\n",
            "Step: 4000, average training loss over last 1000 steps: 3.8229\n",
            "Step: 5000, average training loss over last 1000 steps: 3.7767\n",
            "Step: 6000, average training loss over last 1000 steps: 3.7482\n",
            "Step: 7000, average training loss over last 1000 steps: 3.7148\n",
            "Step: 8000, average training loss over last 1000 steps: 3.6992\n",
            "Step: 9000, average training loss over last 1000 steps: 3.6691\n",
            "Step: 10000, average training loss over last 1000 steps: 3.6506\n",
            "Step: 11000, average training loss over last 1000 steps: 3.6380\n",
            "Step: 12000, average training loss over last 1000 steps: 3.6165\n",
            "Step: 13000, average training loss over last 1000 steps: 3.6083\n",
            "Epoch: 1, validation loss: 8.5828\n",
            "Generated text: \n",
            "(16,)\n",
            "(8,)\n",
            "[37, 939, 36, 264, 62, 196, 397, 36]\n",
            "prompt: and then Harry and\n",
            "and then Harry and Ro\n",
            "and then Harry and Ron\n",
            "and then Harry and Ron\n",
            "and then Harry and Ronbe\n",
            "and then Harry and Ronbe\n",
            "and then Harry and Ronbega\n",
            "and then Harry and Ronbegave\n",
            "and then Harry and Ronbegaves\n",
            "and then Harry and Ronbegaves of\n",
            "and then Harry and Ronbegaves of\n",
            "and then Harry and Ronbegaves of Ha\n",
            "and then Harry and Ronbegaves of Har\n",
            "and then Harry and Ronbegaves of Harry\n",
            "and then Harry and Ronbegaves of Harry,\n",
            "and then Harry and Ronbegaves of Harry, and\n",
            "and then Harry and Ronbegaves of Harry, and\n",
            "and then Harry and Ronbegaves of Harry, and Ro\n",
            "and then Harry and Ronbegaves of Harry, and Ron\n",
            "and then Harry and Ronbegaves of Harry, and Ron,\n",
            "and then Harry and Ronbegaves of Harry, and Ron, and\n",
            "Epoch 2 / 3\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 4.0451\n",
            "Step: 2000, average training loss over last 1000 steps: 3.5708\n",
            "Step: 3000, average training loss over last 1000 steps: 3.5643\n",
            "Step: 4000, average training loss over last 1000 steps: 3.5533\n",
            "Step: 5000, average training loss over last 1000 steps: 3.5464\n",
            "Step: 6000, average training loss over last 1000 steps: 3.5392\n",
            "Step: 7000, average training loss over last 1000 steps: 3.5301\n",
            "Step: 8000, average training loss over last 1000 steps: 3.5252\n",
            "Step: 9000, average training loss over last 1000 steps: 3.5113\n",
            "Step: 10000, average training loss over last 1000 steps: 3.5072\n",
            "Step: 11000, average training loss over last 1000 steps: 3.5029\n",
            "Step: 12000, average training loss over last 1000 steps: 3.4919\n",
            "Step: 13000, average training loss over last 1000 steps: 3.4907\n",
            "Epoch: 2, validation loss: 8.7714\n",
            "Generated text: \n",
            "(16,)\n",
            "(8,)\n",
            "[37, 939, 36, 264, 62, 196, 397, 36]\n",
            "prompt: and then Harry and\n",
            "and then Harry and Ro\n",
            "and then Harry and Ron\n",
            "and then Harry and Ron,\n",
            "and then Harry and Ron, and\n",
            "and then Harry and Ron, and\n",
            "and then Harry and Ron, and Her\n",
            "and then Harry and Ron, and Her\n",
            "and then Harry and Ron, and Hermi\n",
            "and then Harry and Ron, and Hermione\n",
            "and then Harry and Ron, and Hermione,\n",
            "and then Harry and Ron, and Hermione, and\n",
            "and then Harry and Ron, and Hermione, and\n",
            "and then Harry and Ron, and Hermione, and Her\n",
            "and then Harry and Ron, and Hermione, and Her\n",
            "and then Harry and Ron, and Hermione, and Hermi\n",
            "and then Harry and Ron, and Hermione, and Hermione\n",
            "and then Harry and Ron, and Hermione, and Hermione,\n",
            "and then Harry and Ron, and Hermione, and Hermione, and\n",
            "and then Harry and Ron, and Hermione, and Hermione, and\n",
            "and then Harry and Ron, and Hermione, and Hermione, and Her\n",
            "Epoch 3 / 3\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.9140\n",
            "Step: 2000, average training loss over last 1000 steps: 3.4650\n",
            "Step: 3000, average training loss over last 1000 steps: 3.4616\n",
            "Step: 4000, average training loss over last 1000 steps: 3.4593\n",
            "Step: 5000, average training loss over last 1000 steps: 3.4555\n",
            "Step: 6000, average training loss over last 1000 steps: 3.4477\n",
            "Step: 7000, average training loss over last 1000 steps: 3.4376\n",
            "Step: 8000, average training loss over last 1000 steps: 3.4346\n",
            "Step: 9000, average training loss over last 1000 steps: 3.4385\n",
            "Step: 10000, average training loss over last 1000 steps: 3.4358\n",
            "Step: 11000, average training loss over last 1000 steps: 3.4312\n",
            "Step: 12000, average training loss over last 1000 steps: 3.4249\n",
            "Step: 13000, average training loss over last 1000 steps: 3.4185\n",
            "Epoch: 3, validation loss: 8.9008\n",
            "Generated text: \n",
            "(16,)\n",
            "(8,)\n",
            "[37, 939, 36, 264, 62, 196, 397, 36]\n",
            "prompt: and then Harry and\n",
            "and then Harry and Ro\n",
            "and then Harry and Ron\n",
            "and then Harry and Ron,\n",
            "and then Harry and Ron, and\n",
            "and then Harry and Ron, and\n",
            "and then Harry and Ron, and Her\n",
            "and then Harry and Ron, and Her\n",
            "and then Harry and Ron, and Hermi\n",
            "and then Harry and Ron, and Hermione\n",
            "and then Harry and Ron, and Hermione,\n",
            "and then Harry and Ron, and Hermione, and\n",
            "and then Harry and Ron, and Hermione, and\n",
            "and then Harry and Ron, and Hermione, and Ro\n",
            "and then Harry and Ron, and Hermione, and Ron\n",
            "and then Harry and Ron, and Hermione, and Ron,\n",
            "and then Harry and Ron, and Hermione, and Ron, and\n",
            "and then Harry and Ron, and Hermione, and Ron, and\n",
            "and then Harry and Ron, and Hermione, and Ron, and Her\n",
            "and then Harry and Ron, and Hermione, and Ron, and Her\n",
            "and then Harry and Ron, and Hermione, and Ron, and Hermi\n"
          ]
        }
      ],
      "source": [
        "train_loss, val_loss = train(model, train_loader, val_loader, torch.nn.functional.cross_entropy, optimizer, NUM_EPOCHS, DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_sample = train_data"
      ],
      "metadata": {
        "id": "3m2I7HS8MO8M"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_sample[0])\n",
        "test_block = torch.tensor([text_sample[i] for i in range(8)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXfqcgw2MYSz",
        "outputId": "6b7796d0-0960-4c20-de30-ba4f4188d031"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(36)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_block"
      ],
      "metadata": {
        "id": "nJXjZzCnMoSY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e86a868a-5579-47c4-dcb5-62b71c31241f"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 36, 264,  62, 196,  36, 301,  64, 382])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_list = test_block.tolist()"
      ],
      "metadata": {
        "id": "m0uewcO7MwY_"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.decode(test_list)"
      ],
      "metadata": {
        "id": "P8mbTFkAMpic",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "40907dfd-13bf-42ce-b991-f3c78275c543"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Harry Potter'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_block = torch.tensor([text_sample[i] for i in range(100)])\n",
        "test_list = test_block.tolist()\n",
        "vocab.decode(test_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "aAN0uzbYhKa8",
        "outputId": "50f8143f-c7df-4414-9665-d25d1cc8bcdd"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Harry Potter and the Sorcerer's Stone\\n\\n\\nCHAPTER ONE\\n\\nTHE BOY WHO LIVED\\n\\nMr. and Mrs. Dursley, of number four, Privet Drive, were proud to say\\nthat they were perfectly normal, thank you very much. They were the last\\npeople you'd expect to be\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(model, \"there was\", device= DEVICE, n = 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrBrg4lRhOuv",
        "outputId": "0afb284f-08ec-4904-8d7c-39288f77737c"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2,)\n",
            "(2,)\n",
            "[37, 1003]\n",
            "prompt: there was\n",
            "there was another\n",
            "there was another wor\n",
            "there was another worri\n",
            "there was another worried\n",
            "there was another worried.\n",
            "there was another worried. \"\n",
            "there was another worried. \"You\n",
            "there was another worried. \"You'\n",
            "there was another worried. \"You've\n",
            "there was another worried. \"You've got\n",
            "there was another worried. \"You've got a\n",
            "there was another worried. \"You've got a\n",
            "there was another worried. \"You've got acr\n",
            "there was another worried. \"You've got acro\n",
            "there was another worried. \"You've got across\n",
            "there was another worried. \"You've got across the\n",
            "there was another worried. \"You've got across the\n",
            "there was another worried. \"You've got across the S\n",
            "there was another worried. \"You've got across the Sna\n",
            "there was another worried. \"You've got across the Snape\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}