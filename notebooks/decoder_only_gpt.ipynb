{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7GuoX-D2SUi"
      },
      "source": [
        "# Decoder-only based GPT (language model)\n",
        "\n",
        "Here we take a transformer block, the decoder in particular, and use it for the task of language modeling. In general, this is how GPTs are trained. We will do this on a much smaller scale.\n",
        "\n",
        "We take everything we've already built and leverage it in the way Karpathy implements a character level LM here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "p0LzXILI2SUk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "from transformers.transformer_blocks import Transformer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tokenmonster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2fEKfFTtjVa5"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device('mps')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UP_BMiE52SUl",
        "outputId": "26f0aaee-4baa-4e32-9867-ef8d1af3288f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1548865\n"
          ]
        }
      ],
      "source": [
        "harry_potter_text = \" \"\n",
        "for i in range(1, 4): # first 4 books\n",
        "    with open(f'data/hp{i}.txt', 'r', encoding='utf-8') as f:\n",
        "        harry_potter_text += f.read()\n",
        "print(len(harry_potter_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WTsFKtP2SUm"
      },
      "source": [
        "## Tokenization\n",
        "Instead of character level, we're going to model this LM using a tokenizer. in particular, we're going to try to use OpenAI's tiktoken with the gpt2 50k tokenizer. This might end up being too large of a vocab size given compute constraints, but"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jNAlYTIf2SUm"
      },
      "outputs": [],
      "source": [
        "vocab = tokenmonster.load(\"fiction-1024-consistent-v1\")\n",
        "tokens = vocab.tokenize(\"This is a test.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya_OlheL2SUm",
        "outputId": "d5e1bd1e-2672-47e6-9488-7ba8c28a2153"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([138, 918, 108, 318, 202,  17], dtype=uint16)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nZ0pfOIT2SUm"
      },
      "outputs": [],
      "source": [
        "token_example = vocab.tokenize(\"hello world test monster tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1y5eYUD2SUn",
        "outputId": "a0ef8edf-ffe7-4934-8d2a-069b2bc106e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 37, 445, 174, 785, 318, 202, 465, 547, 321, 169, 181, 218,  62],\n",
              "      dtype=uint16)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qyadxmt2SUn",
        "outputId": "73f4164c-9da3-491a-c6a4-5f2f7b789f76"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['',\n",
              " ' hel',\n",
              " 'lo',\n",
              " ' world',\n",
              " ' te',\n",
              " 'st',\n",
              " ' mon',\n",
              " 'ster',\n",
              " ' to',\n",
              " 'ke',\n",
              " 'ni',\n",
              " 'ze',\n",
              " 'r']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[vocab.decode([token]) for token in token_example]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2ks4lD6p2SUn"
      },
      "outputs": [],
      "source": [
        "tokens = np.array(vocab.tokenize(harry_potter_text), dtype=np.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH_4N1qN2SUn",
        "outputId": "bf7fe23f-ba4a-4d32-ae00-afd1ce867b56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([642743]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "dataset = torch.tensor(tokens, dtype=torch.long)\n",
        "print(dataset.shape, dataset.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbbeNPTF2SUo",
        "outputId": "e32d1fea-ae04-467c-cbdd-bf08aee729df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Harry Potter and the Sorcerer's Stone\n",
            "CHAPTER ONE\n",
            "THE BOY WHO LIVED\n",
            "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved\n",
            "train set size: 514174, test: 64271, val: 64273, data size: 642743, dataset_size: 642718\n"
          ]
        }
      ],
      "source": [
        "class HPDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of possible sequences\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Fetch a single sequence x and its corresponding target y\n",
        "        x = self.data[idx:idx + self.block_size]\n",
        "        y = self.data[idx + 1:idx + self.block_size + 1]\n",
        "        return x, y\n",
        "\n",
        "BLOCK_SIZE = 25\n",
        "hp_data = HPDataset(dataset, BLOCK_SIZE)\n",
        "\n",
        "test_block = torch.tensor([dataset[i] for i in range(100)])\n",
        "test_list = test_block.tolist()\n",
        "print(vocab.decode(test_list))\n",
        "\n",
        "train_size = int(len(hp_data) * 0.8)\n",
        "test_size = int(len(hp_data) * 0.1)\n",
        "val_size = len(hp_data) - train_size - test_size\n",
        "\n",
        "print(f\"train set size: {train_size}, test: {test_size}, val: {val_size}, data size: {len(dataset)}, dataset_size: {hp_data.__len__()}\")\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(hp_data, [train_size, val_size, test_size])\n",
        "\n",
        "batch_size = 64\n",
        "train_loader, val_loader, test_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True), DataLoader(val_dataset, batch_size=batch_size, shuffle=True), DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9AWjLIa2SUo",
        "outputId": "31919c44-0484-4b2f-efee-9715b657b898"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8034\n",
            "1005\n",
            "1005\n"
          ]
        }
      ],
      "source": [
        "print(len(train_loader))\n",
        "print(len(test_loader))\n",
        "print(len(val_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5k9W9isInum",
        "outputId": "92355f01-7269-4503-ccd9-4d98e35b0ccc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([234,  37, 264, 215, 186, 781,  36, 118,  37, 324, 166, 949, 116, 173,\n",
            "         17,   1,  75,  75,  75,  75,   5, 138, 604, 470, 255]), tensor([ 37, 264, 215, 186, 781,  36, 118,  37, 324, 166, 949, 116, 173,  17,\n",
            "          1,  75,  75,  75,  75,   5, 138, 604, 470, 255,  53]))\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset.__getitem__(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9D0dkIBU2SUo"
      },
      "outputs": [],
      "source": [
        "class rowlingGPT(nn.Module):\n",
        "    \"\"\"\n",
        "    JK Rowling would probably not approve\n",
        "    \"\"\"\n",
        "    def __init__(self, d_k, d_model, d_v, d_ff, num_heads, num_layers, vocab_size, dropout=0.1) -> None:\n",
        "        super().__init__()\n",
        "        self.decoder_transformer = Transformer(d_k, d_model, d_v, d_ff, num_heads, num_layers, vocab_size=vocab_size, mask=True, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.decoder_transformer(x)\n",
        "        return self.fc(self.layer_norm(out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "oHC-Secr2SUo"
      },
      "outputs": [],
      "source": [
        "def compute_loss(y_target, y_pred, loss_function):\n",
        "    B, T, C = y_pred.shape\n",
        "    y_pred = y_pred.view(B*T, C)\n",
        "    _, max_indices = torch.max(y_pred, dim=1)\n",
        "    y_target_list = y_target.tolist()\n",
        "    max_indices = max_indices.tolist()\n",
        "    y_target = y_target.view(B*T)\n",
        "    return loss_function(y_pred, y_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5Pc9PN-Hnrv-"
      },
      "outputs": [],
      "source": [
        "def generate(model, prompt: str, device,n = 200, block_size=BLOCK_SIZE):\n",
        "  prompt_array = vocab.tokenize(prompt)\n",
        "  prompt_array = np.array(prompt_array[:block_size], dtype=np.int16)\n",
        "  decoded = vocab.decode(prompt_array)\n",
        "  print(f\"prompt: {decoded}\")\n",
        "  cumulative_array = prompt_array\n",
        "  for i in range(n):\n",
        "    prompt_tensor = torch.tensor(prompt_array, dtype=torch.long).to(device)\n",
        "    next_token = predict_next_token(model, prompt_tensor.unsqueeze(0))\n",
        "    next_token_np = next_token.cpu().numpy().flatten()\n",
        "    cumulative_array = np.append(cumulative_array, next_token_np)\n",
        "    prompt_array = np.append(prompt_array[1:], next_token_np)\n",
        "    test_list = cumulative_array.tolist()\n",
        "  print(vocab.decode(test_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "XQ7fkR1zJlI4"
      },
      "outputs": [],
      "source": [
        "def predict_next_token(model, block):\n",
        "  with torch.no_grad():\n",
        "    y_pred = model(block)\n",
        "    token_probs = nn.functional.softmax(y_pred, dim=-1)\n",
        "    _, max_idx = torch.max(token_probs, dim=-1)\n",
        "  return max_idx.squeeze()[-1]  # return only the last next token prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "W2wS6j8Y2SUo"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, loss_function, optim, epochs, device):\n",
        "    losses = [] #group losses for loss visualization\n",
        "    running_loss = 0.0\n",
        "    val_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        print(\"Epoch %d / %d\" % (epoch+1, epochs))\n",
        "        print(\"-\"*10)\n",
        "\n",
        "        for i, batch_data in enumerate(train_loader):\n",
        "            x, y = batch_data\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_pred = model(x)\n",
        "\n",
        "            loss = compute_loss(y, y_pred, loss_function)\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            running_loss += loss.item()\n",
        "            losses.append(loss)\n",
        "\n",
        "            if (i+1) % 1000 == 0:\n",
        "                print(\"Step: {}, average training loss over last 1000 steps: {:.4f}\".format(i+1, running_loss/1000))\n",
        "                running_loss = 0.0\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for i, batch_data in enumerate(val_loader):\n",
        "                x, y = batch_data  # FIX: use x, y order for validation too\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                y_pred = model(x)\n",
        "                loss = compute_loss(y, y_pred, loss_function)\n",
        "                _, predicted_labels = torch.max(y_pred, 1)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "            val_losses.append(val_loss)\n",
        "        print(\"Epoch: {}, validation loss: {:.4f}\".format(epoch+1, val_loss/len(val_loader)))\n",
        "        print(\"Generated text: \")\n",
        "        generate(model, \"Harry\", device=DEVICE, n=20)\n",
        "\n",
        "    return losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "yj_S3bSm2SUo"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 6e-4\n",
        "NUM_EPOCHS = 18\n",
        "DROPOUT = 0.2\n",
        "D_MODEL = 1024\n",
        "NUM_HEADS = 8\n",
        "D_K = int(D_MODEL / NUM_HEADS)\n",
        "D_V = D_K\n",
        "D_FF = D_MODEL * 4\n",
        "NUM_LAYERS = 2\n",
        "VOCAB_SIZE = vocab.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "f314V3J12SUo"
      },
      "outputs": [],
      "source": [
        "model = rowlingGPT(D_K, D_MODEL, D_V, D_FF, num_heads=NUM_HEADS, num_layers=NUM_LAYERS, vocab_size=VOCAB_SIZE)\n",
        "model = model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "iqdfCB9C2SUo"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWmZ65uO6XhP",
        "outputId": "eac1924c-b0e5-452b-c622-f397a4afb7b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='mps')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxStEdDG2SUp",
        "outputId": "81050059-7323-4cc7-c80c-67e36ae1934a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 3.1323\n",
            "Step: 2000, average training loss over last 1000 steps: 2.5467\n",
            "Step: 3000, average training loss over last 1000 steps: 2.2925\n",
            "Step: 4000, average training loss over last 1000 steps: 2.0650\n",
            "Step: 5000, average training loss over last 1000 steps: 1.8473\n",
            "Step: 6000, average training loss over last 1000 steps: 1.6545\n",
            "Step: 7000, average training loss over last 1000 steps: 1.4909\n",
            "Step: 8000, average training loss over last 1000 steps: 1.3525\n",
            "Epoch: 1, validation loss: 1.1011\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry, his voice shaking with anger.\n",
            "����Harry, his\n",
            "Epoch 2 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 1.2434\n",
            "Step: 2000, average training loss over last 1000 steps: 1.1307\n",
            "Step: 3000, average training loss over last 1000 steps: 1.0645\n",
            "Step: 4000, average training loss over last 1000 steps: 1.0122\n",
            "Step: 5000, average training loss over last 1000 steps: 0.9660\n",
            "Step: 6000, average training loss over last 1000 steps: 0.9279\n",
            "Step: 7000, average training loss over last 1000 steps: 0.8931\n",
            "Step: 8000, average training loss over last 1000 steps: 0.8665\n",
            "Epoch: 2, validation loss: 0.6900\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry, \"is that last night Voldemort's going to try and get\n",
            "Epoch 3 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.8414\n",
            "Step: 2000, average training loss over last 1000 steps: 0.8031\n",
            "Step: 3000, average training loss over last 1000 steps: 0.7868\n",
            "Step: 4000, average training loss over last 1000 steps: 0.7696\n",
            "Step: 5000, average training loss over last 1000 steps: 0.7557\n",
            "Step: 6000, average training loss over last 1000 steps: 0.7411\n",
            "Step: 7000, average training loss over last 1000 steps: 0.7280\n",
            "Step: 8000, average training loss over last 1000 steps: 0.7150\n",
            "Epoch: 3, validation loss: 0.5743\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry, who was so largerrow. It had a shabby next to all the\n",
            "Epoch 4 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.7048\n",
            "Step: 2000, average training loss over last 1000 steps: 0.6814\n",
            "Step: 3000, average training loss over last 1000 steps: 0.6731\n",
            "Step: 4000, average training loss over last 1000 steps: 0.6672\n",
            "Step: 5000, average training loss over last 1000 steps: 0.6587\n",
            "Step: 6000, average training loss over last 1000 steps: 0.6504\n",
            "Step: 7000, average training loss over last 1000 steps: 0.6456\n",
            "Step: 8000, average training loss over last 1000 steps: 0.6374\n",
            "Epoch: 4, validation loss: 0.5219\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry was glad Ron took hold of the back of Harry, who was still\n",
            "Epoch 5 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.6338\n",
            "Step: 2000, average training loss over last 1000 steps: 0.6155\n",
            "Step: 3000, average training loss over last 1000 steps: 0.6124\n",
            "Step: 4000, average training loss over last 1000 steps: 0.6079\n",
            "Step: 5000, average training loss over last 1000 steps: 0.6043\n",
            "Step: 6000, average training loss over last 1000 steps: 0.5996\n",
            "Step: 7000, average training loss over last 1000 steps: 0.5963\n",
            "Step: 8000, average training loss over last 1000 steps: 0.5932\n",
            "Epoch: 5, validation loss: 0.4979\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry's wand to shoulder heighted down with arms like club\n",
            "Epoch 6 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.5913\n",
            "Step: 2000, average training loss over last 1000 steps: 0.5743\n",
            "Step: 3000, average training loss over last 1000 steps: 0.5749\n",
            "Step: 4000, average training loss over last 1000 steps: 0.5721\n",
            "Step: 5000, average training loss over last 1000 steps: 0.5697\n",
            "Step: 6000, average training loss over last 1000 steps: 0.5685\n",
            "Step: 7000, average training loss over last 1000 steps: 0.5669\n",
            "Step: 8000, average training loss over last 1000 steps: 0.5635\n",
            "Epoch: 6, validation loss: 0.4776\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry, who could feel him -\"\n",
            "����\"What's that?\"\n",
            "Epoch 7 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.5618\n",
            "Step: 2000, average training loss over last 1000 steps: 0.5486\n",
            "Step: 3000, average training loss over last 1000 steps: 0.5481\n",
            "Step: 4000, average training loss over last 1000 steps: 0.5478\n",
            "Step: 5000, average training loss over last 1000 steps: 0.5460\n",
            "Step: 6000, average training loss over last 1000 steps: 0.5456\n",
            "Step: 7000, average training loss over last 1000 steps: 0.5437\n",
            "Step: 8000, average training loss over last 1000 steps: 0.5426\n",
            "Epoch: 7, validation loss: 0.4668\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry and Ron, who was still eyeing them?\" said Harry\n",
            "Epoch 8 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.5410\n",
            "Step: 2000, average training loss over last 1000 steps: 0.5288\n",
            "Step: 3000, average training loss over last 1000 steps: 0.5297\n",
            "Step: 4000, average training loss over last 1000 steps: 0.5285\n",
            "Step: 5000, average training loss over last 1000 steps: 0.5279\n",
            "Step: 6000, average training loss over last 1000 steps: 0.5281\n",
            "Step: 7000, average training loss over last 1000 steps: 0.5279\n",
            "Step: 8000, average training loss over last 1000 steps: 0.5266\n",
            "Epoch: 8, validation loss: 0.4594\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry and Ron, who was still sitting on the ground. Harry was sure that\n",
            "Epoch 9 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.5254\n",
            "Step: 2000, average training loss over last 1000 steps: 0.5131\n",
            "Step: 3000, average training loss over last 1000 steps: 0.5148\n",
            "Step: 4000, average training loss over last 1000 steps: 0.5146\n",
            "Step: 5000, average training loss over last 1000 steps: 0.5146\n",
            "Step: 6000, average training loss over last 1000 steps: 0.5152\n",
            "Step: 7000, average training loss over last 1000 steps: 0.5140\n",
            "Step: 8000, average training loss over last 1000 steps: 0.5150\n",
            "Epoch: 9, validation loss: 0.4540\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry's arm. \"The entrance to the grounds,\" Dumble\n",
            "Epoch 10 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.5124\n",
            "Step: 2000, average training loss over last 1000 steps: 0.5016\n",
            "Step: 3000, average training loss over last 1000 steps: 0.5027\n",
            "Step: 4000, average training loss over last 1000 steps: 0.5038\n",
            "Step: 5000, average training loss over last 1000 steps: 0.5039\n",
            "Step: 6000, average training loss over last 1000 steps: 0.5033\n",
            "Step: 7000, average training loss over last 1000 steps: 0.5039\n",
            "Step: 8000, average training loss over last 1000 steps: 0.5047\n",
            "Epoch: 10, validation loss: 0.4494\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry, Ron, and Hermione, \"but I can't jus' let\n",
            "Epoch 11 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.5036\n",
            "Step: 2000, average training loss over last 1000 steps: 0.4919\n",
            "Step: 3000, average training loss over last 1000 steps: 0.4931\n",
            "Step: 4000, average training loss over last 1000 steps: 0.4949\n",
            "Step: 5000, average training loss over last 1000 steps: 0.4955\n",
            "Step: 6000, average training loss over last 1000 steps: 0.4951\n",
            "Step: 7000, average training loss over last 1000 steps: 0.4951\n",
            "Step: 8000, average training loss over last 1000 steps: 0.4947\n",
            "Epoch: 11, validation loss: 0.4461\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry and Ron had fought the boggart is simple,\n",
            "Epoch 12 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.4943\n",
            "Step: 2000, average training loss over last 1000 steps: 0.4836\n",
            "Step: 3000, average training loss over last 1000 steps: 0.4854\n",
            "Step: 4000, average training loss over last 1000 steps: 0.4864\n",
            "Step: 5000, average training loss over last 1000 steps: 0.4881\n",
            "Step: 6000, average training loss over last 1000 steps: 0.4879\n",
            "Step: 7000, average training loss over last 1000 steps: 0.4890\n",
            "Step: 8000, average training loss over last 1000 steps: 0.4879\n",
            "Epoch: 12, validation loss: 0.4425\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry and Ron. \"We'll be able to find a thick, black\n",
            "Epoch 13 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.4861\n",
            "Step: 2000, average training loss over last 1000 steps: 0.4771\n",
            "Step: 3000, average training loss over last 1000 steps: 0.4783\n",
            "Step: 4000, average training loss over last 1000 steps: 0.4809\n",
            "Step: 5000, average training loss over last 1000 steps: 0.4810\n",
            "Step: 6000, average training loss over last 1000 steps: 0.4816\n",
            "Step: 7000, average training loss over last 1000 steps: 0.4827\n",
            "Step: 8000, average training loss over last 1000 steps: 0.4822\n",
            "Epoch: 13, validation loss: 0.4397\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry and Ron, looking quite impressed.\n",
            "����\"What\n",
            "Epoch 14 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.4820\n",
            "Step: 2000, average training loss over last 1000 steps: 0.4709\n",
            "Step: 3000, average training loss over last 1000 steps: 0.4744\n",
            "Step: 4000, average training loss over last 1000 steps: 0.4745\n",
            "Step: 5000, average training loss over last 1000 steps: 0.4759\n",
            "Step: 6000, average training loss over last 1000 steps: 0.4765\n",
            "Step: 7000, average training loss over last 1000 steps: 0.4766\n",
            "Step: 8000, average training loss over last 1000 steps: 0.4766\n",
            "Epoch: 14, validation loss: 0.4380\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry's wand to bring silence fell. The red envelope,\n",
            "Epoch 15 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.4759\n",
            "Step: 2000, average training loss over last 1000 steps: 0.4662\n",
            "Step: 3000, average training loss over last 1000 steps: 0.4686\n",
            "Step: 4000, average training loss over last 1000 steps: 0.4702\n",
            "Step: 5000, average training loss over last 1000 steps: 0.4706\n",
            "Step: 6000, average training loss over last 1000 steps: 0.4716\n",
            "Step: 7000, average training loss over last 1000 steps: 0.4711\n",
            "Step: 8000, average training loss over last 1000 steps: 0.4729\n",
            "Epoch: 15, validation loss: 0.4359\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry looked over at the Slytherin, the Heir of Life! But I don't\n",
            "Epoch 16 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.4718\n",
            "Step: 2000, average training loss over last 1000 steps: 0.4624\n",
            "Step: 3000, average training loss over last 1000 steps: 0.4644\n",
            "Step: 4000, average training loss over last 1000 steps: 0.4664\n",
            "Step: 5000, average training loss over last 1000 steps: 0.4668\n",
            "Step: 6000, average training loss over last 1000 steps: 0.4674\n",
            "Step: 7000, average training loss over last 1000 steps: 0.4677\n",
            "Step: 8000, average training loss over last 1000 steps: 0.4689\n",
            "Epoch: 16, validation loss: 0.4351\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry looked up and saw Hermione, however, took an uncertain stepped forward\n",
            "Epoch 17 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.4679\n",
            "Step: 2000, average training loss over last 1000 steps: 0.4581\n",
            "Step: 3000, average training loss over last 1000 steps: 0.4605\n",
            "Step: 4000, average training loss over last 1000 steps: 0.4621\n",
            "Step: 5000, average training loss over last 1000 steps: 0.4623\n",
            "Step: 6000, average training loss over last 1000 steps: 0.4642\n",
            "Step: 7000, average training loss over last 1000 steps: 0.4654\n",
            "Step: 8000, average training loss over last 1000 steps: 0.4648\n",
            "Epoch: 17, validation loss: 0.4339\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry's wand and moved out of sight.\n",
            "����\"You'\n",
            "Epoch 18 / 18\n",
            "----------\n",
            "Step: 1000, average training loss over last 1000 steps: 0.4640\n",
            "Step: 2000, average training loss over last 1000 steps: 0.4556\n",
            "Step: 3000, average training loss over last 1000 steps: 0.4580\n",
            "Step: 4000, average training loss over last 1000 steps: 0.4579\n",
            "Step: 5000, average training loss over last 1000 steps: 0.4605\n",
            "Step: 6000, average training loss over last 1000 steps: 0.4602\n",
            "Step: 7000, average training loss over last 1000 steps: 0.4613\n",
            "Step: 8000, average training loss over last 1000 steps: 0.4626\n",
            "Epoch: 18, validation loss: 0.4324\n",
            "Generated text: \n",
            "prompt: Harry\n",
            "Harry, who had never had anything to share my toilet.\"\n",
            "���\n"
          ]
        }
      ],
      "source": [
        "train_loss, val_loss = train(model, train_loader, val_loader, torch.nn.functional.cross_entropy, optimizer, NUM_EPOCHS, DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "3m2I7HS8MO8M"
      },
      "outputs": [],
      "source": [
        "text_sample = dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXfqcgw2MYSz",
        "outputId": "5720fbfe-2b96-4e8c-8d4a-b381d545144e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(36)\n"
          ]
        }
      ],
      "source": [
        "print(text_sample[0])\n",
        "test_block = torch.tensor([text_sample[i] for i in range(8)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJXjZzCnMoSY",
        "outputId": "d8ec8bcc-d77b-49ab-8629-26f2606d0861"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 36, 264,  62, 196,  36, 301,  64, 382])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "m0uewcO7MwY_"
      },
      "outputs": [],
      "source": [
        "test_list = test_block.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "P8mbTFkAMpic",
        "outputId": "307dc970-d12e-4da3-da61-dabbba13eb0b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' Harry Potter'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab.decode(test_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "aAN0uzbYhKa8",
        "outputId": "0dd8298f-553e-4034-e5ca-243c58d92fa4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\" Harry Potter and the Sorcerer's Stone\\nCHAPTER ONE\\nTHE BOY WHO LIVED\\nMr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved\""
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_block = torch.tensor([text_sample[i] for i in range(100)])\n",
        "test_list = test_block.tolist()\n",
        "vocab.decode(test_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrBrg4lRhOuv",
        "outputId": "5d17ee92-d8f7-48db-e12f-ff9a6cc6072a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prompt: Harry Potter and the monkeys\n",
            "Harry Potter and the monkeys are ready for cutting at last. Tonight, we will be able to revive those people who have been Petrified. I need hardly remind you all that one of them may well be able to tell us who, or what, attacked them. I am hopeful that this dreadful year will end with our catching the culprit.\"\n",
            "����There was an explosion of cheering. Harry looked over at the Slytherin table and saw a horrible ghost\n",
            "sitting there, with blank staring eyes, a gaunt face, and robes stained\n",
            "with silver blood. He was right next to Malfoy who, Harry was pleased to see, didn't look too pleased with the seating arrangements.\n",
            "\"How did he get covered in blood?\" asked Seamus with great interest.\n",
            "\"I've never seen you before!\" said Harry, as Dedalus Diggle's top hat fell off in his excitement. \"You bowed to me once in a shop.\"\n",
            "\"He remembers!\" cried Dedalus Diggle, looking around at everyone. \"Did you hear that? He remembers me!\" Harry shook hands again and again -- Doris Crockford shook Harry's hand one last time, and Hagrid led them through the bar and out into a small, walled courtyard, where there was nothing but a trash can and a few weeds.\n",
            "Hagrid grinned at Harry.\n",
            "\"Told yeh, didn't I? Told yeh you was famous. Even Professor Quirrell\n",
            "54\n",
            "was tremblin' ter meet yeh -- mind you, he's usually tremblin'.\" \"Is he always that nervous?\"\n",
            "\"Oh, yeah. Poor bloke. Brilliant mind. He was fine while he was\n",
            "studyin' outta books but then he took a year off ter get some firsthand experience.... They say he met vampires in the Black Forest, and there was a nasty bit o' trouble with a hag -- never been the same since. Scared of the students filed past the hospital wing trying to catch a glimpse of her face -- and was startled to see that she was in tears.\n",
            "\"I think she heard you.\"\n",
            "\"So?\" said Ron, but he looked a bit uncomfortable. \"She must've noticed she's got no friends.\"\n",
            "Hermione didn't turn up for the next class and wasn't seen all\n",
            "afternoon. On their way down to the Great Hall for the Halloween feast, Harry and Ron overheard Parvati Patil telling her friend Lavender that Hermione was crying in the girls' bathroom and wanted to be left alone. Ron looked still more awkward at this, but a moment later they had entered the Great Hall, where the Halloween decorations put Hermione out of their minds.\n",
            "A thousand live bats fluttered from the walls and ceiling while a thousand more swooped over the tables in low black clouds, making the candles in the pumpkins stutter. The feast appeared suddenly on the golden plates, as it had at the start-of-term banquet will begin shortly, but before you take your seats in the Great Hall, you will be sorted into your houses. The Sorting is a very important ceremony because, while you are here, your house will be something like your family within Hogwar\n"
          ]
        }
      ],
      "source": [
        "generate(model, \"Harry Potter and the monkeys\", device= DEVICE, n = 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
