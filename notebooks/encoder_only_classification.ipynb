{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-only based classification\n",
    "\n",
    "Here we take a transformer block, the encoder in particular, and stack another MLP on top of it to see if it can accurately classify text in the AG News dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import sys \n",
    "sys.path.append(\"../models\")\n",
    "from transformer_blocks import Transformer\n",
    "from torch.utils.data import random_split\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTransformer(nn.Module): \n",
    "    def __init__(self, d_k, d_model, d_v, d_ff, num_heads, num_layers, num_classes, vocab_size, dropout=0.1) -> None:\n",
    "        super(ClassificationTransformer, self).__init__()\n",
    "        self.encoder_only_transformer = Transformer(d_k, d_model, d_v, d_ff, num_heads, num_layers, vocab_size, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # self.fc1 = nn.Linear(d_model, d_model) what if I got rid of one layer? how does that impact complexity and learning\n",
    "        self.fc2 = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.encoder_only_transformer(x)\n",
    "        avg_pool = torch.mean(out, dim=-2)\n",
    "        return self.fc2(self.dropout(self.dropout(avg_pool)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "# Convert to list to enable random splitting\n",
    "train_dataset = list(train_iter)\n",
    "\n",
    "#80-20 train-val split \n",
    "train_size = int(len(train_dataset) * 0.8)  \n",
    "val_size = len(train_dataset) - train_size  \n",
    "train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "VOCAB_SIZE = 5000\n",
    "\n",
    "# Build vocab based on the train_data\n",
    "train_data_iter = (text for _, text in train_data)\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_data_iter), specials=[\"<unk>\"], max_tokens=VOCAB_SIZE)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "def collate_batch(batch):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    \n",
    "    # Sort the batch in the descending order\n",
    "    batch.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "        \n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.int64)\n",
    "    \n",
    "    # Pad sequences\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "    \n",
    "    return label_list.to(device), text_list.to(device), lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size = 8, shuffle = True, collate_fn = collate_batch)\n",
    "val_loader = DataLoader(val_data, batch_size = 8, shuffle = False, collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 50\n",
    "DROPOUT = 0.2\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "D_K = 128\n",
    "D_V = D_K\n",
    "D_MODEL = D_K * 2\n",
    "D_FF = D_MODEL * 4\n",
    "NUM_LAYERS = 2\n",
    "OUTPUT_DIM = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationTransformer(D_K, D_MODEL, D_V, D_FF, num_heads=3, num_classes=OUTPUT_DIM, num_layers=2, vocab_size=VOCAB_SIZE)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, loss_function, optim, epochs, device):\n",
    "    losses = [] #group losses for loss visualization \n",
    "    running_loss = 0.0\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        print(\"Epoch %d / %d\" % (epoch+1, epochs))\n",
    "        print(\"-\"*10)\n",
    "    \n",
    "        for i, batch_data in enumerate(train_loader):\n",
    "            (y, x, x_size) = batch_data\n",
    "            logits = model(x)\n",
    "            loss = loss_function(logits, y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            running_loss += loss.item()\n",
    "            losses.append(loss)\n",
    "\n",
    "            if (i+1) % 1000 == 0:\n",
    "                print(\"Step: {}, average training loss over last 2000 steps: {:.4f}\".format(i+1, running_loss/1000))\n",
    "                running_loss = 0.0\n",
    "            \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            correct_pred = 0.0\n",
    "            num_samples = 0\n",
    "            for i, batch_data in enumerate(val_loader):\n",
    "                (y, x, x_size) = batch_data\n",
    "                y, x, x_size = y.to(device), x.to(device), x_size.to(device)\n",
    "                logits = model(x)\n",
    "                loss = loss_function(logits, y)\n",
    "                _, predicted_labels = torch.max(logits, 1)\n",
    "                correct_pred += (predicted_labels.long() == y.long()).sum()\n",
    "                num_samples+=predicted_labels.shape[0]\n",
    "                val_loss += loss.item()\n",
    "            \n",
    "            val_accuracy = (correct_pred / num_samples) * 100\n",
    "            val_losses.append(val_loss)\n",
    "        print(\"Epoch: {}, validation loss: {:.4f}, val accuracy: {:.2f}\".format(epoch+1, val_loss/len(val_loader), val_accuracy))\n",
    "    \n",
    "    return losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 1.2023\n",
      "Step: 2000, average training loss over last 2000 steps: 0.7046\n",
      "Step: 3000, average training loss over last 2000 steps: 0.5208\n",
      "Step: 4000, average training loss over last 2000 steps: 0.4589\n",
      "Step: 5000, average training loss over last 2000 steps: 0.4205\n",
      "Step: 6000, average training loss over last 2000 steps: 0.3984\n",
      "Step: 7000, average training loss over last 2000 steps: 0.3926\n",
      "Step: 8000, average training loss over last 2000 steps: 0.3743\n",
      "Step: 9000, average training loss over last 2000 steps: 0.3475\n",
      "Step: 10000, average training loss over last 2000 steps: 0.3467\n",
      "Step: 11000, average training loss over last 2000 steps: 0.3370\n",
      "Step: 12000, average training loss over last 2000 steps: 0.3488\n",
      "Epoch: 1, validation loss: 0.3298, val accuracy: 89.41\n",
      "Epoch 2 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.3248\n",
      "Step: 2000, average training loss over last 2000 steps: 0.3060\n",
      "Step: 3000, average training loss over last 2000 steps: 0.3034\n",
      "Step: 4000, average training loss over last 2000 steps: 0.3072\n",
      "Step: 5000, average training loss over last 2000 steps: 0.3045\n",
      "Step: 6000, average training loss over last 2000 steps: 0.3104\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2916\n",
      "Step: 8000, average training loss over last 2000 steps: 0.3066\n",
      "Step: 9000, average training loss over last 2000 steps: 0.3249\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2960\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2878\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2963\n",
      "Epoch: 2, validation loss: 0.2982, val accuracy: 90.40\n",
      "Epoch 3 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2559\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2779\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2843\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2816\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2770\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2777\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2873\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2769\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2761\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2780\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2894\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2743\n",
      "Epoch: 3, validation loss: 0.2900, val accuracy: 90.27\n",
      "Epoch 4 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2559\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2549\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2514\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2658\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2640\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2681\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2636\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2762\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2652\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2692\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2574\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2582\n",
      "Epoch: 4, validation loss: 0.2837, val accuracy: 90.63\n",
      "Epoch 5 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2425\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2406\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2481\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2521\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2500\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2570\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2578\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2551\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2614\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2706\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2645\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2625\n",
      "Epoch: 5, validation loss: 0.2967, val accuracy: 90.37\n",
      "Epoch 6 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2248\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2315\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2410\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2471\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2339\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2588\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2425\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2568\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2637\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2674\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2457\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2553\n",
      "Epoch: 6, validation loss: 0.3094, val accuracy: 89.95\n",
      "Epoch 7 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2227\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2379\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2462\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2424\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2411\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2403\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2432\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2640\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2483\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2590\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2451\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2486\n",
      "Epoch: 7, validation loss: 0.2952, val accuracy: 90.11\n",
      "Epoch 8 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2267\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2195\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2303\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2316\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2304\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2388\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2550\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2523\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2589\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2482\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2555\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2589\n",
      "Epoch: 8, validation loss: 0.2906, val accuracy: 90.38\n",
      "Epoch 9 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2284\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2155\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2244\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2260\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2347\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2445\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2489\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2568\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2401\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2455\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2489\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2325\n",
      "Epoch: 9, validation loss: 0.2998, val accuracy: 90.23\n",
      "Epoch 10 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2287\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2197\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2354\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2253\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2308\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2296\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2291\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2437\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2685\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2442\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2465\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2297\n",
      "Epoch: 10, validation loss: 0.3024, val accuracy: 90.25\n",
      "Epoch 11 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2256\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2089\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2352\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2370\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2216\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2318\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2336\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2400\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2412\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2461\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2520\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2344\n",
      "Epoch: 11, validation loss: 0.3007, val accuracy: 90.42\n",
      "Epoch 12 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2084\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2315\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2223\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2393\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loss, val_loss \u001b[39m=\u001b[39m train(model, train_loader, val_loader, torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mcross_entropy, optimizer, NUM_EPOCHS, DEVICE)\n",
      "Cell \u001b[0;32mIn[38], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, loss_function, optim, epochs, device)\u001b[0m\n\u001b[1;32m     15\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     16\u001b[0m optim\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> 17\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     18\u001b[0m losses\u001b[39m.\u001b[39mappend(loss)\n\u001b[1;32m     20\u001b[0m \u001b[39mif\u001b[39;00m (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = train(model, train_loader, val_loader, torch.nn.functional.cross_entropy, optimizer, NUM_EPOCHS, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do I want to see from this training? \n",
    "\n",
    "- I want to see how long it takes for this to converge \n",
    "- I want to see the computational complexity (how long per epoch?)\n",
    "    - anywhere from 60 seconds an epoch to 3 minutes an epoch depending on hyperparameters \n",
    "\n",
    "- How does the model accuracy respond to changes in hyperparams? Not just LR and the usuals, but also d_model, d_k, d_v, d_ff\n",
    "    - This is intuitive, higher values for these hyper params work better (until a point of diminishing return) since there's a balance between trying to capture all the intricacies of the data in high dimensional vector space and the actual complexity of the data to begin with. (as well as computational constraints) - doubling these dimensions led to more batch efficient learning but at the same time increased runtime by a proportional 2x \n",
    "\n",
    "\n",
    "- How does the model respond when we take out one of the linear layers in the classification transformer? \n",
    "    - from the looks of it, really well actually. It seems to be learning faster and reaching better validation accuracies without the intermediate layer after the encoder and before the classification layer. This might be because our model is large enough in parameter space to capture the distribution of our input data. The added layer was only increasing computational complexity and overall model complexity. With less parameters to optimize and enough to capture information about the input, the model seems to be training better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts so far: \n",
    "\n",
    "This architecture is way more compute intensive but at the same time so much more batch efficient than previously explored ones when tuned with the right hyperparameters (LSTMs, RNNs even when bidirectional + multi layered and so on) that we see near 90% validation accuracy after just one or two epochs. These epochs, however, take tremendously long (7 minutes per epoch v/s the 1 minute LSTMs were taking). Part of this could be becauase of the implementation being from scratch (torch.transformer might be faster), but most of this comes from the computational load the multiple blocks (and their individual complexities)\n",
    "\n",
    "Now that we have the encoder and were able to build something useful off of it and ensure its functionality, we should tweak our multihead attention to enable some form of masking and then build a decoder.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_accel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
