{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import sys \n",
    "sys.path.append(\"../models\")\n",
    "from transformer_blocks import Transformer\n",
    "from torch.utils.data import random_split\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTransformer(nn.Module): \n",
    "    def __init__(self, d_k, d_model, d_v, d_ff, num_heads, num_layers, num_classes, vocab_size, dropout=0.1) -> None:\n",
    "        super(ClassificationTransformer, self).__init__()\n",
    "        self.encoder_only_transformer = Transformer(d_k, d_model, d_v, d_ff, num_heads, num_layers, vocab_size, dropout=0.1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(d_model, d_model)\n",
    "        self.fc2 = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.encoder_only_transformer(x)\n",
    "        avg_pool = torch.mean(out, dim=-2)\n",
    "        return self.fc2(self.dropout(self.fc1(self.dropout(avg_pool))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "# Convert to list to enable random splitting\n",
    "train_dataset = list(train_iter)\n",
    "\n",
    "#80-20 train-val split \n",
    "train_size = int(len(train_dataset) * 0.8)  \n",
    "val_size = len(train_dataset) - train_size  \n",
    "train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "VOCAB_SIZE = 5000\n",
    "\n",
    "# Build vocab based on the train_data\n",
    "train_data_iter = (text for _, text in train_data)\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_data_iter), specials=[\"<unk>\"], max_tokens=VOCAB_SIZE)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "def collate_batch(batch):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    \n",
    "    # Sort the batch in the descending order\n",
    "    batch.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "        \n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.int64)\n",
    "    \n",
    "    # Pad sequences\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "    \n",
    "    return label_list.to(device), text_list.to(device), lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size = 8, shuffle = True, collate_fn = collate_batch)\n",
    "val_loader = DataLoader(val_data, batch_size = 8, shuffle = False, collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 3e-4\n",
    "NUM_EPOCHS = 50\n",
    "DROPOUT = 0.2\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "D_K = 128\n",
    "D_V = D_K\n",
    "D_MODEL = D_K * 2\n",
    "D_FF = D_MODEL * 2\n",
    "NUM_LAYERS = 2\n",
    "OUTPUT_DIM = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationTransformer(D_K, D_MODEL, D_V, D_FF, num_heads=3, num_classes=OUTPUT_DIM, num_layers=2, vocab_size=VOCAB_SIZE)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, loss_function, optim, epochs, device):\n",
    "    losses = [] #group losses for loss visualization \n",
    "    running_loss = 0.0\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        print(\"Epoch %d / %d\" % (epoch+1, epochs))\n",
    "        print(\"-\"*10)\n",
    "    \n",
    "        for i, batch_data in enumerate(train_loader):\n",
    "            (y, x, x_size) = batch_data\n",
    "            logits = model(x)\n",
    "            loss = loss_function(logits, y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            running_loss += loss.item()\n",
    "            losses.append(loss)\n",
    "\n",
    "            if (i+1) % 1000 == 0:\n",
    "                print(\"Step: {}, average training loss over last 2000 steps: {:.4f}\".format(i+1, running_loss/1000))\n",
    "                running_loss = 0.0\n",
    "            \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            correct_pred = 0.0\n",
    "            num_samples = 0\n",
    "            for i, batch_data in enumerate(val_loader):\n",
    "                (y, x, x_size) = batch_data\n",
    "                y, x, x_size = y.to(device), x.to(device), x_size.to(device)\n",
    "                logits = model(x)\n",
    "                loss = loss_function(logits, y)\n",
    "                _, predicted_labels = torch.max(logits, 1)\n",
    "                correct_pred += (predicted_labels.long() == y.long()).sum()\n",
    "                num_samples+=predicted_labels.shape[0]\n",
    "                val_loss += loss.item()\n",
    "            \n",
    "            val_accuracy = (correct_pred / num_samples) * 100\n",
    "            val_losses.append(val_loss)\n",
    "        print(\"Epoch: {}, validation loss: {:.4f}, val accuracy: {:.2f}\".format(epoch+1, val_loss/len(val_loader), val_accuracy))\n",
    "    \n",
    "    return losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aryaman.pandya/ml_accel/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 1.0266\n",
      "Step: 2000, average training loss over last 2000 steps: 0.5943\n",
      "Step: 3000, average training loss over last 2000 steps: 0.5062\n",
      "Step: 4000, average training loss over last 2000 steps: 0.4616\n",
      "Step: 5000, average training loss over last 2000 steps: 0.4308\n",
      "Step: 6000, average training loss over last 2000 steps: 0.4151\n",
      "Step: 7000, average training loss over last 2000 steps: 0.4083\n",
      "Step: 8000, average training loss over last 2000 steps: 0.3991\n",
      "Step: 9000, average training loss over last 2000 steps: 0.4013\n",
      "Step: 10000, average training loss over last 2000 steps: 0.3784\n",
      "Step: 11000, average training loss over last 2000 steps: 0.3690\n",
      "Step: 12000, average training loss over last 2000 steps: 0.3558\n",
      "Epoch: 1, validation loss: 0.3677, val accuracy: 87.29\n",
      "Epoch 2 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.3211\n",
      "Step: 2000, average training loss over last 2000 steps: 0.3210\n",
      "Step: 3000, average training loss over last 2000 steps: 0.3254\n",
      "Step: 4000, average training loss over last 2000 steps: 0.3286\n",
      "Step: 5000, average training loss over last 2000 steps: 0.3280\n",
      "Step: 6000, average training loss over last 2000 steps: 0.3164\n",
      "Step: 7000, average training loss over last 2000 steps: 0.3234\n",
      "Step: 8000, average training loss over last 2000 steps: 0.3239\n",
      "Step: 9000, average training loss over last 2000 steps: 0.3229\n",
      "Step: 10000, average training loss over last 2000 steps: 0.3213\n",
      "Step: 11000, average training loss over last 2000 steps: 0.3313\n",
      "Step: 12000, average training loss over last 2000 steps: 0.3078\n",
      "Epoch: 2, validation loss: 0.3329, val accuracy: 88.97\n",
      "Epoch 3 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2858\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2785\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2737\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2916\n",
      "Step: 5000, average training loss over last 2000 steps: 0.3145\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2962\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2888\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2894\n",
      "Step: 9000, average training loss over last 2000 steps: 0.3058\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2910\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2935\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2948\n",
      "Epoch: 3, validation loss: 0.2969, val accuracy: 90.20\n",
      "Epoch 4 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2656\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2816\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2681\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2671\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2865\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2740\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2623\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2790\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2686\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2851\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2819\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2981\n",
      "Epoch: 4, validation loss: 0.2911, val accuracy: 90.38\n",
      "Epoch 5 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2454\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2426\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2787\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2768\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2634\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2686\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2584\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2529\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2868\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2754\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2758\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2861\n",
      "Epoch: 5, validation loss: 0.2897, val accuracy: 90.55\n",
      "Epoch 6 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2646\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2415\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2532\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2560\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2690\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2590\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2513\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2774\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2716\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2564\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2639\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2597\n",
      "Epoch: 6, validation loss: 0.3326, val accuracy: 88.55\n",
      "Epoch 7 / 50\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = train(model, train_loader, val_loader, torch.nn.functional.cross_entropy, optimizer, NUM_EPOCHS, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do I want to see from this training? \n",
    "\n",
    "- I want to see how long it takes for this to converge \n",
    "- I want to see the computational complexity (how long per epoch?)\n",
    "    - anywhere from 60 seconds an epoch to 3 minutes an epoch depending on hyperparameters \n",
    "    - With the low\n",
    "- How does the model accuracy respond to changes in hyperparams? Not just LR and the usuals, but also d_model, d_k, d_v, d_ff\n",
    "    - This is intuitive, higher values for these hyper params work better (until a point of diminishing return) since there's a balance between trying to capture all the intricacies of the data in high dimensional vector space and the actual complexity of the data to begin with. (as well as computational constraints) - doubling these dimensions led to more batch efficient learning but at the same time increased runtime by a proportional 2x \n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts so far: \n",
    "\n",
    "This architecture is way more compute intensive but at the same time so much more batch efficient than previously explored ones when tuned with the right hyperparameters (LSTMs, RNNs even when bidirectional + multi layered and so on) that we see near 90% validation accuracy after just one or two epochs. These epochs, however, take tremendously long (7 minutes per epoch v/s the 1 minute LSTMs were taking). Part of this could be becauase of the implementation being from scratch (torch.transformer might be faster), but most of this comes from the computational load the multiple blocks (and their individual complexities)\n",
    "\n",
    "Now that we have the encoder and were able to build something useful off of it and ensure its functionality, we should tweak our multihead attention to enable some form of masking and then build a decoder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_accel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
