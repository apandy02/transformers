{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers 101\n",
    "\n",
    "This notebook serves as an exploration of the transformer architecture (Vaswani et. al.) Here, we'll implement in native PyTorch the basic building blocks of the transformer and then put them all together so we have a model architecture to put into `../models`\n",
    "\n",
    "In the process of putting this together (much like my other exploratory projects) I tried to limit viewing existing code online, and primarily used my notes (pdf attached for anyone interested) as a foundation for this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want something with output dims: (sequence_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_embedding(input_tensor: torch.Tensor, output_dim: int, n=10000): \n",
    "    \"\"\"\n",
    "    Here, we implement the naive approach from the original \n",
    "    paper with the sin and cosine functions. \n",
    "    \"\"\"\n",
    "    P = torch.zeros((input_tensor.shape[-1], output_dim))\n",
    "    indices = torch.arange(input_tensor.size(-1))\n",
    "    i_values = torch.arange(int(output_dim/2))\n",
    "    denominators = torch.float_power(n, 2*i_values/output_dim)\n",
    "    P[:, 0::2] = torch.sin(indices.unsqueeze(1) / denominators.unsqueeze(0)) # start at 0, step by 2 sin for even nums\n",
    "    P[:, 1::2] = torch.cos(indices.unsqueeze(1) / denominators.unsqueeze(0)) # start at 1, step by 2 cos for odd nums\n",
    "    return P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000],\n",
       "        [ 0.8415,  0.5403,  0.8415],\n",
       "        [ 0.9093, -0.4161,  0.9093],\n",
       "        [ 0.1411, -0.9900,  0.1411],\n",
       "        [-0.7568, -0.6536, -0.7568]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((2, 5))\n",
    "output_dims = 3\n",
    "positional_embedding(a, output_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(x): \n",
    "    \"\"\"\n",
    "    Simple dot product based attention\n",
    "    \"\"\"\n",
    "    query_layer, key_layer, value_layer = nn.Linear(x.shape[-1], x.shape[-1]), nn.Linear(x.shape[-1], x.shape[-1]), nn.Linear(x.shape[-1], x.shape[-1])\n",
    "    query, key, value = query_layer(x), key_layer(x), value_layer(x)\n",
    "    attention_weights  = torch.nn.Softmax(-1)(torch.tensordot(query, key, dims=1))\n",
    "    return torch.sum(value * attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2987, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1, 12)\n",
    "attention(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_norm(residual: torch.Tensor, hidden: torch.Tensor): \n",
    "    if residual.shape != hidden.shape: \n",
    "        raise ValueError(\"Shapes mismatch\")\n",
    "    else: \n",
    "        output = residual + hidden # element wise addition\n",
    "        layer_norm = nn.LayerNorm([residual.shape[-2], residual.shape[-1]])\n",
    "        return layer_norm(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8880, 0.2732, 0.9008, 0.7261, 0.3817, 0.5167],\n",
      "         [0.9505, 0.1640, 0.9440, 0.8701, 0.3452, 0.9666],\n",
      "         [0.8365, 0.6466, 0.7730, 0.3295, 0.8195, 0.6639],\n",
      "         [0.5948, 0.3429, 0.1542, 0.4754, 0.5124, 0.6733],\n",
      "         [0.8461, 0.8573, 0.4217, 0.9035, 0.1438, 0.6029]]])\n",
      "tensor([[[0.3105, 0.7641, 0.8900, 0.7077, 0.2874, 0.8929],\n",
      "         [0.8561, 0.3155, 0.8284, 0.6441, 0.8224, 0.9303],\n",
      "         [0.8458, 0.9986, 0.6091, 0.9989, 0.5183, 0.3800],\n",
      "         [0.2779, 0.3161, 0.4261, 0.1374, 0.2840, 0.5407],\n",
      "         [0.1030, 0.4531, 0.0115, 0.3710, 0.4745, 0.6053]]])\n",
      "Added: tensor([[[1.1985, 1.0373, 1.7908, 1.4338, 0.6692, 1.4096],\n",
      "         [1.8067, 0.4795, 1.7724, 1.5142, 1.1676, 1.8969],\n",
      "         [1.6823, 1.6453, 1.3820, 1.3284, 1.3378, 1.0439],\n",
      "         [0.8727, 0.6590, 0.5803, 0.6128, 0.7964, 1.2140],\n",
      "         [0.9491, 1.3104, 0.4331, 1.2745, 0.6183, 1.2083]]])\n",
      "Final: tensor([[[ 0.0655, -0.3161,  1.4676,  0.6224, -1.1876,  0.5652],\n",
      "         [ 1.5051, -1.6365,  1.4239,  0.8129, -0.0076,  1.7187],\n",
      "         [ 1.2108,  1.1231,  0.4999,  0.3730,  0.3953, -0.3006],\n",
      "         [-0.7058, -1.2117, -1.3979, -1.3209, -0.8864,  0.1023],\n",
      "         [-0.5249,  0.3304, -1.7463,  0.2453, -1.3079,  0.0886]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# usage example: \n",
    "\n",
    "tensor_a = torch.rand([1, 5, 6]) # batch size, sequence length, embedding dimensions\n",
    "tensor_b = torch.rand([1, 5, 6])\n",
    "print(tensor_a)\n",
    "print(tensor_b)\n",
    "print(f\"Final: {add_norm(tensor_a, tensor_b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention():\n",
    "    \"\"\"\n",
    "    Scaled Dot product based multi-head attention\n",
    "    \"\"\"\n",
    "    query_layer, key_layer, value_layer = nn.Linear(x.shape[-1], x.shape[-1]), nn.Linear(x.shape[-1], x.shape[-1]), nn.Linear(x.shape[-1], x.shape[-1])\n",
    "    query, key, value = query_layer(x), key_layer(x), value_layer(x)\n",
    "    attention_weights  = torch.nn.Softmax(-1)(torch.tensordot(query, key, dims=1))\n",
    "    return torch.sum(value * attention_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_accel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
