{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers 101\n",
    "\n",
    "This notebook serves as an exploration of the transformer architecture (Vaswani et. al.) Here, we'll implement in native PyTorch the basic building blocks of the transformer and then put them all together so we have a model architecture to put into `../models`\n",
    "\n",
    "In the process of putting this together (much like my other exploratory projects) I tried to limit viewing existing code online, and primarily used my notes (pdf attached for anyone interested) as a foundation for this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want something with output dims: (sequence_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_embedding(input_tensor: torch.Tensor, output_dim: int, n=10000): \n",
    "    \"\"\"\n",
    "    Here, we implement the naive approach from the original \n",
    "    paper with the sin and cosine functions. \n",
    "    \"\"\"\n",
    "    P = torch.zeros((input_tensor.shape[-1], output_dim))\n",
    "    indices = torch.arange(input_tensor.size(-1))\n",
    "    i_values = torch.arange(int(output_dim/2))\n",
    "    denominators = torch.float_power(n, 2*i_values/output_dim)\n",
    "    P[:, 0::2] = torch.sin(indices.unsqueeze(1) / denominators.unsqueeze(0)) # start at 0, step by 2 sin for even nums\n",
    "    P[:, 1::2] = torch.cos(indices.unsqueeze(1) / denominators.unsqueeze(0)) # start at 1, step by 2 cos for odd nums\n",
    "    return P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000],\n",
       "        [ 0.8415,  0.5403,  0.8415],\n",
       "        [ 0.9093, -0.4161,  0.9093],\n",
       "        [ 0.1411, -0.9900,  0.1411],\n",
       "        [-0.7568, -0.6536, -0.7568]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((2, 5))\n",
    "output_dims = 3\n",
    "positional_embedding(a, output_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(x): \n",
    "    \"\"\"\n",
    "    Simple dot product based attention\n",
    "    \"\"\"\n",
    "    query_layer, key_layer, value_layer = nn.Linear(x.shape[-1], x.shape[-1]), nn.Linear(x.shape[-1], x.shape[-1]), nn.Linear(x.shape[-1], x.shape[-1])\n",
    "    query, key, value = query_layer(x), key_layer(x), value_layer(x)\n",
    "    attention_weights  = torch.nn.Softmax(-1)(torch.tensordot(query, key, dims=1))\n",
    "    return torch.sum(value * attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0080, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1, 12)\n",
    "attention(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_norm(residual: torch.Tensor, hidden: torch.Tensor): \n",
    "    if residual.shape != hidden.shape: \n",
    "        raise ValueError(\"Shapes mismatch\")\n",
    "    else: \n",
    "        output = residual + hidden # element wise addition\n",
    "        layer_norm = nn.LayerNorm([residual.shape[-2], residual.shape[-1]])\n",
    "        return layer_norm(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1045, 0.4312, 0.0299, 0.8858, 0.9968, 0.1152],\n",
      "         [0.8752, 0.2635, 0.2181, 0.6305, 0.3234, 0.0159],\n",
      "         [0.5359, 0.7384, 0.2475, 0.3941, 0.5815, 0.7737],\n",
      "         [0.0514, 0.8584, 0.9550, 0.1863, 0.0683, 0.8739],\n",
      "         [0.0726, 0.5960, 0.9250, 0.5273, 0.2494, 0.8675]]])\n",
      "tensor([[[0.9378, 0.8276, 0.3353, 0.4124, 0.3723, 0.7553],\n",
      "         [0.1377, 0.5286, 0.3779, 0.4345, 0.5243, 0.9111],\n",
      "         [0.5583, 0.2620, 0.9314, 0.6691, 0.2225, 0.4130],\n",
      "         [0.9031, 0.4765, 0.7315, 0.8451, 0.5465, 0.3815],\n",
      "         [0.9565, 0.4849, 0.0307, 0.1105, 0.7993, 0.4004]]])\n",
      "Final: tensor([[[ 0.0758,  0.9001, -2.5009,  1.0501,  1.3197, -0.5781],\n",
      "         [-0.0360, -0.8759, -1.6224,  0.1626, -0.6648, -0.3628],\n",
      "         [ 0.2734, -0.0834,  0.5958,  0.1556, -0.8310,  0.6256],\n",
      "         [-0.2583,  1.1896,  2.5278,  0.0344, -1.5507,  0.8871],\n",
      "         [ 0.0259,  0.2226, -0.2535, -1.4633,  0.1003,  0.9343]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# usage example: \n",
    "\n",
    "tensor_a = torch.rand([1, 5, 6]) # batch size, sequence length, embedding dimensions\n",
    "tensor_b = torch.rand([1, 5, 6])\n",
    "print(tensor_a)\n",
    "print(tensor_b)\n",
    "print(f\"Final: {add_norm(tensor_a, tensor_b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, d_k):\n",
    "    # in order to align the dimensions for the dot product, we transpose k along the last two dimensions like this\n",
    "    return torch.nn.Softmax(-1)(torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d_k and d_v are essentially hyperparameters that are fixed before training. This allows for the query and keys to have the same dimensionality, and for all 3 of them to have consistent dimensionality. In many transformer implementations, d_k and d_v are set to be the same for simplicity but this is not always the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention(k, q, v, d_k, d_v, d_model, num_heads):\n",
    "    \"\"\"\n",
    "    Scaled Dot product based multi-head attention\n",
    "    \"\"\"\n",
    "    \n",
    "    query_layer, key_layer, value_layer = nn.Linear(d_model, num_heads* d_k), nn.Linear(d_model, num_heads* d_k), nn.Linear(d_model, num_heads*d_v)\n",
    "    k_len, q_len, v_len, batch_size = k.size(1), q.size(1), v.size(1),  q.size(0)\n",
    "    print(key_layer(k).shape)\n",
    "    print(query_layer(q).shape)\n",
    "    print(value_layer(v).shape)\n",
    "    k, q, v = key_layer(k).view(batch_size, k_len,  num_heads, d_k), query_layer(q).view(batch_size, q_len,  num_heads, d_k), value_layer(v).view(batch_size, v_len,  num_heads, d_v)\n",
    "    \n",
    "    residual = q\n",
    "    \n",
    "    q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "    attention = scaled_dot_product_attention(q, k, d_k)\n",
    "    output = torch.matmul(attention, v)\n",
    "\n",
    "    output = output.transpose(1, 2).contiguous().view(batch_size, q_len, -1)\n",
    "    concatenated_projection = nn.Linear(num_heads * d_v, d_model, bias=False)\n",
    "\n",
    "    output = concatenated_projection(output)\n",
    "    print(r)\n",
    "    output += residual\n",
    "\n",
    "    layer_norm = nn.LayerNorm([residual.shape[-2], residual.shape[-1]])\n",
    "    output = layer_norm(output)\n",
    "\n",
    "    return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 20])\n",
      "torch.Size([1, 2, 20])\n",
      "torch.Size([1, 2, 20])\n",
      "hey\n",
      "torch.Size([1, 4, 2, 5])\n",
      "torch.Size([1, 4, 2, 5])\n",
      "torch.Size([1, 4, 2, 5])\n",
      "torch.Size([1, 4, 2, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (512) must match the size of tensor b (5) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m d_k, d_v \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m, \u001b[39m5\u001b[39m\n\u001b[1;32m      6\u001b[0m num_heads \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[0;32m----> 8\u001b[0m multihead_attention(k, q, v, d_k, d_v, d_model, num_heads)\n",
      "Cell \u001b[0;32mIn[43], line 28\u001b[0m, in \u001b[0;36mmultihead_attention\u001b[0;34m(k, q, v, d_k, d_v, d_model, num_heads)\u001b[0m\n\u001b[1;32m     25\u001b[0m concatenated_projection \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(num_heads \u001b[39m*\u001b[39m d_v, d_model, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m output \u001b[39m=\u001b[39m concatenated_projection(output)\n\u001b[0;32m---> 28\u001b[0m output \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m residual\n\u001b[1;32m     30\u001b[0m layer_norm \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm([residual\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m], residual\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]])\n\u001b[1;32m     31\u001b[0m output \u001b[39m=\u001b[39m layer_norm(output)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (5) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "\n",
    "# from the paper: To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d model \n",
    "k, q, v = torch.rand((1, 2, d_model)), torch.rand((1, 2, d_model)), torch.rand((1, 2, d_model))\n",
    "d_k, d_v = 5, 5\n",
    "num_heads = 4\n",
    "\n",
    "multihead_attention(k, q, v, d_k, d_v, d_model, num_heads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_accel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
