{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers 101\n",
    "\n",
    "This notebook serves as an exploration of the transformer architecture (Vaswani et. al.) Here, we'll implement in native PyTorch the basic building blocks of the transformer and then put them all together so we have a model architecture to put into `../models`\n",
    "\n",
    "In the process of putting this together (much like my other exploratory projects) I tried to limit viewing existing code online, and primarily used my notes (pdf attached for anyone interested) as a foundation for this work.\n",
    "\n",
    "PS: The notes in this notebook are very much stream of thought, interpret accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "In the first half of the notebook we'll develop all the pieces needed to implement the transformer encoder, then train and test it on some task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want something with output dims: (sequence_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_embedding(input_tensor: torch.Tensor, output_dim: int, n=10000): \n",
    "    \"\"\"\n",
    "    Here, we implement the naive approach from the original \n",
    "    paper with the sin and cosine functions. \n",
    "    \"\"\"\n",
    "    P = torch.zeros((input_tensor.shape[-1], output_dim))\n",
    "    indices = torch.arange(input_tensor.size(-1))\n",
    "    i_values = torch.arange(int(output_dim/2))\n",
    "    denominators = torch.float_power(n, 2*i_values/output_dim)\n",
    "    P[:, 0::2] = torch.sin(indices.unsqueeze(1) / denominators.unsqueeze(0)) # start at 0, step by 2 sin for even nums\n",
    "    P[:, 1::2] = torch.cos(indices.unsqueeze(1) / denominators.unsqueeze(0)) # start at 1, step by 2 cos for odd nums\n",
    "    return P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000],\n",
       "        [ 0.8415,  0.5403,  0.8415],\n",
       "        [ 0.9093, -0.4161,  0.9093],\n",
       "        [ 0.1411, -0.9900,  0.1411],\n",
       "        [-0.7568, -0.6536, -0.7568]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((2, 5))\n",
    "output_dims = 3\n",
    "positional_embedding(a, output_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(x): \n",
    "    \"\"\"\n",
    "    Simple dot product based attention\n",
    "    \"\"\"\n",
    "    query_layer, key_layer, value_layer = nn.Linear(x.shape[-1], x.shape[-1]), nn.Linear(x.shape[-1], x.shape[-1]), nn.Linear(x.shape[-1], x.shape[-1])\n",
    "    query, key, value = query_layer(x), key_layer(x), value_layer(x)\n",
    "    attention_weights  = torch.nn.Softmax(-1)(torch.tensordot(query, key, dims=1))\n",
    "    return torch.sum(value * attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0560, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1, 12)\n",
    "attention(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to emulate how it would be implemented, we write out the add norm function below. However in practice, this will be encompassed by each transformer sub module since each of them are followed by addition with residual and layer normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_norm(residual: torch.Tensor, hidden: torch.Tensor): \n",
    "    if residual.shape != hidden.shape: \n",
    "        raise ValueError(\"Shapes mismatch\")\n",
    "    else: \n",
    "        output = residual + hidden # element wise addition\n",
    "        layer_norm = nn.LayerNorm([residual.shape[-2], residual.shape[-1]])\n",
    "        return layer_norm(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9532, 0.5663, 0.1592, 0.6349, 0.5473, 0.1249],\n",
      "         [0.6367, 0.4738, 0.2162, 0.6691, 0.6481, 0.0346],\n",
      "         [0.8744, 0.8900, 0.4515, 0.6918, 0.8821, 0.2913],\n",
      "         [0.5342, 0.6217, 0.8234, 0.6057, 0.1478, 0.6799],\n",
      "         [0.5399, 0.8586, 0.7641, 0.9113, 0.9008, 0.5750]]])\n",
      "tensor([[[0.8720, 0.5362, 0.6601, 0.7187, 0.5448, 0.8877],\n",
      "         [0.0837, 0.1568, 0.9402, 0.2934, 0.1990, 0.9947],\n",
      "         [0.2649, 0.6515, 0.6303, 0.5171, 0.0292, 0.5292],\n",
      "         [0.8399, 0.3406, 0.7965, 0.7523, 0.4417, 0.5256],\n",
      "         [0.0591, 0.7710, 0.9399, 0.9753, 0.7430, 0.9825]]])\n",
      "Final: tensor([[[ 1.7711, -0.2112, -0.9880,  0.4775, -0.2398, -0.4578],\n",
      "         [-1.2589, -1.5056, -0.0635, -0.5949, -0.9117, -0.4118],\n",
      "         [-0.1102,  0.9927, -0.2677,  0.0808, -0.7356, -0.9846],\n",
      "         [ 0.5337, -0.5956,  1.2081,  0.4895, -1.6181,  0.0714],\n",
      "         [-1.5922,  1.2344,  1.4386,  1.9394,  1.2734,  1.0366]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# usage example: \n",
    "\n",
    "tensor_a = torch.rand([1, 5, 6]) # batch size, sequence length, embedding dimensions\n",
    "tensor_b = torch.rand([1, 5, 6])\n",
    "print(tensor_a)\n",
    "print(tensor_b)\n",
    "print(f\"Final: {add_norm(tensor_a, tensor_b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, d_k, mask = None):\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        # think of the last two dimensions of the attention tensor to be attention values for token-token pairs based on their indices (like a look up table of attention weights)\n",
    "        # the diagonal will be the attention to the current token (self). The upper diagonal will contain attention with respect to tokens in the future and the lower in the past\n",
    "        # therefore our \n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    return nn.Softmax(-1)(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d_k and d_v are essentially hyperparameters that are fixed before training. This allows for the query and keys to have the same dimensionality, and for all 3 of them to have consistent dimensionality. In many transformer implementations, d_k and d_v are set to be the same for simplicity but this is not always the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention(k, q, v, d_k, d_v, d_model, num_heads):\n",
    "    \"\"\"\n",
    "    Scaled Dot product based multi-head attention\n",
    "    \"\"\"\n",
    "    # declare projection layers - assume all inputs have d_model size in the last dimension, and project to number of heads * d_k or d_v \n",
    "    query_layer, key_layer, value_layer = nn.Linear(d_model, num_heads* d_k), nn.Linear(d_model, num_heads* d_k), nn.Linear(d_model, num_heads*d_v)\n",
    "    k_len, q_len, v_len, batch_size = k.size(1), q.size(1), v.size(1),  q.size(0)\n",
    "    residual = q\n",
    "\n",
    "    # in the following line we apply the linear projections and then reshape the outputs for multihead attention. \n",
    "    #The reshaping splits the last dimension of the linear layer's output into num_heads and d_k (or d_v for value). \n",
    "    # This creates multiple \"heads\" in the tensor, each with its own d_k (or d_v) dimension\n",
    "    k, q, v = key_layer(k).view(batch_size, k_len,  num_heads, d_k), query_layer(q).view(batch_size, q_len,  num_heads, d_k), value_layer(v).view(batch_size, v_len,  num_heads, d_v)\n",
    "    \n",
    "    # we perform the following transpose so that the num heads dimension preceeds the seq length dimension. This way, each head can capture different information about the same sequence\n",
    "    q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "    attention = scaled_dot_product_attention(q, k, d_k)\n",
    "    print(f\"attention shape: {attention.shape}, value_shape: {v.shape}\")\n",
    "    output = torch.matmul(attention, v)\n",
    "    print(output.shape)\n",
    "    # following reshaping is done so that we can add our output to the residual \n",
    "    output = output.transpose(1, 2).contiguous().view(batch_size, q_len, -1)\n",
    "    concatenated_projection = nn.Linear(num_heads * d_v, d_model, bias=False)\n",
    "\n",
    "    output = concatenated_projection(output)\n",
    "    output += residual\n",
    "\n",
    "    print(residual.shape)\n",
    "    layer_norm = nn.LayerNorm([residual.shape[-2], residual.shape[-1]])\n",
    "    output = layer_norm(output)\n",
    "\n",
    "    return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 512])\n",
      "attention shape: torch.Size([1, 4, 2, 2]), value_shape: torch.Size([1, 4, 2, 5])\n",
      "torch.Size([1, 4, 2, 5])\n",
      "torch.Size([1, 2, 512])\n",
      "torch.Size([1, 2, 512]) torch.Size([1, 4, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "\n",
    "# from the paper: To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d model \n",
    "k, q, v = torch.rand((1, 2, d_model)), torch.rand((1, 2, d_model)), torch.rand((1, 2, d_model))\n",
    "d_k, d_v = 5, 5\n",
    "num_heads = 4\n",
    "print(v.shape)\n",
    "out, attn = multihead_attention(k, q, v, d_k, d_v, d_model, num_heads)\n",
    "print(out.shape, attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module): \n",
    "    def __init__(self, d_model, d_ff, dropout) -> None:\n",
    "        super(PositionWiseFFN, self).__init__()\n",
    "        self.fc1 = nn.Sequential(nn.Linear(d_model, d_ff, bias=True),nn.ReLU())\n",
    "        self.fc2 = nn.Linear(d_ff, d_model, bias=True)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.fc2(self.fc1(x))        \n",
    "        return self.dropout(self.layer_norm(x+residual))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4779, -1.2587,  1.5941,  ...,  1.1901,  0.0202, -0.0000],\n",
       "         [-0.8031, -1.1463,  0.5704,  ..., -2.0207, -0.0000,  1.3551]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = PositionWiseFFN(d_model, 2048, 0.1)\n",
    "x = torch.rand((1, 2, d_model))\n",
    "ffn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've implemented the lowest level building blocks of the transormer, below we put them together to build transformer blocks, encoder and decoder layers, and the complete transformer architecture. Now we try and condense everything to a more concise-less experimental implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module): \n",
    "    def __init__(self, d_k, d_model, d_v, dropout, num_heads) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_k, self.d_v, self.d_model, self.num_heads = d_k, d_v, d_model, num_heads\n",
    "        self.query_layer, self.key_layer, self.value_layer = nn.Linear(d_model, num_heads* d_k), nn.Linear(d_model, num_heads* d_k), nn.Linear(d_model, num_heads*d_v)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.concat_projection = nn.Linear(num_heads*d_v, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask = None):\n",
    "        k_len, q_len, v_len, batch_size = k.size(1), q.size(1), v.size(1),  q.size(0)\n",
    "        residual = q\n",
    "        k, q, v = self.key_layer(k).view(batch_size, k_len,  self.num_heads, self.d_k), self.query_layer(q).view(batch_size, q_len,  self.num_heads, self.d_k), self.value_layer(v).view(batch_size, v_len,  self.num_heads, self.d_v)\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "        attention = scaled_dot_product_attention(q, k, self.d_k, mask)\n",
    "        output = torch.matmul(attention, v)\n",
    "        output = self.concat_projection(output.transpose(1, 2).contiguous().view(batch_size, q_len, -1))\n",
    "        return self.dropout(self.layer_norm(output+residual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module): \n",
    "    def __init__(self, d_k, d_model, d_v, num_heads, d_ff, dropout) -> None:\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.k_layer, self.q_layer, self.v_layer = nn.Linear(d_model, d_model), nn.Linear(d_model, d_model), nn.Linear(d_model, d_model)\n",
    "        self.multihead_attention = MultiHeadAttention(d_k, d_model, d_v, dropout, num_heads)\n",
    "        self.pointwise_ffn = PositionWiseFFN(d_model, d_ff, dropout)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        k, q, v = self.k_layer(x), self.q_layer(x), self.v_layer(x)\n",
    "        output = self.multihead_attention(q, k, v)\n",
    "        return self.pointwise_ffn(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following encoder implementation is based off of the block diagram from Attention Is All You Need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_k, d_model, d_v, d_ff, num_heads, num_layers, vocab_size, dropout=0.1) -> None:\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.positional_embedding = positional_embedding\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layers = [EncoderLayer(d_k, d_model, d_v, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        x = self.dropout(embedded + self.positional_embedding(x, self.d_model))\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary testing on text classification task (encoder only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTransformer(nn.Module): \n",
    "    def __init__(self, d_k, d_model, d_v, d_ff, num_heads, num_layers, num_classes, vocab_size, dropout=0.1) -> None:\n",
    "        super(ClassificationTransformer, self).__init__()\n",
    "        self.encoder_only_transformer = Encoder(d_k, d_model, d_v, d_ff, num_heads, num_layers, vocab_size, dropout=0.1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(d_model, d_model)\n",
    "        self.fc2 = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.encoder_only_transformer(x)\n",
    "        avg_pool = torch.mean(out, dim=-2)\n",
    "        return self.fc2(self.dropout(self.fc1(self.dropout(avg_pool))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "# Convert to list to enable random splitting\n",
    "train_dataset = list(train_iter)\n",
    "\n",
    "#80-20 train-val split \n",
    "train_size = int(len(train_dataset) * 0.8)  \n",
    "val_size = len(train_dataset) - train_size  \n",
    "train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "VOCAB_SIZE = 5000\n",
    "\n",
    "# Build vocab based on the train_data\n",
    "train_data_iter = (text for _, text in train_data)\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_data_iter), specials=[\"<unk>\"], max_tokens=VOCAB_SIZE)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "def collate_batch(batch):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    \n",
    "    # Sort the batch in the descending order\n",
    "    batch.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "        \n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.int64)\n",
    "    \n",
    "    # Pad sequences\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "    \n",
    "    return label_list.to(device), text_list.to(device), lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size = 8, shuffle = True, collate_fn = collate_batch)\n",
    "val_loader = DataLoader(val_data, batch_size = 8, shuffle = False, collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-4\n",
    "NUM_EPOCHS = 50\n",
    "DROPOUT = 0.5\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "D_K = 128\n",
    "D_V = 128\n",
    "D_FF = 512\n",
    "D_MODEL = 256\n",
    "NUM_LAYERS = 2\n",
    "OUTPUT_DIM = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationTransformer(D_K, D_MODEL, D_V, D_FF, num_heads=3, num_classes=OUTPUT_DIM, num_layers=2, vocab_size=VOCAB_SIZE)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, loss_function, optim, epochs, device):\n",
    "    losses = [] #group losses for loss visualization \n",
    "    running_loss = 0.0\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        print(\"Epoch %d / %d\" % (epoch+1, epochs))\n",
    "        print(\"-\"*10)\n",
    "    \n",
    "        for i, batch_data in enumerate(train_loader):\n",
    "            (y, x, x_size) = batch_data\n",
    "            logits = model(x)\n",
    "            loss = loss_function(logits, y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            running_loss += loss.item()\n",
    "            losses.append(loss)\n",
    "\n",
    "            if (i+1) % 1000 == 0:\n",
    "                print(\"Step: {}, average training loss over last 2000 steps: {:.4f}\".format(i+1, running_loss/1000))\n",
    "                running_loss = 0.0\n",
    "            \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            correct_pred = 0.0\n",
    "            num_samples = 0\n",
    "            for i, batch_data in enumerate(val_loader):\n",
    "                (y, x, x_size) = batch_data\n",
    "                y, x, x_size = y.to(device), x.to(device), x_size.to(device)\n",
    "                logits = model(x)\n",
    "                loss = loss_function(logits, y)\n",
    "                _, predicted_labels = torch.max(logits, 1)\n",
    "                correct_pred += (predicted_labels.long() == y.long()).sum()\n",
    "                num_samples+=predicted_labels.shape[0]\n",
    "                val_loss += loss.item()\n",
    "            \n",
    "            val_accuracy = (correct_pred / num_samples) * 100\n",
    "            val_losses.append(val_loss)\n",
    "        print(\"Epoch: {}, validation loss: {:.4f}, val accuracy: {:.2f}\".format(epoch+1, val_loss/len(val_loader), val_accuracy))\n",
    "    \n",
    "    return losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 1.2308\n",
      "Step: 2000, average training loss over last 2000 steps: 0.8157\n",
      "Step: 3000, average training loss over last 2000 steps: 0.6615\n",
      "Step: 4000, average training loss over last 2000 steps: 0.5951\n",
      "Step: 5000, average training loss over last 2000 steps: 0.5554\n",
      "Step: 6000, average training loss over last 2000 steps: 0.5198\n",
      "Step: 7000, average training loss over last 2000 steps: 0.5130\n",
      "Step: 8000, average training loss over last 2000 steps: 0.4613\n",
      "Step: 9000, average training loss over last 2000 steps: 0.4418\n",
      "Step: 10000, average training loss over last 2000 steps: 0.4660\n",
      "Step: 11000, average training loss over last 2000 steps: 0.4549\n",
      "Step: 12000, average training loss over last 2000 steps: 0.4407\n",
      "Epoch: 1, validation loss: 0.3764, val accuracy: 87.27\n",
      "Epoch 2 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.3963\n",
      "Step: 2000, average training loss over last 2000 steps: 0.4060\n",
      "Step: 3000, average training loss over last 2000 steps: 0.3941\n",
      "Step: 4000, average training loss over last 2000 steps: 0.3864\n",
      "Step: 5000, average training loss over last 2000 steps: 0.3762\n",
      "Step: 6000, average training loss over last 2000 steps: 0.3745\n",
      "Step: 7000, average training loss over last 2000 steps: 0.3743\n",
      "Step: 8000, average training loss over last 2000 steps: 0.3730\n",
      "Step: 9000, average training loss over last 2000 steps: 0.3710\n",
      "Step: 10000, average training loss over last 2000 steps: 0.3623\n",
      "Step: 11000, average training loss over last 2000 steps: 0.3608\n",
      "Step: 12000, average training loss over last 2000 steps: 0.3561\n",
      "Epoch: 2, validation loss: 0.3506, val accuracy: 88.10\n",
      "Epoch 3 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.3431\n",
      "Step: 2000, average training loss over last 2000 steps: 0.3509\n",
      "Step: 3000, average training loss over last 2000 steps: 0.3375\n",
      "Step: 4000, average training loss over last 2000 steps: 0.3327\n",
      "Step: 5000, average training loss over last 2000 steps: 0.3106\n",
      "Step: 6000, average training loss over last 2000 steps: 0.3350\n",
      "Step: 7000, average training loss over last 2000 steps: 0.3413\n",
      "Step: 8000, average training loss over last 2000 steps: 0.3271\n",
      "Step: 9000, average training loss over last 2000 steps: 0.3125\n",
      "Step: 10000, average training loss over last 2000 steps: 0.3220\n",
      "Step: 11000, average training loss over last 2000 steps: 0.3301\n",
      "Step: 12000, average training loss over last 2000 steps: 0.3408\n",
      "Epoch: 3, validation loss: 0.3277, val accuracy: 89.28\n",
      "Epoch 4 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.3101\n",
      "Step: 2000, average training loss over last 2000 steps: 0.3119\n",
      "Step: 3000, average training loss over last 2000 steps: 0.3072\n",
      "Step: 4000, average training loss over last 2000 steps: 0.3193\n",
      "Step: 5000, average training loss over last 2000 steps: 0.3029\n",
      "Step: 6000, average training loss over last 2000 steps: 0.3160\n",
      "Step: 7000, average training loss over last 2000 steps: 0.3156\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2946\n",
      "Step: 9000, average training loss over last 2000 steps: 0.3038\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2998\n",
      "Step: 11000, average training loss over last 2000 steps: 0.3157\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2954\n",
      "Epoch: 4, validation loss: 0.3169, val accuracy: 89.40\n",
      "Epoch 5 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2934\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2993\n",
      "Step: 3000, average training loss over last 2000 steps: 0.3004\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2841\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2944\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2904\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2976\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2948\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2995\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2910\n",
      "Step: 11000, average training loss over last 2000 steps: 0.3019\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2925\n",
      "Epoch: 5, validation loss: 0.3044, val accuracy: 89.92\n",
      "Epoch 6 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2745\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2719\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2749\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2891\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2858\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2869\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2992\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2866\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2855\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2864\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2839\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2961\n",
      "Epoch: 6, validation loss: 0.3316, val accuracy: 88.72\n",
      "Epoch 7 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2740\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2938\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2765\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2715\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2699\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2763\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2786\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2720\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2763\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2877\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2758\n",
      "Step: 12000, average training loss over last 2000 steps: 0.3002\n",
      "Epoch: 7, validation loss: 0.3001, val accuracy: 89.68\n",
      "Epoch 8 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2650\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2646\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2730\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2795\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2587\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2867\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2766\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2670\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2757\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2789\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2856\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2774\n",
      "Epoch: 8, validation loss: 0.2977, val accuracy: 90.31\n",
      "Epoch 9 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2661\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2625\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2649\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2612\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2762\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2735\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2687\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2778\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2718\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2680\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2794\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2559\n",
      "Epoch: 9, validation loss: 0.3015, val accuracy: 90.22\n",
      "Epoch 10 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2562\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2569\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2510\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2597\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2581\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2614\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2656\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2631\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2757\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2569\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2859\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2909\n",
      "Epoch: 10, validation loss: 0.3018, val accuracy: 89.78\n",
      "Epoch 11 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2491\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2695\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2497\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2610\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2535\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2798\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2621\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2715\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loss, val_loss \u001b[39m=\u001b[39m train(model, train_loader, val_loader, torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mcross_entropy, optimizer, NUM_EPOCHS, DEVICE)\n",
      "Cell \u001b[0;32mIn[70], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, loss_function, optim, epochs, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m i, batch_data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     11\u001b[0m     (y, x, x_size) \u001b[39m=\u001b[39m batch_data\n\u001b[0;32m---> 12\u001b[0m     logits \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     13\u001b[0m     loss \u001b[39m=\u001b[39m loss_function(logits, y)\n\u001b[1;32m     14\u001b[0m     optim\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[64], line 10\u001b[0m, in \u001b[0;36mClassificationTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 10\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder_only_transformer(x)\n\u001b[1;32m     11\u001b[0m     avg_pool \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(out, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(avg_pool))))\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/ml_accel/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[63], line 12\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     11\u001b[0m     embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[0;32m---> 12\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(embedded \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpositional_embedding(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49md_model))\n\u001b[1;32m     13\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m     14\u001b[0m         x \u001b[39m=\u001b[39m layer(x)\n",
      "Cell \u001b[0;32mIn[50], line 11\u001b[0m, in \u001b[0;36mpositional_embedding\u001b[0;34m(input_tensor, output_dim, n)\u001b[0m\n\u001b[1;32m      9\u001b[0m denominators \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfloat_power(n, \u001b[39m2\u001b[39m\u001b[39m*\u001b[39mi_values\u001b[39m/\u001b[39moutput_dim)\n\u001b[1;32m     10\u001b[0m P[:, \u001b[39m0\u001b[39m::\u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msin(indices\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m denominators\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)) \u001b[39m# start at 0, step by 2 sin for even nums\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m P[:, \u001b[39m1\u001b[39m::\u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcos(indices\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m) \u001b[39m/\u001b[39;49m denominators\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m)) \u001b[39m# start at 1, step by 2 cos for odd nums\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mreturn\u001b[39;00m P\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = train(model, train_loader, val_loader, torch.nn.functional.cross_entropy, optimizer, NUM_EPOCHS, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do I want to see from this training? \n",
    "\n",
    "- I want to see how long it takes for this to converge \n",
    "- I want to see the computational complexity (how long per epoch?)\n",
    "    - anywhere from 90 seconds an epoch to 7 minutes an epoch depending on hyperparameters \n",
    "    - With the low\n",
    "- How does the model accuracy respond to changes in hyperparams? Not just LR and the usuals, but also d_model, d_k, d_v, d_ff\n",
    "    - This is intuitive, higher values for these hyper params work better (until a point of diminishing return) since there's a balance between trying to capture all the intricacies of the data in high dimensional vector space and the actual complexity of the data to begin with. (as well as computational constraints) - doubling these dimensions led to more batch efficient learning but at the same time increased runtime by a proportional 2x \n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts so far: \n",
    "\n",
    "This architecture is way more compute intensive but at the same time so much more batch efficient than previously explored ones when tuned with the right hyperparameters (LSTMs, RNNs even when bidirectional + multi layered and so on) that we see near 90% validation accuracy after just one or two epochs. These epochs, however, take tremendously long (7 minutes per epoch v/s the 1 minute LSTMs were taking). Part of this could be becauase of the implementation being from scratch (torch.transformer might be faster), but most of this comes from the computational load the multiple blocks (and their individual complexities)\n",
    "\n",
    "Now that we have the encoder and were able to build something useful off of it and ensure its functionality, we should tweak our multihead attention to enable some form of masking and then build a decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Now, we build the decoder and train and test it for some task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_k, d_model, d_v, num_heads, d_ff, dropout) -> None:\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.k_layer, self.q_layer, self.v_layer = nn.Linear(d_model, d_model), nn.Linear(d_model, d_model), nn.Linear(d_model, d_model)\n",
    "        self.k_layer2, self.q_layer2, self.v_layer2 = nn.Linear(d_model, d_model), nn.Linear(d_model, d_model), nn.Linear(d_model, d_model)\n",
    "        self.masked_multihead_attention = MultiHeadAttention(d_k, d_model, d_v, dropout, num_heads)\n",
    "        self.multihead_attention = MultiHeadAttention(d_k, d_model, d_v, dropout, num_heads)\n",
    "        self.positiontwise_ffn = PositionWiseFFN(d_model, d_ff, dropout)\n",
    "    \n",
    "    def forward(self, outputs, encoder_output):\n",
    "        k1, q1, v1 = self.k_layer(outputs), self.q_layer(outputs), self.v_layer(outputs)\n",
    "        outputs = self.masked_multihead_attention(q1, k1, v1, True)\n",
    "        k2, q2, v2 = self.k_layer2(encoder_output), self.q_layer2(outputs), self.v_layer2(encoder_output)  # query comes from outputs of previous decoder layer while k, v come from the encoder's output\n",
    "        outputs = self.multihead_attention(q2, k2, v2)\n",
    "        return self.positiontwise_ffn(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_accel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
