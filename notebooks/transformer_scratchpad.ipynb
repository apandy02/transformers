{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers 101\n",
    "\n",
    "This notebook serves as an exploration of the transformer architecture (Vaswani et. al.) Here, we'll implement in native PyTorch the basic building blocks of the transformer and then put them all together so we have a model architecture to put into `../models`\n",
    "\n",
    "In the process of putting this together (much like my other exploratory projects) I tried to limit viewing existing code online, and primarily used my notes (pdf attached for anyone interested) as a foundation for this work.\n",
    "\n",
    "PS: The notes in this notebook are very much stream of thought, interpret accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "In the first half of the notebook we'll develop all the pieces needed to implement the transformer encoder, then train and test it on some task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want something with output dims: (sequence_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_embedding(input_tensor: torch.Tensor, output_dim: int, n=10000): \n",
    "    \"\"\"\n",
    "    Here, we implement the naive approach from the original \n",
    "    paper with the sin and cosine functions. \n",
    "    \"\"\"\n",
    "    P = torch.zeros((input_tensor.shape[-1], output_dim))\n",
    "    indices = torch.arange(input_tensor.size(-1))\n",
    "    i_values = torch.arange(int(output_dim/2))\n",
    "    denominators = torch.float_power(n, 2*i_values/output_dim)\n",
    "    P[:, 0::2] = torch.sin(indices.unsqueeze(1) / denominators.unsqueeze(0)) # start at 0, step by 2 sin for even nums\n",
    "    P[:, 1::2] = torch.cos(indices.unsqueeze(1) / denominators.unsqueeze(0)) # start at 1, step by 2 cos for odd nums\n",
    "    return P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000],\n",
       "        [ 0.8415,  0.5403,  0.8415],\n",
       "        [ 0.9093, -0.4161,  0.9093],\n",
       "        [ 0.1411, -0.9900,  0.1411],\n",
       "        [-0.7568, -0.6536, -0.7568]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((2, 5))\n",
    "output_dims = 3\n",
    "positional_embedding(a, output_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(x): \n",
    "    \"\"\"\n",
    "    Simple dot product based attention\n",
    "    \"\"\"\n",
    "    query_layer, key_layer, value_layer = nn.Linear(x.shape[-1], x.shape[-1]), nn.Linear(x.shape[-1], x.shape[-1]), nn.Linear(x.shape[-1], x.shape[-1])\n",
    "    query, key, value = query_layer(x), key_layer(x), value_layer(x)\n",
    "    attention_weights  = torch.nn.Softmax(-1)(torch.tensordot(query, key, dims=1))\n",
    "    return torch.sum(value * attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1738, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1, 12)\n",
    "attention(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to emulate how it would be implemented, we write out the add norm function below. However in practice, this will be encompassed by each transformer sub module since each of them are followed by addition with residual and layer normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_norm(residual: torch.Tensor, hidden: torch.Tensor): \n",
    "    if residual.shape != hidden.shape: \n",
    "        raise ValueError(\"Shapes mismatch\")\n",
    "    else: \n",
    "        output = residual + hidden # element wise addition\n",
    "        layer_norm = nn.LayerNorm([residual.shape[-2], residual.shape[-1]])\n",
    "        return layer_norm(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.7833, 0.9884, 0.5462, 0.3524, 0.5592, 0.4115],\n",
      "         [0.1378, 0.6331, 0.4987, 0.1650, 0.2863, 0.0467],\n",
      "         [0.6652, 0.0613, 0.9855, 0.1929, 0.7510, 0.1926],\n",
      "         [0.1679, 0.4150, 0.0477, 0.0384, 0.9646, 0.1112],\n",
      "         [0.9000, 0.0346, 0.9401, 0.6464, 0.6922, 0.9834]]])\n",
      "tensor([[[0.7859, 0.7694, 0.0275, 0.7968, 0.8284, 0.9946],\n",
      "         [0.6791, 0.0083, 0.4875, 0.6828, 0.9286, 0.4735],\n",
      "         [0.6746, 0.5864, 0.0128, 0.3205, 0.0690, 0.1453],\n",
      "         [0.1133, 0.6379, 0.8655, 0.2424, 0.0102, 0.9089],\n",
      "         [0.6790, 0.6481, 0.8799, 0.7385, 0.6040, 0.0767]]])\n",
      "Final: tensor([[[ 1.3746,  1.8270, -1.0120,  0.3677,  0.9394,  0.9837],\n",
      "         [-0.4288, -0.8495, -0.0230, -0.3549,  0.5254, -1.1403],\n",
      "         [ 0.8246, -0.8344,  0.0060, -1.1566, -0.4215, -1.5772],\n",
      "         [-1.7133,  0.1368, -0.1981, -1.7144, -0.0502,  0.0582],\n",
      "         [ 1.3983, -0.7508,  1.9759,  0.9328,  0.7201,  0.1543]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# usage example: \n",
    "\n",
    "tensor_a = torch.rand([1, 5, 6]) # batch size, sequence length, embedding dimensions\n",
    "tensor_b = torch.rand([1, 5, 6])\n",
    "print(tensor_a)\n",
    "print(tensor_b)\n",
    "print(f\"Final: {add_norm(tensor_a, tensor_b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, d_k, mask = None):\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        # think of the last two dimensions of the attention tensor to be attention values for token-token pairs based on their indices (like a look up table of attention weights)\n",
    "        # the diagonal will be the attention to the current token (self). The upper diagonal will contain attention with respect to tokens in the future and the lower in the past\n",
    "        # therefore our \n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    return nn.Softmax(-1)(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d_k and d_v are essentially hyperparameters that are fixed before training. This allows for the query and keys to have the same dimensionality, and for all 3 of them to have consistent dimensionality. In many transformer implementations, d_k and d_v are set to be the same for simplicity but this is not always the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention(k, q, v, d_k, d_v, d_model, num_heads):\n",
    "    \"\"\"\n",
    "    Scaled Dot product based multi-head attention\n",
    "    \"\"\"\n",
    "    # declare projection layers - assume all inputs have d_model size in the last dimension, and project to number of heads * d_k or d_v \n",
    "    query_layer, key_layer, value_layer = nn.Linear(d_model, num_heads* d_k), nn.Linear(d_model, num_heads* d_k), nn.Linear(d_model, num_heads*d_v)\n",
    "    k_len, q_len, v_len, batch_size = k.size(1), q.size(1), v.size(1),  q.size(0)\n",
    "    residual = q\n",
    "\n",
    "    # in the following line we apply the linear projections and then reshape the outputs for multihead attention. \n",
    "    #The reshaping splits the last dimension of the linear layer's output into num_heads and d_k (or d_v for value). \n",
    "    # This creates multiple \"heads\" in the tensor, each with its own d_k (or d_v) dimension\n",
    "    k, q, v = key_layer(k).view(batch_size, k_len,  num_heads, d_k), query_layer(q).view(batch_size, q_len,  num_heads, d_k), value_layer(v).view(batch_size, v_len,  num_heads, d_v)\n",
    "    \n",
    "    # we perform the following transpose so that the num heads dimension preceeds the seq length dimension. This way, each head can capture different information about the same sequence\n",
    "    q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "    attention = scaled_dot_product_attention(q, k, d_k)\n",
    "    print(f\"attention shape: {attention.shape}, value_shape: {v.shape}\")\n",
    "    output = torch.matmul(attention, v)\n",
    "    print(output.shape)\n",
    "    # following reshaping is done so that we can add our output to the residual \n",
    "    output = output.transpose(1, 2).contiguous().view(batch_size, q_len, -1)\n",
    "    concatenated_projection = nn.Linear(num_heads * d_v, d_model, bias=False)\n",
    "\n",
    "    output = concatenated_projection(output)\n",
    "    output += residual\n",
    "\n",
    "    print(residual.shape)\n",
    "    layer_norm = nn.LayerNorm([residual.shape[-2], residual.shape[-1]])\n",
    "    output = layer_norm(output)\n",
    "\n",
    "    return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 512])\n",
      "attention shape: torch.Size([1, 4, 2, 2]), value_shape: torch.Size([1, 4, 2, 5])\n",
      "torch.Size([1, 4, 2, 5])\n",
      "torch.Size([1, 2, 512])\n",
      "torch.Size([1, 2, 512]) torch.Size([1, 4, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "\n",
    "# from the paper: To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d model \n",
    "k, q, v = torch.rand((1, 2, d_model)), torch.rand((1, 2, d_model)), torch.rand((1, 2, d_model))\n",
    "d_k, d_v = 5, 5\n",
    "num_heads = 4\n",
    "print(v.shape)\n",
    "out, attn = multihead_attention(k, q, v, d_k, d_v, d_model, num_heads)\n",
    "print(out.shape, attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module): \n",
    "    def __init__(self, d_model, d_ff, dropout) -> None:\n",
    "        super(PositionWiseFFN, self).__init__()\n",
    "        self.fc1 = nn.Sequential(nn.Linear(d_model, d_ff, bias=True),nn.ReLU())\n",
    "        self.fc2 = nn.Linear(d_ff, d_model, bias=True)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.fc2(self.fc1(x))        \n",
    "        return self.dropout(self.layer_norm(x+residual))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0000e+00, -1.9143e+00, -1.7751e-01,  ..., -7.1614e-01,\n",
       "           1.7585e+00, -2.4166e-02],\n",
       "         [-7.9474e-02, -1.4749e-03, -1.4262e+00,  ...,  9.2286e-01,\n",
       "           1.7577e+00, -9.9777e-02]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = PositionWiseFFN(d_model, 2048, 0.1)\n",
    "x = torch.rand((1, 2, d_model))\n",
    "ffn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've implemented the lowest level building blocks of the transormer, below we put them together to build transformer blocks, encoder and decoder layers, and the complete transformer architecture. Now we try and condense everything to a more concise-less experimental implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module): \n",
    "    def __init__(self, d_k, d_model, d_v, dropout, num_heads) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_k, self.d_v, self.d_model, self.num_heads = d_k, d_v, d_model, num_heads\n",
    "        self.query_layer, self.key_layer, self.value_layer = nn.Linear(d_model, num_heads* d_k), nn.Linear(d_model, num_heads* d_k), nn.Linear(d_model, num_heads*d_v)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.concat_projection = nn.Linear(num_heads*d_v, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask = None):\n",
    "        k_len, q_len, v_len, batch_size = k.size(1), q.size(1), v.size(1),  q.size(0)\n",
    "        residual = q\n",
    "        k, q, v = self.key_layer(k).view(batch_size, k_len,  self.num_heads, self.d_k), self.query_layer(q).view(batch_size, q_len,  self.num_heads, self.d_k), self.value_layer(v).view(batch_size, v_len,  self.num_heads, self.d_v)\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "        attention = scaled_dot_product_attention(q, k, self.d_k, mask)\n",
    "        output = torch.matmul(attention, v)\n",
    "        output = self.concat_projection(output.transpose(1, 2).contiguous().view(batch_size, q_len, -1))\n",
    "        return self.dropout(self.layer_norm(output+residual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module): \n",
    "    def __init__(self, d_k, d_model, d_v, num_heads, d_ff, dropout) -> None:\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.k_layer, self.q_layer, self.v_layer = nn.Linear(d_model, d_model), nn.Linear(d_model, d_model), nn.Linear(d_model, d_model)\n",
    "        self.multihead_attention = MultiHeadAttention(d_k, d_model, d_v, dropout, num_heads)\n",
    "        self.pointwise_ffn = PositionWiseFFN(d_model, d_ff, dropout)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        k, q, v = self.k_layer(x), self.q_layer(x), self.v_layer(x)\n",
    "        output = self.multihead_attention(q, k, v)\n",
    "        return self.pointwise_ffn(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following encoder implementation is based off of the block diagram from Attention Is All You Need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_k, d_model, d_v, d_ff, num_heads, num_layers, vocab_size, dropout=0.1) -> None:\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.positional_embedding = positional_embedding\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layers = [EncoderLayer(d_k, d_model, d_v, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        x = self.dropout(embedded + self.positional_embedding(x, self.d_model).to(x.device))\n",
    "        for layer in self.layers:\n",
    "            layer = layer.to(x.device)\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary testing on text classification task (encoder only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTransformer(nn.Module): \n",
    "    def __init__(self, d_k, d_model, d_v, d_ff, num_heads, num_layers, num_classes, vocab_size, dropout=0.1) -> None:\n",
    "        super(ClassificationTransformer, self).__init__()\n",
    "        self.encoder_only_transformer = Encoder(d_k, d_model, d_v, d_ff, num_heads, num_layers, vocab_size, dropout=0.1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(d_model, d_model)\n",
    "        self.fc2 = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.encoder_only_transformer(x)\n",
    "        avg_pool = torch.mean(out, dim=-2)\n",
    "        return self.fc2(self.dropout(self.fc1(self.dropout(avg_pool))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "# Convert to list to enable random splitting\n",
    "train_dataset = list(train_iter)\n",
    "\n",
    "#80-20 train-val split \n",
    "train_size = int(len(train_dataset) * 0.8)  \n",
    "val_size = len(train_dataset) - train_size  \n",
    "train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "VOCAB_SIZE = 5000\n",
    "\n",
    "# Build vocab based on the train_data\n",
    "train_data_iter = (text for _, text in train_data)\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_data_iter), specials=[\"<unk>\"], max_tokens=VOCAB_SIZE)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "def collate_batch(batch):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    \n",
    "    # Sort the batch in the descending order\n",
    "    batch.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "        \n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.int64)\n",
    "    \n",
    "    # Pad sequences\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "    \n",
    "    return label_list.to(device), text_list.to(device), lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size = 8, shuffle = True, collate_fn = collate_batch)\n",
    "val_loader = DataLoader(val_data, batch_size = 8, shuffle = False, collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-4\n",
    "NUM_EPOCHS = 50\n",
    "DROPOUT = 0.5\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "D_K = 128\n",
    "D_V = 128\n",
    "D_FF = 512\n",
    "D_MODEL = 256\n",
    "NUM_LAYERS = 2\n",
    "OUTPUT_DIM = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationTransformer(D_K, D_MODEL, D_V, D_FF, num_heads=3, num_classes=OUTPUT_DIM, num_layers=2, vocab_size=VOCAB_SIZE)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, loss_function, optim, epochs, device):\n",
    "    losses = [] #group losses for loss visualization \n",
    "    running_loss = 0.0\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        print(\"Epoch %d / %d\" % (epoch+1, epochs))\n",
    "        print(\"-\"*10)\n",
    "    \n",
    "        for i, batch_data in enumerate(train_loader):\n",
    "            (y, x, x_size) = batch_data\n",
    "            logits = model(x)\n",
    "            loss = loss_function(logits, y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            running_loss += loss.item()\n",
    "            losses.append(loss)\n",
    "\n",
    "            if (i+1) % 1000 == 0:\n",
    "                print(\"Step: {}, average training loss over last 2000 steps: {:.4f}\".format(i+1, running_loss/1000))\n",
    "                running_loss = 0.0\n",
    "            \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            correct_pred = 0.0\n",
    "            num_samples = 0\n",
    "            for i, batch_data in enumerate(val_loader):\n",
    "                (y, x, x_size) = batch_data\n",
    "                y, x, x_size = y.to(device), x.to(device), x_size.to(device)\n",
    "                logits = model(x)\n",
    "                loss = loss_function(logits, y)\n",
    "                _, predicted_labels = torch.max(logits, 1)\n",
    "                correct_pred += (predicted_labels.long() == y.long()).sum()\n",
    "                num_samples+=predicted_labels.shape[0]\n",
    "                val_loss += loss.item()\n",
    "            \n",
    "            val_accuracy = (correct_pred / num_samples) * 100\n",
    "            val_losses.append(val_loss)\n",
    "        print(\"Epoch: {}, validation loss: {:.4f}, val accuracy: {:.2f}\".format(epoch+1, val_loss/len(val_loader), val_accuracy))\n",
    "    \n",
    "    return losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aryaman.pandya/ml_accel/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 1.2368\n",
      "Step: 2000, average training loss over last 2000 steps: 0.8032\n",
      "Step: 3000, average training loss over last 2000 steps: 0.6700\n",
      "Step: 4000, average training loss over last 2000 steps: 0.5944\n",
      "Step: 5000, average training loss over last 2000 steps: 0.5584\n",
      "Step: 6000, average training loss over last 2000 steps: 0.5351\n",
      "Step: 7000, average training loss over last 2000 steps: 0.4960\n",
      "Step: 8000, average training loss over last 2000 steps: 0.4939\n",
      "Step: 9000, average training loss over last 2000 steps: 0.4759\n",
      "Step: 10000, average training loss over last 2000 steps: 0.4260\n",
      "Step: 11000, average training loss over last 2000 steps: 0.4329\n",
      "Step: 12000, average training loss over last 2000 steps: 0.4271\n",
      "Epoch: 1, validation loss: 0.3704, val accuracy: 87.67\n",
      "Epoch 2 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.4143\n",
      "Step: 2000, average training loss over last 2000 steps: 0.3966\n",
      "Step: 3000, average training loss over last 2000 steps: 0.3806\n",
      "Step: 4000, average training loss over last 2000 steps: 0.3912\n",
      "Step: 5000, average training loss over last 2000 steps: 0.3856\n",
      "Step: 6000, average training loss over last 2000 steps: 0.3651\n",
      "Step: 7000, average training loss over last 2000 steps: 0.3651\n",
      "Step: 8000, average training loss over last 2000 steps: 0.3590\n",
      "Step: 9000, average training loss over last 2000 steps: 0.3750\n",
      "Step: 10000, average training loss over last 2000 steps: 0.3716\n",
      "Step: 11000, average training loss over last 2000 steps: 0.3387\n",
      "Step: 12000, average training loss over last 2000 steps: 0.3539\n",
      "Epoch: 2, validation loss: 0.3450, val accuracy: 88.41\n",
      "Epoch 3 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.3177\n",
      "Step: 2000, average training loss over last 2000 steps: 0.3340\n",
      "Step: 3000, average training loss over last 2000 steps: 0.3321\n",
      "Step: 4000, average training loss over last 2000 steps: 0.3426\n",
      "Step: 5000, average training loss over last 2000 steps: 0.3482\n",
      "Step: 6000, average training loss over last 2000 steps: 0.3233\n",
      "Step: 7000, average training loss over last 2000 steps: 0.3192\n",
      "Step: 8000, average training loss over last 2000 steps: 0.3272\n",
      "Step: 9000, average training loss over last 2000 steps: 0.3278\n",
      "Step: 10000, average training loss over last 2000 steps: 0.3248\n",
      "Step: 11000, average training loss over last 2000 steps: 0.3327\n",
      "Step: 12000, average training loss over last 2000 steps: 0.3135\n",
      "Epoch: 3, validation loss: 0.3195, val accuracy: 89.52\n",
      "Epoch 4 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.3046\n",
      "Step: 2000, average training loss over last 2000 steps: 0.3099\n",
      "Step: 3000, average training loss over last 2000 steps: 0.3242\n",
      "Step: 4000, average training loss over last 2000 steps: 0.3201\n",
      "Step: 5000, average training loss over last 2000 steps: 0.3061\n",
      "Step: 6000, average training loss over last 2000 steps: 0.3087\n",
      "Step: 7000, average training loss over last 2000 steps: 0.3043\n",
      "Step: 8000, average training loss over last 2000 steps: 0.3013\n",
      "Step: 9000, average training loss over last 2000 steps: 0.3018\n",
      "Step: 10000, average training loss over last 2000 steps: 0.3011\n",
      "Step: 11000, average training loss over last 2000 steps: 0.3052\n",
      "Step: 12000, average training loss over last 2000 steps: 0.3121\n",
      "Epoch: 4, validation loss: 0.3142, val accuracy: 89.73\n",
      "Epoch 5 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2968\n",
      "Step: 2000, average training loss over last 2000 steps: 0.3081\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2835\n",
      "Step: 4000, average training loss over last 2000 steps: 0.3018\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2956\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2947\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2943\n",
      "Step: 8000, average training loss over last 2000 steps: 0.3028\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2912\n",
      "Step: 10000, average training loss over last 2000 steps: 0.3004\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2902\n",
      "Step: 12000, average training loss over last 2000 steps: 0.3037\n",
      "Epoch: 5, validation loss: 0.3061, val accuracy: 89.69\n",
      "Epoch 6 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2804\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2771\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2655\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2810\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2915\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2838\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2872\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2719\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2828\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2982\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2991\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2885\n",
      "Epoch: 6, validation loss: 0.3048, val accuracy: 89.68\n",
      "Epoch 7 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2650\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2907\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2709\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2772\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2705\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2660\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2843\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2845\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2863\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2939\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2739\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2825\n",
      "Epoch: 7, validation loss: 0.3400, val accuracy: 88.86\n",
      "Epoch 8 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2743\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2677\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2694\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2729\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2709\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2598\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2681\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2743\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2799\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2768\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2704\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2821\n",
      "Epoch: 8, validation loss: 0.2962, val accuracy: 90.10\n",
      "Epoch 9 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2551\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2572\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2651\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2766\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2803\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2681\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2661\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2757\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2685\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2712\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2592\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2839\n",
      "Epoch: 9, validation loss: 0.3093, val accuracy: 89.41\n",
      "Epoch 10 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2630\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2745\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2566\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2651\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2778\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2594\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2698\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2559\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2665\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2673\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2776\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2643\n",
      "Epoch: 10, validation loss: 0.3009, val accuracy: 89.77\n",
      "Epoch 11 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2444\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2719\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2500\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2706\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2630\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2551\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2610\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2618\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2644\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2536\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2729\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2716\n",
      "Epoch: 11, validation loss: 0.3037, val accuracy: 89.74\n",
      "Epoch 12 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2572\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2552\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2522\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2593\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2567\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2715\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2669\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2600\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2678\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2624\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2587\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2559\n",
      "Epoch: 12, validation loss: 0.2979, val accuracy: 90.44\n",
      "Epoch 13 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2569\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2452\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2633\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2622\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2418\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2544\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2734\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2584\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2822\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2389\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2640\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2738\n",
      "Epoch: 13, validation loss: 0.3176, val accuracy: 89.57\n",
      "Epoch 14 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2561\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2400\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2588\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2557\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2540\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2564\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2552\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2625\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2600\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2573\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2615\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2711\n",
      "Epoch: 14, validation loss: 0.3032, val accuracy: 89.76\n",
      "Epoch 15 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2460\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2471\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2611\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2361\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2508\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2630\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2495\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2544\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2617\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2608\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2664\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2662\n",
      "Epoch: 15, validation loss: 0.3057, val accuracy: 89.99\n",
      "Epoch 16 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2497\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2481\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2508\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2415\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2558\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2556\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2501\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2556\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2607\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2719\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2431\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2566\n",
      "Epoch: 16, validation loss: 0.3007, val accuracy: 90.14\n",
      "Epoch 17 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2468\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2434\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2330\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2561\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2602\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2584\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2564\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2598\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2442\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2641\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2404\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2607\n",
      "Epoch: 17, validation loss: 0.2952, val accuracy: 90.17\n",
      "Epoch 18 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2460\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2454\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2486\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2536\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2583\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2551\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2515\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2476\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2424\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2588\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2565\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2462\n",
      "Epoch: 18, validation loss: 0.3098, val accuracy: 89.83\n",
      "Epoch 19 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2412\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2493\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2496\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2484\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2533\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2583\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2516\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2558\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2389\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2617\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2506\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2559\n",
      "Epoch: 19, validation loss: 0.3002, val accuracy: 90.10\n",
      "Epoch 20 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2289\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2486\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2557\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2540\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2581\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2494\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2486\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2402\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2474\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2365\n",
      "Step: 11000, average training loss over last 2000 steps: 0.2752\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2422\n",
      "Epoch: 20, validation loss: 0.3078, val accuracy: 90.13\n",
      "Epoch 21 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2406\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = train(model, train_loader, val_loader, torch.nn.functional.cross_entropy, optimizer, NUM_EPOCHS, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do I want to see from this training? \n",
    "\n",
    "- I want to see how long it takes for this to converge \n",
    "- I want to see the computational complexity (how long per epoch?)\n",
    "    - anywhere from 60 seconds an epoch to 3 minutes an epoch depending on hyperparameters \n",
    "    - With the low\n",
    "- How does the model accuracy respond to changes in hyperparams? Not just LR and the usuals, but also d_model, d_k, d_v, d_ff\n",
    "    - This is intuitive, higher values for these hyper params work better (until a point of diminishing return) since there's a balance between trying to capture all the intricacies of the data in high dimensional vector space and the actual complexity of the data to begin with. (as well as computational constraints) - doubling these dimensions led to more batch efficient learning but at the same time increased runtime by a proportional 2x \n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts so far: \n",
    "\n",
    "This architecture is way more compute intensive but at the same time so much more batch efficient than previously explored ones when tuned with the right hyperparameters (LSTMs, RNNs even when bidirectional + multi layered and so on) that we see near 90% validation accuracy after just one or two epochs. These epochs, however, take tremendously long (7 minutes per epoch v/s the 1 minute LSTMs were taking). Part of this could be becauase of the implementation being from scratch (torch.transformer might be faster), but most of this comes from the computational load the multiple blocks (and their individual complexities)\n",
    "\n",
    "Now that we have the encoder and were able to build something useful off of it and ensure its functionality, we should tweak our multihead attention to enable some form of masking and then build a decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Now, we build the decoder and train and test it for some task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_k, d_model, d_v, num_heads, d_ff, dropout) -> None:\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.k_layer, self.q_layer, self.v_layer = nn.Linear(d_model, d_model), nn.Linear(d_model, d_model), nn.Linear(d_model, d_model)\n",
    "        self.k_layer2, self.q_layer2, self.v_layer2 = nn.Linear(d_model, d_model), nn.Linear(d_model, d_model), nn.Linear(d_model, d_model)\n",
    "        self.masked_multihead_attention = MultiHeadAttention(d_k, d_model, d_v, dropout, num_heads)\n",
    "        self.multihead_attention = MultiHeadAttention(d_k, d_model, d_v, dropout, num_heads)\n",
    "        self.positiontwise_ffn = PositionWiseFFN(d_model, d_ff, dropout)\n",
    "    \n",
    "    def forward(self, outputs, encoder_output):\n",
    "        k1, q1, v1 = self.k_layer(outputs), self.q_layer(outputs), self.v_layer(outputs)\n",
    "        outputs = self.masked_multihead_attention(q1, k1, v1, True)\n",
    "        k2, q2, v2 = self.k_layer2(encoder_output), self.q_layer2(outputs), self.v_layer2(encoder_output)  # query comes from outputs of previous decoder layer while k, v come from the encoder's output\n",
    "        outputs = self.multihead_attention(q2, k2, v2)\n",
    "        return self.positiontwise_ffn(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_k, d_model, d_v, d_ff, num_heads, num_layers, vocab_size, out_dims, dropout=0.1) -> None:\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.positional_embedding = positional_embedding\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layers = [DecoderLayer(d_k, d_model, d_v, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        self.fc = nn.Linear(d_model, out_dims)\n",
    "    \n",
    "    def forward(self, x, encoder_output):\n",
    "        embedded = self.embedding(x)\n",
    "        x = self.dropout(embedded + self.positional_embedding(x, self.d_model).to(x.device))\n",
    "        for layer in self.layers:\n",
    "            layer = layer.to(x.device)\n",
    "            x = layer(x, encoder_output)\n",
    "        return self.fc(self.dropout(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_accel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
