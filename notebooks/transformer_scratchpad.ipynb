{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers 101\n",
    "\n",
    "This notebook serves as an exploration of the transformer architecture (Vaswani et. al.) Here, we'll implement in native PyTorch the basic building blocks of the transformer and then put them all together so we have a model architecture to put into `../models`\n",
    "\n",
    "In the process of putting this together (much like my other exploratory projects) I tried to limit viewing existing code online, and primarily used my notes (pdf attached for anyone interested) as a foundation for this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "In the first half of the notebook we'll develop all the pieces needed to implement the transformer encoder, then train and test it on some task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want something with output dims: (sequence_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_embedding(input_tensor: torch.Tensor, output_dim: int, n=10000): \n",
    "    \"\"\"\n",
    "    Here, we implement the naive approach from the original \n",
    "    paper with the sin and cosine functions. \n",
    "    \"\"\"\n",
    "    P = torch.zeros((input_tensor.shape[-1], output_dim))\n",
    "    indices = torch.arange(input_tensor.size(-1))\n",
    "    i_values = torch.arange(int(output_dim/2))\n",
    "    denominators = torch.float_power(n, 2*i_values/output_dim)\n",
    "    P[:, 0::2] = torch.sin(indices.unsqueeze(1) / denominators.unsqueeze(0)) # start at 0, step by 2 sin for even nums\n",
    "    P[:, 1::2] = torch.cos(indices.unsqueeze(1) / denominators.unsqueeze(0)) # start at 1, step by 2 cos for odd nums\n",
    "    return P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000],\n",
       "        [ 0.8415,  0.5403,  0.8415],\n",
       "        [ 0.9093, -0.4161,  0.9093],\n",
       "        [ 0.1411, -0.9900,  0.1411],\n",
       "        [-0.7568, -0.6536, -0.7568]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((2, 5))\n",
    "output_dims = 3\n",
    "positional_embedding(a, output_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(x): \n",
    "    \"\"\"\n",
    "    Simple dot product based attention\n",
    "    \"\"\"\n",
    "    query_layer, key_layer, value_layer = nn.Linear(x.shape[-1], x.shape[-1]), nn.Linear(x.shape[-1], x.shape[-1]), nn.Linear(x.shape[-1], x.shape[-1])\n",
    "    query, key, value = query_layer(x), key_layer(x), value_layer(x)\n",
    "    attention_weights  = torch.nn.Softmax(-1)(torch.tensordot(query, key, dims=1))\n",
    "    return torch.sum(value * attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1158, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1, 12)\n",
    "attention(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to emulate how it would be implemented, we write out the add norm function below. However in practice, this will be encompassed by each transformer sub module since each of them are followed by addition with residual and layer normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_norm(residual: torch.Tensor, hidden: torch.Tensor): \n",
    "    if residual.shape != hidden.shape: \n",
    "        raise ValueError(\"Shapes mismatch\")\n",
    "    else: \n",
    "        output = residual + hidden # element wise addition\n",
    "        layer_norm = nn.LayerNorm([residual.shape[-2], residual.shape[-1]])\n",
    "        return layer_norm(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.7958, 0.4520, 0.4050, 0.9265, 0.7730, 0.0397],\n",
      "         [0.6503, 0.8041, 0.2931, 0.2508, 0.3734, 0.3316],\n",
      "         [0.2894, 0.1084, 0.2821, 0.8636, 0.9661, 0.8210],\n",
      "         [0.7355, 0.9744, 0.5085, 0.9770, 0.6735, 0.8006],\n",
      "         [0.6041, 0.9980, 0.2944, 0.1588, 0.9294, 0.0419]]])\n",
      "tensor([[[0.9855, 0.8248, 0.9370, 0.0482, 0.4321, 0.6577],\n",
      "         [0.6149, 0.9740, 0.9499, 0.9845, 0.0657, 0.2232],\n",
      "         [0.5183, 0.6081, 0.8234, 0.5644, 0.5225, 0.3515],\n",
      "         [0.9325, 0.8321, 0.4569, 0.0563, 0.7681, 0.3281],\n",
      "         [0.9918, 0.1495, 0.6933, 0.4767, 0.4997, 0.3312]]])\n",
      "Final: tensor([[[ 1.6153,  0.3089,  0.4777, -0.4732,  0.1234, -1.1910],\n",
      "         [ 0.2791,  1.6069,  0.2216,  0.2017, -1.8600, -1.5603],\n",
      "         [-0.9057, -1.1418, -0.1347,  0.7003,  0.8574,  0.0388],\n",
      "         [ 1.3216,  1.6804, -0.4972, -0.3215,  0.7356, -0.0745],\n",
      "         [ 1.1351, -0.0258, -0.4394, -1.3513,  0.7033, -2.0308]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# usage example: \n",
    "\n",
    "tensor_a = torch.rand([1, 5, 6]) # batch size, sequence length, embedding dimensions\n",
    "tensor_b = torch.rand([1, 5, 6])\n",
    "print(tensor_a)\n",
    "print(tensor_b)\n",
    "print(f\"Final: {add_norm(tensor_a, tensor_b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, d_k, mask):\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    return nn.Softmax(-1)(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d_k and d_v are essentially hyperparameters that are fixed before training. This allows for the query and keys to have the same dimensionality, and for all 3 of them to have consistent dimensionality. In many transformer implementations, d_k and d_v are set to be the same for simplicity but this is not always the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention(k, q, v, d_k, d_v, d_model, num_heads):\n",
    "    \"\"\"\n",
    "    Scaled Dot product based multi-head attention\n",
    "    \"\"\"\n",
    "    # declare projection layers - assume all inputs have d_model size in the last dimension, and project to number of heads * d_k or d_v \n",
    "    query_layer, key_layer, value_layer = nn.Linear(d_model, num_heads* d_k), nn.Linear(d_model, num_heads* d_k), nn.Linear(d_model, num_heads*d_v)\n",
    "    k_len, q_len, v_len, batch_size = k.size(1), q.size(1), v.size(1),  q.size(0)\n",
    "    residual = q\n",
    "\n",
    "    # in the following line we apply the linear projections and then reshape the outputs for multihead attention. \n",
    "    #The reshaping splits the last dimension of the linear layer's output into num_heads and d_k (or d_v for value). \n",
    "    # This creates multiple \"heads\" in the tensor, each with its own d_k (or d_v) dimension\n",
    "    k, q, v = key_layer(k).view(batch_size, k_len,  num_heads, d_k), query_layer(q).view(batch_size, q_len,  num_heads, d_k), value_layer(v).view(batch_size, v_len,  num_heads, d_v)\n",
    "    \n",
    "    # we perform the following transpose so that the num heads dimension preceeds the seq length dimension. This way, each head can capture different information about the same sequence\n",
    "    q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "    attention = scaled_dot_product_attention(q, k, d_k)\n",
    "    output = torch.matmul(attention, v)\n",
    "\n",
    "    # following reshaping is done so that we can add our output to the residual \n",
    "    output = output.transpose(1, 2).contiguous().view(batch_size, q_len, -1)\n",
    "    concatenated_projection = nn.Linear(num_heads * d_v, d_model, bias=False)\n",
    "\n",
    "    output = concatenated_projection(output)\n",
    "    output += residual\n",
    "\n",
    "    print(residual.shape)\n",
    "    layer_norm = nn.LayerNorm([residual.shape[-2], residual.shape[-1]])\n",
    "    output = layer_norm(output)\n",
    "\n",
    "    return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 512])\n",
      "torch.Size([1, 2, 512]) torch.Size([1, 4, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "\n",
    "# from the paper: To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d model \n",
    "k, q, v = torch.rand((1, 2, d_model)), torch.rand((1, 2, d_model)), torch.rand((1, 2, d_model))\n",
    "d_k, d_v = 5, 5\n",
    "num_heads = 4\n",
    "\n",
    "out, attn = multihead_attention(k, q, v, d_k, d_v, d_model, num_heads)\n",
    "print(out.shape, attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module): \n",
    "    def __init__(self, d_model, d_ff, dropout) -> None:\n",
    "        super(PositionWiseFFN, self).__init__()\n",
    "        self.fc1 = nn.Sequential(nn.Linear(d_model, d_ff, bias=True),nn.ReLU())\n",
    "        self.fc2 = nn.Linear(d_ff, d_model, bias=True)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.fc2(self.fc1(x))        \n",
    "        return self.dropout(self.layer_norm(x+residual))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0040, -0.6596, -1.0033,  ..., -1.1891, -1.5098,  0.2274],\n",
       "         [-1.8384, -0.6719,  0.6867,  ...,  0.5574, -1.3400,  1.0287]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = PositionWiseFFN(d_model, 2048, 0.1)\n",
    "x = torch.rand((1, 2, d_model))\n",
    "ffn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've implemented the lowest level building blocks of the transormer, below we put them together to build transformer blocks, encoder and decoder layers, and the complete transformer architecture. Now we try and condense everything to a more concise-less experimental implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module): \n",
    "    def __init__(self, d_k, d_model, d_v, dropout, num_heads) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_k, self.d_v, self.d_model, self.num_heads = d_k, d_v, d_model, num_heads\n",
    "        self.query_layer, self.key_layer, self.value_layer = nn.Linear(d_model, num_heads* d_k), nn.Linear(d_model, num_heads* d_k), nn.Linear(d_model, num_heads*d_v)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.concat_projection = nn.Linear(num_heads*d_v, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask = None):\n",
    "        k_len, q_len, v_len, batch_size = k.size(1), q.size(1), v.size(1),  q.size(0)\n",
    "        residual = q\n",
    "        k, q, v = self.key_layer(k).view(batch_size, k_len,  self.num_heads, self.d_k), self.query_layer(q).view(batch_size, q_len,  self.num_heads, self.d_k), self.value_layer(v).view(batch_size, v_len,  self.num_heads, self.d_v)\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "        attention = scaled_dot_product_attention(q, k, self.d_k, mask)\n",
    "        output = torch.matmul(attention, v)\n",
    "        output = self.concat_projection(output.transpose(1, 2).contiguous().view(batch_size, q_len, -1))\n",
    "        return self.dropout(self.layer_norm(output+residual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module): \n",
    "    def __init__(self, d_k, d_model, d_v, num_heads, d_ff, dropout) -> None:\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.k_layer, self.q_layer, self.v_layer = nn.Linear(d_model, d_model), nn.Linear(d_model, d_model), nn.Linear(d_model, d_model)\n",
    "        self.multihead_attention = MultiHeadAttention(d_k, d_model, d_v, dropout, num_heads)\n",
    "        self.pointwise_ffn = PositionWiseFFN(d_model, d_ff, dropout)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        k, q, v = self.k_layer(x), self.q_layer(x), self.v_layer(x)\n",
    "        output = self.multihead_attention(q, k, v)\n",
    "        return self.pointwise_ffn(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following encoder implementation is based off of the block diagram from Attention Is All You Need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_k, d_model, d_v, d_ff, num_heads, num_layers, vocab_size, dropout=0.1) -> None:\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.positional_embedding = positional_embedding\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layers = [EncoderLayer(d_k, d_model, d_v, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        x = self.dropout(embedded + self.positional_embedding(x, self.d_model))\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary testing on text classification task (encoder only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTransformer(nn.Module): \n",
    "    def __init__(self, d_k, d_model, d_v, d_ff, num_heads, num_layers, num_classes, vocab_size, dropout=0.1) -> None:\n",
    "        super(ClassificationTransformer, self).__init__()\n",
    "        self.encoder_only_transformer = Encoder(d_k, d_model, d_v, d_ff, num_heads, num_layers, vocab_size, dropout=0.1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(d_model, d_model)\n",
    "        self.fc2 = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.encoder_only_transformer(x)\n",
    "        avg_pool = torch.mean(out, dim=-2)\n",
    "        return self.fc2(self.dropout(self.fc1(self.dropout(avg_pool))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "# Convert to list to enable random splitting\n",
    "train_dataset = list(train_iter)\n",
    "\n",
    "#80-20 train-val split \n",
    "train_size = int(len(train_dataset) * 0.8)  \n",
    "val_size = len(train_dataset) - train_size  \n",
    "train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "VOCAB_SIZE = 5000\n",
    "\n",
    "# Build vocab based on the train_data\n",
    "train_data_iter = (text for _, text in train_data)\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_data_iter), specials=[\"<unk>\"], max_tokens=VOCAB_SIZE)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "def collate_batch(batch):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    \n",
    "    # Sort the batch in the descending order\n",
    "    batch.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "        \n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.int64)\n",
    "    \n",
    "    # Pad sequences\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "    \n",
    "    return label_list.to(device), text_list.to(device), lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size = 8, shuffle = True, collate_fn = collate_batch)\n",
    "val_loader = DataLoader(val_data, batch_size = 8, shuffle = False, collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-4\n",
    "NUM_EPOCHS = 50\n",
    "DROPOUT = 0.3\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "D_K = 128\n",
    "D_V = 128\n",
    "D_FF = 512\n",
    "D_MODEL = 256\n",
    "NUM_LAYERS = 2\n",
    "OUTPUT_DIM = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationTransformer(D_K, D_MODEL, D_V, D_FF, num_heads=4, num_classes=OUTPUT_DIM, num_layers=2, vocab_size=VOCAB_SIZE)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, loss_function, optim, epochs, device):\n",
    "    losses = [] #group losses for loss visualization \n",
    "    running_loss = 0.0\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        print(\"Epoch %d / %d\" % (epoch+1, epochs))\n",
    "        print(\"-\"*10)\n",
    "    \n",
    "        for i, batch_data in enumerate(train_loader):\n",
    "            (y, x, x_size) = batch_data\n",
    "            logits = model(x)\n",
    "            loss = loss_function(logits, y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            running_loss += loss.item()\n",
    "            losses.append(loss)\n",
    "\n",
    "            if (i+1) % 1000 == 0:\n",
    "                print(\"Step: {}, average training loss over last 2000 steps: {:.4f}\".format(i+1, running_loss/1000))\n",
    "                running_loss = 0.0\n",
    "            \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            correct_pred = 0.0\n",
    "            num_samples = 0\n",
    "            for i, batch_data in enumerate(val_loader):\n",
    "                (y, x, x_size) = batch_data\n",
    "                y, x, x_size = y.to(device), x.to(device), x_size.to(device)\n",
    "                logits = model(x)\n",
    "                loss = loss_function(logits, y)\n",
    "                _, predicted_labels = torch.max(logits, 1)\n",
    "                correct_pred += (predicted_labels.long() == y.long()).sum()\n",
    "                num_samples+=predicted_labels.shape[0]\n",
    "                val_loss += loss.item()\n",
    "            \n",
    "            val_accuracy = (correct_pred / num_samples) * 100\n",
    "            val_losses.append(val_loss)\n",
    "        print(\"Epoch: {}, validation loss: {:.4f}, val accuracy: {:.2f}\".format(epoch+1, val_loss/len(val_loader), val_accuracy))\n",
    "    \n",
    "    return losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 1.2433\n",
      "Step: 2000, average training loss over last 2000 steps: 0.8219\n",
      "Step: 3000, average training loss over last 2000 steps: 0.6680\n",
      "Step: 4000, average training loss over last 2000 steps: 0.5879\n",
      "Step: 5000, average training loss over last 2000 steps: 0.5594\n",
      "Step: 6000, average training loss over last 2000 steps: 0.5212\n",
      "Step: 7000, average training loss over last 2000 steps: 0.5074\n",
      "Step: 8000, average training loss over last 2000 steps: 0.4896\n",
      "Step: 9000, average training loss over last 2000 steps: 0.4554\n",
      "Step: 10000, average training loss over last 2000 steps: 0.4656\n",
      "Step: 11000, average training loss over last 2000 steps: 0.4156\n",
      "Step: 12000, average training loss over last 2000 steps: 0.4386\n",
      "Epoch: 1, validation loss: 0.3730, val accuracy: 87.60\n",
      "Epoch 2 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.3983\n",
      "Step: 2000, average training loss over last 2000 steps: 0.3956\n",
      "Step: 3000, average training loss over last 2000 steps: 0.3784\n",
      "Step: 4000, average training loss over last 2000 steps: 0.3798\n",
      "Step: 5000, average training loss over last 2000 steps: 0.3922\n",
      "Step: 6000, average training loss over last 2000 steps: 0.3831\n",
      "Step: 7000, average training loss over last 2000 steps: 0.3655\n",
      "Step: 8000, average training loss over last 2000 steps: 0.3765\n",
      "Step: 9000, average training loss over last 2000 steps: 0.3669\n",
      "Step: 10000, average training loss over last 2000 steps: 0.3527\n",
      "Step: 11000, average training loss over last 2000 steps: 0.3543\n",
      "Step: 12000, average training loss over last 2000 steps: 0.3568\n",
      "Epoch: 2, validation loss: 0.3366, val accuracy: 88.97\n",
      "Epoch 3 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.3389\n",
      "Step: 2000, average training loss over last 2000 steps: 0.3218\n",
      "Step: 3000, average training loss over last 2000 steps: 0.3404\n",
      "Step: 4000, average training loss over last 2000 steps: 0.3358\n",
      "Step: 5000, average training loss over last 2000 steps: 0.3260\n",
      "Step: 6000, average training loss over last 2000 steps: 0.3302\n",
      "Step: 7000, average training loss over last 2000 steps: 0.3350\n",
      "Step: 8000, average training loss over last 2000 steps: 0.3368\n",
      "Step: 9000, average training loss over last 2000 steps: 0.3227\n",
      "Step: 10000, average training loss over last 2000 steps: 0.3384\n",
      "Step: 11000, average training loss over last 2000 steps: 0.3211\n",
      "Step: 12000, average training loss over last 2000 steps: 0.3178\n",
      "Epoch: 3, validation loss: 0.3148, val accuracy: 89.52\n",
      "Epoch 4 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.3137\n",
      "Step: 2000, average training loss over last 2000 steps: 0.3218\n",
      "Step: 3000, average training loss over last 2000 steps: 0.3239\n",
      "Step: 4000, average training loss over last 2000 steps: 0.3113\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2932\n",
      "Step: 6000, average training loss over last 2000 steps: 0.3152\n",
      "Step: 7000, average training loss over last 2000 steps: 0.3018\n",
      "Step: 8000, average training loss over last 2000 steps: 0.3030\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2952\n",
      "Step: 10000, average training loss over last 2000 steps: 0.3122\n",
      "Step: 11000, average training loss over last 2000 steps: 0.3081\n",
      "Step: 12000, average training loss over last 2000 steps: 0.3107\n",
      "Epoch: 4, validation loss: 0.3278, val accuracy: 88.94\n",
      "Epoch 5 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.3019\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2886\n",
      "Step: 3000, average training loss over last 2000 steps: 0.3016\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2997\n",
      "Step: 5000, average training loss over last 2000 steps: 0.3133\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2870\n",
      "Step: 7000, average training loss over last 2000 steps: 0.3016\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2968\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2868\n",
      "Step: 10000, average training loss over last 2000 steps: 0.2871\n",
      "Step: 11000, average training loss over last 2000 steps: 0.3063\n",
      "Step: 12000, average training loss over last 2000 steps: 0.2985\n",
      "Epoch: 5, validation loss: 0.3090, val accuracy: 89.88\n",
      "Epoch 6 / 50\n",
      "----------\n",
      "Step: 1000, average training loss over last 2000 steps: 0.2750\n",
      "Step: 2000, average training loss over last 2000 steps: 0.2796\n",
      "Step: 3000, average training loss over last 2000 steps: 0.2719\n",
      "Step: 4000, average training loss over last 2000 steps: 0.2995\n",
      "Step: 5000, average training loss over last 2000 steps: 0.2723\n",
      "Step: 6000, average training loss over last 2000 steps: 0.2941\n",
      "Step: 7000, average training loss over last 2000 steps: 0.2937\n",
      "Step: 8000, average training loss over last 2000 steps: 0.2911\n",
      "Step: 9000, average training loss over last 2000 steps: 0.2860\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = train(model, train_loader, val_loader, torch.nn.functional.cross_entropy, optimizer, NUM_EPOCHS, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do I want to see from this training? \n",
    "\n",
    "- I want to see how long it took for this to converge \n",
    "- I want to see the computational complexity (how long per epoch?)\n",
    "- How does the model accuracy respond to changes in hyperparams? Not just LR and the usuals, but also d_model, d_k, d_v, d_ff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts so far: \n",
    "\n",
    "This architecture is way more compute intensive but at the same time so much more batch efficient than previously explored ones (LSTMs, RNNs even when bidirectional + multi layered and so on) that we see near 90% validation accuracy after just one or two epochs. These epochs, however, take tremendously long (7 minutes per epoch v/s the 1 minute LSTMs were taking). Part of this could be becauase of the implementation being from scratch (torch.transformer might be faster), but most of this comes from the computational load the multiple blocks (and their individual complexities)\n",
    "\n",
    "Now that we have the encoder and were able to build something useful off of it and ensure its functionality, we should tweak our multihead attention to enable some form of masking and then build a decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Now, we build the decoder and train and test it for some task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_accel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
