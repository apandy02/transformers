{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers 101\n",
    "\n",
    "This notebook serves as an exploration of the transformer architecture (Vaswani et. al.) Here, we'll implement in native PyTorch the basic building blocks of the transformer and then put them all together so we have a model architecture to put into `../models`\n",
    "\n",
    "In the process of putting this together (much like my other exploratory projects) I tried to limit viewing existing code online, and primarily used my notes (pdf attached for anyone interested) as a foundation for this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want something with output dims: (sequence_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_embedding(input_tensor: torch.Tensor, output_dim: int, n=10000): \n",
    "    \"\"\"\n",
    "    Here, we implement the naive approach from the original \n",
    "    paper with the sin and cosine functions. \n",
    "    \"\"\"\n",
    "    P = torch.zeros((input_tensor.shape[-1], output_dim))\n",
    "    indices = torch.arange(input_tensor.size(-1))\n",
    "    i_values = torch.arange(int(output_dim/2))\n",
    "    denominators = torch.float_power(n, 2*i_values/output_dim)\n",
    "    P[:, 0::2] = torch.sin(indices.unsqueeze(1) / denominators.unsqueeze(0)) # start at 0, step by 2 sin for even nums\n",
    "    P[:, 1::2] = torch.cos(indices.unsqueeze(1) / denominators.unsqueeze(0)) # start at 1, step by 2 cos for odd nums\n",
    "    return P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
       "        [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
       "        [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
       "        [ 0.1411, -0.9900,  0.0300,  0.9996],\n",
       "        [-0.7568, -0.6536,  0.0400,  0.9992]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((2, 5))\n",
    "output_dims = 4\n",
    "positional_embedding(a, output_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(x): \n",
    "    \"\"\"\n",
    "    Simple dot product based attention\n",
    "    \"\"\"\n",
    "    query_layer, key_layer, value_layer = nn.Linear(x.shape[-1], x.shape[-1]), nn.Linear(x.shape[-1], x.shape[-1]), nn.Linear(x.shape[-1], x.shape[-1])\n",
    "    query, key, value = query_layer(x), key_layer(x), value_layer(x)\n",
    "    attention_weights  = torch.nn.Softmax(-1)(torch.tensordot(query, key, dims=1))\n",
    "    return torch.sum(value * attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.1136, grad_fn=<SumBackward0>),\n",
       " tensor([[0.0715, 0.0979, 0.0829, 0.0886, 0.0833, 0.0835, 0.0754, 0.0744, 0.0844,\n",
       "          0.0754, 0.0918, 0.0908]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[-0.0482, -0.3826, -0.8782,  0.3126, -0.3020, -0.4886,  0.4078, -0.6122,\n",
       "          -0.3082,  0.3724,  0.1097,  0.4523]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1, 12)\n",
    "attention(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_accel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
