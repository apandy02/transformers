# Transformers

This repository serves as a comprehensive record of my implementation and exploration of the Transformer architecture. It's intended to dissect and comprehend the model's components through practical application for different use cases.

## Introduction

The Transformer model stands as a cornerstone in the realm of natural language processing and is now unifying different sub-areas of deep learning. This project is an effort to decode its complexity by piecing together its elements one at a time.

## Repository Structure

- `/notebooks` - Contains Jupyter notebooks that meticulously detail the coding process and explanations for each segment of the Transformer.
  - `transformer_from_scratch.ipynb` - A notebook documenting the sequential development of a Transformer.

- `/Attention and Transformers.pdf` - My notes on the transformer architecture from watching lectures online and reading the paper.

## Use Cases Explored

(As use cases are tackled and implemented, this section will catalog them with succinct descriptions and references to the corresponding notebooks.)

## Getting Started

To engage with this repository:

1. Clone the repository to your local system.
2. Ensure the prerequisites listed in [requirements.txt](requirements.txt) are installed.
3. Navigate through the notebooks in the `/notebooks` directory to explore the implementations.

## Acknowledgments

- This work is inspired by the "Attention is All You Need" paper by Vaswani et al., which introduced the Transformer model.
- Prof. Pascal Poupart's open Waterloo Machine learning lectures on YouTube

Note: This README will be periodically updated to reflect the progression and expansion of the project.
